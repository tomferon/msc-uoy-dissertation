%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage{color}
\definecolor{note_fontcolor}{rgb}{0.800781, 0.800781, 0.800781}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter

\newcommand{\comment}[1]{\color{blue}#1\color{black}}
\newcommand{\tomcomment}[1]{\color{orange}#1\color{black}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\DeclareRobustCommand*{\lyxarrow}{%
\@ifstar
{\leavevmode\,$\triangleleft$\,\allowbreak}
{\leavevmode\,$\triangleright$\,\allowbreak}}
%% The greyedout annotation environment
\newenvironment{lyxgreyedout}
  {\textcolor{note_fontcolor}\bgroup\ignorespaces}
  {\ignorespacesafterend\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{theorem}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[theorem]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{assumption}[theorem]{\protect\assumptionname}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\usepackage{amsfonts}
\newcommand{\commentMJC}[1]{{\color{red}#1}}

\makeatother

\providecommand{\assumptionname}{Assumption}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Title}
\author{Thomas Feron}
\date{~}

\maketitle
\vspace{2.5in}

\noindent \begin{center}
Dissertation submitted for the MSc in Mathematical Finance
\par\end{center}

\begin{center}
\bigskip{}
\par\end{center}

\begin{center}
Department of Mathematics

University of York\bigskip{}
\par\end{center}

\begin{center}
\today
\par\end{center}

\vspace{1in}

\begin{center}
Supervisor: Maciej J. Capi\'nski
\par\end{center}

\newpage{}

\tableofcontents{}\newpage{}

\pagebreak
%%%%%%%%%%%%%
\comment{I have moved your latex file to the UoY MSc latex template.}
%%%%%%%%%%%%%

%%%%%%%%%%%%%
\comment{Best not to use the section* environments. All sections should be numbered.}
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\comment{It would be best to send me sections which are finished or at least close to finished. What is here seems to be far from it, I am skipping it.}
%%%%%%%%%%%%%


FIXME: Quick intro to the topic and explanation of what is coming.
%%%%%%%%%%%%%
\comment{Above was without preparation or context. Was this supposed to be some kind of introduction? Seems rather heavy for an introduction. I am not sure how a ``Quick intro to the topic and explanation of what is coming" can incorporate the above.}
%%%%%%%%%%%%%

\pagebreak
\section{Preliminaries}

% Develop a preliminaries section, in which the tools of stochastic Ito calculus are introduced. Assume that your reader has background in the standard undergraduate modules in mathematics (including analysis, measure theory, probability theory and statistics), but introduce all the notions concerning stochastic processes from scratch. This section should not contain any proofs. Simply set up and state the needed results, providing citations to sources. While developing this section, restrict to the setup needed for the model from [2].

This section establishes the preliminaries of stochastic calculus required by further sections without proofs. See \textcite{capinski_stochastic_2012} and \textcite{capinski_blackscholes_2012} for further details.

In the following, we implicitly assume that we work in a probability space $(\Omega, \mathcal{F}, P)$ unless stated otherwise. We also restrict ourselves to processes defined on a time interval $[0,T]$ for some $T$ as it is sufficient in this context. When relevant, we further assume that we work with a filtration $(\mathcal{F}_t)_{t \in [0,T]}$.

We will denote the Borel subsets of $A$ by $\mathcal{B}(A)$, e.g. $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subets of $\mathbb{R}^d$.

\begin{definition}
  A \textbf{stochastic process} is a measurable function $X : [0,T] \times \Omega \to \mathbb{R}^d$ with respect to the $\sigma$-field $\mathcal{B}([0,T]) \times \mathcal{F}$.
%%%%%%%%%%%%%
\comment{We should explain the notation $\mathcal{B}([0,T])$}
%%%%%%%%%%%%%
\end{definition}

In the following, when we write $X(t)$ for $t \in [0,T]$, it denotes the random variable $\omega \mapsto X(t, \omega)$.
%%%%%%%%%%%%%
\comment{Formally $X(t)$ is not defined, since $X : [0,T] \times \Omega \to \mathbb{R}^d$. The choice of the phrase ``Note that ...'' might not be the best.}
%%%%%%%%%%%%%

\begin{definition}
  A \textbf{filtration} is a family $(\mathcal{F}_t)_{t \in [0,T]}$ of sub-$\sigma$-fields of $\mathcal{F}$ such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $0 \le s < t \le T$.
\end{definition}

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\}
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$ for any $\mathcal{F}$-measurable random variable $Y$.

%%%%%%%%%%%%%
\comment{small but {\bf important} comment: We must be aware how we structure paragraphs. Compare your three paragraphs to `my' single paragraph below:

...

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\}
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$ for any $\mathcal{F}$-measurable random variable $Y$.

...

The important issue is that each empty line in \LaTeX\, creates a new paragraph. We must be aware of this and use this on purpose.
}
%%%%%%%%%%%%%

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be \textbf{adapted} to a filtration $(\mathcal{F}_t)_{t \in [0,T]}$ if for all $t \in [0,T]$, $X(t)$ is $\mathcal{F}_t$-measurable.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be a \textbf{martingale} for a filtration $\mathcal{F}_t$ if $X(t)$ is integrable for each $t \in [0,T]$ and
  \comment{We have redundant line brakes, which spoil formatting. Please correct throughout the draft.}
  \begin{align*}
    \mathbb{E}(X(t) \mid \mathcal{F}_s) = X(s)
  \end{align*}
  for all $0 \le s < t \le T$.
\end{definition}

A very important stochastic process is the Wiener process. Here, we give an axiomatic definition. For a construction of such a process and thus a proof of existence, see \textcite{capinski_stochastic_2012}.

\begin{definition}
  A \textbf{Wiener process}, also called Brownian motion, is a stochastic process $(W(t))_{t \in [0,T]}$ that satisfies
  \begin{itemize}
    \item $W(0) = 0$ almost surely,
    \item for all $0 \le s < t \le T$, the increment $W(t) - W(s)$ follows a normal distribution with mean 0 and variance $t - s$,
    \item for all $0 \le t_1 < t_2 < \cdots < t_m$, the increments $W(t_k) - W(t_{k-1}), k=2,\ldots,m$ are independent,
    \item almost all paths are continuous, i.e. $t \mapsto W(t,\omega)$ are continuous functions for almost all $\omega \in \Omega$.
  \end{itemize}
\end{definition}

\begin{definition}
  A \textbf{$d$-dimensional Wiener process} is a stochastic process $\mathbf{W}(t) = (W_1(t), W_2(t), \ldots, W_d(t))$ where $W_j(t), j=1,\ldots,n$ are independent Wiener processes.
\end{definition}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{at this point we have not written out what $\mathcal{S}^2$ and $ \mathcal{M}^2$ are, so the reader cannot see that $\mathcal{S}^2 \subset \mathcal{M}^2$}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{Each equation should be part of a sentence, below we have equations which are detached from the text.}
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\begin{definition}
  We say that a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ is a \textbf{simple process}, and denote it by $X \in \mathcal{S}^2$, if
  \begin{align*}
    X(t,\omega) = \xi_0 \mathbf{1}_{\{0\}}(t) + \sum_{k=0}^{n-1} \xi_k(\omega) \mathbf{1}_{(t_k,t_{k+1}]}(t)
  \end{align*}
  for some $n > 0$, $0 = t_0 < t_1 < \cdots < t_n = T$ and $\mathcal{F}^W_{t_k}$-measurable random variables $\xi_k$ such that $\mathbb{E}(\xi_k^2) < \infty$ for $k = 0,1,\ldots,n-1$.
\end{definition}
\comment{Above we should have $\mathbf{1}_{\{0\}}(t)$ instead of $\mathbf{1}_0$. (I know that this comes from the book, but it seems that there is a typo in the book.)}

%%%%%%%%%%%%%
\begin{definition}
  The \textbf{stochastic integral}, also called It\^o integral, of a process $X \in \mathcal{S}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \sum_{k=0}^{n-1} \xi_k (W(t_{k+1}) - W(t_k)).
  \end{align*}
\end{definition}

\begin{definition}
  The set $\mathcal{M}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t \right) < \infty.
  \end{align*}
\end{definition}
\comment{Which filtration $\mathcal{F}_t$ is used here? Arbitrary filtration?}

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.4)\label{prop:s2-m2-conv}
%%%%%%%%%%%%%
\comment{citation missing. Please add a citation to a bibliography item in every theorem, lemma and proposition in the draft.}
%%%%%%%%%%%%%
  For all $X \in \mathcal{M}^2$, there exists a sequence $(X_n)_{n \ge 1}$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \lim_{n \to \infty} \int_0^T X_n(t) \mathrm{d}W(t).
  \end{align*}
  for a sequence $(X_n)$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{definition}

The limit in this definition exists and does not in fact depend on which specific sequence $(X_n)$ convergent to $X$ we choose. This together with \ref{prop:s2-m2-conv}
%%%%%%%%%%%%%
\comment{Proposition \ref{prop:s2-m2-conv} (here you are referencing a proposition not an equation)}
%%%%%%%%%%%%%
ensures that the stochastic integral is well-defined on $\mathcal{M}^2$. See \cite{oksendal_stochastic_2003}, Definition 3.1.6.

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.15)\label{prop:stochastic-integral-martingale}
  For all $X \in \mathcal{M}^2$, there exists a martingale $M : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that
  \begin{align*}
    M(t) = \int_0^T \mathbf{1}_{[0,t]}(s) X(s) \mathrm{d}W(s)
  \end{align*}
  almost surely for all $t \in [0,T]$.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ is defined as the process
  \begin{align*}
    \int_0^t X(s) \mathrm{d}W(s) = M(t)
  \end{align*}
  for $t \in [0,T]$ where $M$ is the martingale given by \ref{prop:stochastic-integral-martingale}.
\end{definition}

\begin{definition}
  The set $\mathcal{P}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \int_0^T X(t)^2 \mathrm{d}t < \infty
  \end{align*}
  almost surely.
\end{definition}

\begin{proposition}
  Let $X \in \mathcal{P}^2$ and let $(X_n)_{n \ge 1}$ be the sequence given by
  \begin{align*}
    X_n(t) = \int ...
  \end{align*}

  FIXME
\end{proposition}

FIXME: Extension of the integral on $\mathcal{P}^2$.

%%%%%%%%%%%%%
\comment{Better to formulate this as a theorem and give a citation:}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{this sentence was not clear. (Side issue, why are we discussing this; is this important; will we use this fact later?)}
%%%%%%%%%%%%%

%%%%%%%%%%%%%
\comment{I am not quite sure if I understand the below FIXMEs. }
%%%%%%%%%%%%%

FIXME: Multidimensional It\^o integrals.

FIXME: $b \in \mathcal{P}^2$ in the following:

\begin{definition}
  An \textbf{It\^o process} is a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ of the form
  \begin{align}\label{eq:def-ito-process}
    X(t) = X(0) + \int_0^t a(s) \mathrm{d}s + \int_0^t b(s) \mathrm{d}W(s), t \in [0,T]
  \end{align}
  for some $a, b : [0,T] \times \Omega \to \mathbb{R}$, called the \textbf{characteristics} of the It\^o process, such that $a$ is $\mathcal{F}^W_t$-adapted and satisfies $\int_0^T |a(s)| \mathrm{d}s < \infty$ almost surely, and $b \in \mathcal{P}^2$.
  %%%%%%%%%%%%%
\comment{The definition is incomplete. We must specify the conditions required by $a,b$.}
%%%%%%%%%%%%%
\end{definition}

For convenience, we will also write \eqref{eq:def-ito-process}
%%%%%%%%%%%%%
\comment{broken label }
%%%%%%%%%%%%%
 using the following terser notation, its \textbf{stochastic differential}:
\begin{align*}
  \mathrm{d}X(t) = a(t) \mathrm{d}t + b(t) \mathrm{d}W(t).
\end{align*}

\begin{definition}
  A \textbf{stochastic differential equation} (or \textbf{SDE} for short) is an equation of the form
  \begin{align}\label{eq:sde-init-value}
    X(t) = X(0) + \int_0^t u(s, X(s)) \mathrm{d}s + \int_0^t v(s, X(s)) \mathrm{d}W(s), &&
    X(0) = x_0
  \end{align}
  for some $u, v : \mathbb{R}^2 \to \mathbb{R}$ and $x_0 \in \mathbb{R}$.
\end{definition}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{why does the sentence end? }
\comment{What do we mean by ``consider''? Are we defining what is an SDE or should the reader know this. This is unclear from the context.}
%%%%%%%%%%%%%

\begin{theorem}(\cite{capinski_stochastic_2012}, Theorem 5.8)\label{thm:sde-solution}
  Provided that both the coefficients $u(t,x)$ and $v(t,x)$ satisfy the following conditions.
  \begin{itemize}
    \item Linear growth in the first variable: there exists $C > 0$ such that
      \begin{align*}
        |u(t,x)| + |v(t,x)| \le C (1 + |x|), \text{ for } t \in [0,T], x \in \mathbb{R}.
      \end{align*}

    \item Lipschitz continuity in the second variable: there exists $K > 0$ such that
      \begin{align*}
        |u(t,x) - u(x,y)| + |v(t,x) - v(t,y)| \le K |x-y|, \text{ for } t \in [0,T], x,y \in \mathbb{R}.
      \end{align*}
  \end{itemize}
  Then \eqref{eq:sde-init-value} has a unique solution with continuous paths such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t\right) < \infty.
  \end{align*}
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{``to derive new SDEs from old ones" does not sound accurate.}
%%%%%%%%%%%%%

\begin{theorem}(\cite{capinski_stochastic_2012}, Theorem 4.22)\label{thm:ito-formula}
%%%%%%%%%%%%%
\comment{citation missing}
%%%%%%%%%%%%%
  (\textbf{It\^o formula}) If $F : [0,T] \times \mathbb{R} \to \mathbb{R}$ is in $C^{1,2}$
  %%%%%%%%%%%%%
\comment{we could explain what is $C^{1,2}$}
%%%%%%%%%%%%%
   and $X$ is an It\^o process with characteristics $a$ and $b$, i.e.
  \begin{align*}
    \mathrm{d}X(t) &= a(t) \mathrm{d}t + b(t) \mathrm{d}W(t).
  \end{align*}
  Then $F(t,X(t))$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}F(t,X(t)) &= \left(F_t(t,X(t)) + F_x(t,X(t)) a(t) + \frac{1}{2} F_{xx}(t,X(t)) b(t)^2 \right) \mathrm{d}t\\
    &\ \ \ \ + F_x(t,X(t)) b(t) \mathrm{d}W(t).
  \end{align*}
  %%%%%%%%%%%%%
\comment{the statement as made here requires the Ito processes to involve Ito integrals in $\mathcal{P}^2$.}
%%%%%%%%%%%%%
\end{theorem}

\begin{definition}
  A \textbf{multidimensional It\^o process} is a stochastic process $\mathbf{X}(t) = (X_1(t), X_2(t), \ldots, X_d(t))$ that satisfy

  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t) \mathrm{d}t + \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t), \text{ for } i=1,\ldots,d
  \end{align*}

  where $W_j(t), j=1,\ldots,n$ are $n$ Wiener processes, $a_i(t), i=1,\ldots,d$ are stochastic processes such that $\int_0^T |a_i(t)| \mathrm{d}t < \infty$ and $b_{ij} \in \mathcal{M}^2$ for $i=1,\ldots,d$ and $j=1,\ldots,n$.
\end{definition}

FIXME: Check whether the multidimensional It\^o processes can be defined such that $d=n$ and simplify the following theorems.

FIXME: Uniqueness of characteristics of multi-dimensional It\^o processes (BS, theorem 6.9)

We can now state the multidimensional It\^o formula. A proof can be found in \textcite{capinski_blackscholes_2012}.

\begin{theorem}\label{thm:ito-formula-multi}
  (\textbf{Multidimensional It\^o formula}) If $F : [0,T] \times \mathbb{R}^d \to \mathbb{R}$ of class $C^1$ in the first variable and $C^2$ in the others, and $\mathbf{X}(t)$ is a $d$-dimensional It\^o process driven by $n$ independent Wiener processes. Then $F(t,\mathbf{X}(t)) = F(t,X_1(t),X_2(t),\ldots,X_d(t))$ is an It\^o process with stochastic differential

  \begin{align*}
    \mathrm{d}F(t,\mathbf{X}(t))
    &= F_t(t,\mathbf{X}(t)) \mathrm{d}t + \sum_{i=1}^d F_{x_i}(t,\mathbf{X}(t)) a_i(t) \mathrm{d}t\\
    &\ \ \ \ + \sum_{i=1}^d \left(F_{x_i}(t,\mathbf{X}(t)) \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t) \right)\\
    &\ \ \ \ + \frac{1}{2} \sum_{j=1}^n \sum_{i,l=1}^d F_{x_i x_l}(t,\mathbf{X}(t)) b_{ij}(t)b_{lj}(t) \mathrm{d}t.
  \end{align*}
  %%%%%%%%%%%%%
\comment{again, likely we need Ito integral on $\mathcal{P}^2$.}
%%%%%%%%%%%%%
\end{theorem}

This formula can be used to prove the following lemma which will come handy later on. See \textcite{capinski_blackscholes_2012}.

\begin{lemma}\label{thm:exp-sums-martingale}
  If $\theta_j(t), j=1,\ldots,n$ are deterministic and $W_j(t), j=1,\ldots,d$ are independent Wiener processes, then the stochastic process

  \begin{align*}
    M(t) = \exp \left( \sum_{j=1}^{d} \theta_j(s) \mathrm{d}W_j(s) - \frac{1}{2} \sum_{j=1}^{d} \theta_j(s)^2 \mathrm{d}s \right).
  \end{align*}

  is a martingale.
\end{lemma}

We now turn our attention to product of It\^o processes starting
%%%%%%%%%%%%%
\comment{Why ``starting''? There does not seem to be the multidimensional one later on. }
%%%%%%%%%%%%%
 with the unidimensional case. See \textcite{capinski_stochastic_2012} for a proof where it is referred to as integration by parts.

\begin{theorem}
  Given two It\^o processes $X$ and $Y$ satisfying

  \begin{align*}
    \mathrm{d}X(t) &= a_X(t) \mathrm{d}t + b_X(t) \mathrm{d}W(t),\\
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}

  then the product $XY$ is an It\^o process satisfying

  \begin{align*}
    \mathrm{d}[X(t) Y(t)]
    &= a_X(t) Y(t) \mathrm{d}t + b_X(t) Y(t) \mathrm{d}W(t)\\
    &\ \ \ \ + a_Y(t) X(t) \mathrm{d}t + b_Y(t) X(t) \mathrm{d}W(t)\\
    &\ \ \ \ + b_X(t) b_Y(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

FIXME: Multidimensional It\^o product rule? If not needed, then some paragraph above needs to be changed to remove its mention.

%%%%%%%%%%%%%
\comment{We need some introduction and setup to the financial statements that follow. So far we discussed stochastic processes and change of topic is somewhat sudden.}
%%%%%%%%%%%%%

For pricing purposes and since the physical (real) probability $P$ is unknown, we will want to construct a risk-neutral probability $Q$ such that the discounted prices of assets form a martingale under $Q$, i.e. $\mathbb{E}_Q(e^{-rt}S(t) \mid \mathcal{F}_s) = e^{-rs}S(s)$ for $s < t$. The following theorem will allow us to construct such a measure and a Wiener process with respect to it. A proof can be found in \textcite{capinski_blackscholes_2012}.

%%%%%%%%%%%%%
\comment{I in fact think that the Girsanov theorem is a mathematical statement, which does not necessarily require a financial introduction.}
%%%%%%%%%%%%%

\begin{theorem}\label{thm:girsanov}
  (\textbf{Girsanov theorem}) Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $\theta_j, j=1,\ldots,d$ be $\mathcal{F}^\mathbf{W}_t$-adapted processes such that

  \begin{align*}
    M(t) = \exp \left( - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s \right)
  \end{align*}

  is a martingale under $P$ and let $Q$ be the measure with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$, i.e.

  \begin{align*}
    Q(A) = \int_A M(T) \mathrm{d}P.
  \end{align*}

  Then the process $\mathbf{W}^Q(t) = (W^Q_1(t), W^Q_2(t), \ldots, W^Q_d(t))$ with

  \begin{align*}
    W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
  \end{align*}

  is a $d$-dimensional Wiener process under $Q$.
\end{theorem}

In order to construct replicating strategies in the multi-asset Black-Scholes model, we will need one more theorem that allows us to represent a martingale in terms of a stochastic integral. See \textcite{shreve_stochastic_2004}.
%%%%%%%%%%%%%
\comment{Such statements, without preparation, are not helpful. I would skip this. This does not look necessary.}
%%%%%%%%%%%%%

\begin{theorem}
  (\textbf{Martingale representation theorem}) Let $\mathbf{W}(t)$ be a $d$-dimensional Wiener process and $M(t)$ be a martingale. Then there is an adapted
  %%%%%%%%%%%%%
\comment{adapted to which filtration? (The filtration could be specified at the beginning.)}
%%%%%%%%%%%%%
   $d$-dimensional process $\mathbf{\Gamma}(t)$ such that

  \begin{align*}
    M(t) = M(0) + \sum_{j=1}^{d} \int_0^t \Gamma_j(s) \mathrm{d}W_j(s).
  \end{align*}
\end{theorem}

\begin{lemma}\label{lem:ito-integral-girsanov}
  Consider two probability measures $P, Q$ on the same probability space. Let $W$ be a Wiener process under $P$, $W^Q$ a Wiener process under $Q$ and $X$ an It\^o process such that

  \begin{align*}
    \mathrm{d}X(t) &= a(t) \mathrm{d}t + b(t) \mathrm{d}W(t),\\
    \mathrm{d}W^Q(t) &= \theta(t) \mathrm{d}t + \mathrm{d}W(t).\\
  \end{align*}

  Then

  \begin{align*}
    \int_0^t X(s) \mathrm{d}W^Q(s) = \int_0^t a(s) \theta(s) \mathrm{d}s + \int_0^t b(s) \mathrm{d}W(s).
  \end{align*}
%%%%%%%%%%%%%
\comment{Are you sure that above is true? To me it would seem that
\[XdW^Q = X\theta dt + X dW.\]

Take $X=W$ and $W^Q=W$ (i.e. $a=0,b=1,\theta=0$). Then my equation, which I believe to be true, gives
\[WdW^Q = W\theta dt + W dW=WdW,\]
but yours gives
\[WdW^Q =  0 dt + 1 dW = dW.\]
This looks incorrect, since then we would have $WdW=dW$. I think that what you have stated here is not true.
}

%%%%%%%%%%%%%
  \begin{proof}
    FIXME: Find reference to proof somewhere out there? or write a proof properly + address [0,t] v [0,T].

    If $(X_n)$ is a sequence of simple functions that converges in $L^2$ to $X$, then

    \begin{align*}
      \int_0^T X_n(s) \theta(s) \mathrm{d}s + \int_0^T X_n(s) \mathrm{d}W(s)
      &= \sum_{i=0}^N \left( \int_{t_i}^{t_{i+1}} \xi_{n,i} \theta(s) \mathrm{d}s + \xi_{n,i} (W(t_{i+1}) - W(t_i)) \right)\\
      &= \sum_{i=0}^N \xi_{n,i} \left( \int_{t_i}^{t_{i+1}} \theta(s) \mathrm{d}s + \int_{t_i}^{t_{i+1}} \mathrm{d}W(s) \right)\\
      &= \sum_{i=0}^N \xi_{n,i} (W^Q(t_{i+1}) - W^Q(t_i))\\
      &= \int_0^T X_n(s) \mathrm{d}W^Q(s)\\
      &\underset{L^2}{\longrightarrow} \int_0^T X(s) \mathrm{d}W^Q(s)
    \end{align*}
  \end{proof}
\end{lemma}

\pagebreak
\section{Multi-asset Black-Scholes model}

% Introduce the multidimensional Black-Scholes model. Use [1] as a reference.

The unidimensional Black-Scholes consist of one risk-free asset with price $A(t)$ and one risky asset with price $S(t)$ at time $t$ satisfying the following stochastic differential equations.
%%%%%%%%%%%%%
\comment{sentence should not stop here.}
%%%%%%%%%%%%%

\begin{align*}
  \mathrm{d}A(t) &= r A(t) \mathrm{d}t,\\
  \mathrm{d}S(t) &= \mu S(t) \mathrm{d}t + \sigma S(t) \mathrm{d}W(t),
\end{align*}
%%%%%%%%%%%%%
\comment{why are we discussing the one dimensional model? The section title talks of the multi-asset Black-Scholes model. }
%%%%%%%%%%%%%

where $r$ is the risk-free rate, $W$ is a Wiener process with respect to the physical probability, $\mu$ is the drift and $\sigma$ the volatility.

In this section, we expand this model in two ways to arrive at the multi-asset Black-Scholes model with variable coefficients. First, we extend to $d$ risky assets $S_j(t), j =  1, \ldots, d$, each driven by $d$ independent Wiener process $W_i(t), i = 1, \ldots, d$. Second, the coefficients are now functions of time. The dynamics of risky assets becomes
%%%%%%%%%%%%%
\comment{``becomes''? How? from where does the below follow? Is this not simply assumed?}
%%%%%%%%%%%%%

\begin{align}
  \mathrm{d}A(t) &= r(t) A(t) \mathrm{d}t\notag\\
  \mathrm{d}S_i(t) &= \mu_i(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W_j(t), \text{ for } i = 1,\ldots,d.\label{eq:multi-bs-eq}
\end{align}

Some further assumptions are required on the coefficients: $\mu_i(t), c_{ij}(t)$ are adapted to the filtration generated by the Wiener processes, have continuous paths and are bounded by a deterministic constant. We'll also assume that the matrix of volatily coefficients $\mathbf{C}(t) = [c_{ij}(t)]_{i,j=1,\ldots,d}$ is invertible.

FIXME: Justificiation of $d$ Wiener processes for $d$ assets?

FIXME: Comment about the fact that assumptions are justified in the following development + references to where.

FIXME: Are they?

FIXME: Recap of what will be done

\subsection{Solution}
%%%%%%%%%%%%%
\comment{strange section title.}
%%%%%%%%%%%%%

The solution to \eqref{eq:multi-bs-eq} is
%%%%%%%%%%%%%
\comment{this should be stated as a lemma and we should either provide a reference of give the proof.}
%%%%%%%%%%%%%

\begin{align*}
  S_i(t) &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right).
\end{align*}

This can be verified using the multidimensional It\^o formula. Uniqueness follows from \eqref{eq:ito-uniqueness-md} since we assume

FIXME: \^{} Prove it

\pagebreak
\subsection{Risk-neutral probability}

FIXME: It also has to be equivalent to $P$ which it is since Girsanov implies $Q \ll P$ implies $P \sim Q$.

We want to find a probability measure under which the discounted stock prices form martingales. This will in turn allow us to express derivative prices in terms of conditional expectations.

Very briefly, this is achieved in the unidimensional Black-Scholes model with constant coefficients by noticing that its solution satisfies
%%%%%%%%%%%%%
\comment{why are we discussing the one dimensional model? This is a section on the multidimensional model.}
%%%%%%%%%%%%%
\begin{align*}
  S(t)
  &= S(0) \exp \left( \mu t - \frac{1}{2} \sigma^2 t + \sigma W(t) \right)\\
  &= S(0) \exp \left( r t - \frac{1}{2} \sigma^2 t + \sigma \left( \frac{\mu - r}{\sigma} t + W(t) \right) \right)\\
  &= S(0) \exp \left( r t - \frac{1}{2} \sigma^2 t + \sigma W^Q(t) \right)
\end{align*}

where $W^Q(t) = \int_0^t \frac{\mu-r}{\sigma} \mathrm{d}s + W(t)$ is a Wiener process under the probability $Q$ given by the Girsanov theorem \ref{thm:girsanov} with $d=1$, $a_1(t) = \frac{\mu - r}{\sigma}$. Under this probability, the discounted stock prices then form a martingale so $Q$ is the risk-neutral probability.

We will use a similar technique to obtain a risk-neutral probability for the multi-asset Black-Scholes model with variable coefficients.

First, notice that, for all $i=1,\ldots,d$, if we can find $\theta_j(t), j=1,\ldots,d$ such that
%%%%%%%%%%%%%
\comment{how can we notice this? Is the existence of such $\theta$ evident?}
%%%%%%%%%%%%%

\begin{align}\label{eq:bs-theta}
  \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t)
\end{align}

then

%%%%%%%%%%%%%
\comment{it is unclear where the discussion is heading. We should state the result and prove it. Moreover, the formulae spill onto the margin.}
%%%%%%%%%%%%%

\begin{align*}
  S_i(t)
  &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\\
  &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \int_0^t (\mu_i(s) - r(s)) \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\\
  &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \int_0^t \left(\sum_{j=1}^d c_{ij}(s) \theta_j(s)\right) \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\\
  &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \sum_{j=1}^d \left( \int_0^t c_{ij}(s) \theta_j(s) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\right).
\end{align*}

Furthermore, if

\begin{align*}
  M(t) = \exp \left( - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s \right)
\end{align*}

is a martingale under $P$, then by the Girsanov theorem \ref{thm:girsanov}
%%%%%%%%%%%%%
\comment{incorrect referencing style. This is not an equation.}
%%%%%%%%%%%%%
, the processes

\begin{align*}
  W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
\end{align*}

are Wiener processes under the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$.

But then, by lemma \ref{lem:ito-integral-girsanov}
%%%%%%%%%%%%%
\comment{incorrect referencing style. This is not an equation.}
%%%%%%%%%%%%%
,

\begin{align*}
  S_i(t)
  &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \sum_{j=1}^d \left( \int_0^t c_{ij}(s) \theta_j(s) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\right)\\
  &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W^Q_j(s) \right).
\end{align*}

Therefore, the discounted asset prices are martingales under $Q$ by lemma \ref{thm:exp-sums-martingale} and since

\begin{align*}
  \tilde{S}_i(t)
  &= e^{-\int_0^t r(s) \mathrm{d}s} S_i(t)
  = S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}^2(s) \mathrm{d}s  + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W^Q_j(s) \right).
\end{align*}

In order words, $Q$ is a risk-neutral probability. In summary,

\begin{theorem}
  Let $\theta_j(t), j=1,\ldots,d$
  %%%%%%%%%%%%%
\comment{be }
%%%%%%%%%%%%%
  such that

  \begin{itemize}
    \item $\mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t)$ and
    \item $M(t) = \exp \left( - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s \right)$ is a martingale under the physical probability $P$.
  \end{itemize}

  Then the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$ is a risk-neutral probability, i.e. the processes $\tilde{S}_i, i=1,\ldots,d$ are martingales under $Q$.
\end{theorem}

Moreover, we can prove that the model assumption that $\mathbf{C}(t)$ is invertible implies the existence of $\theta_j(t), j=1,\ldots,d$ satisfying \eqref{eq:bs-theta}
%%%%%%%%%%%%%
\comment{above we have written as if this was obvious.}
%%%%%%%%%%%%%
.

\begin{proposition}
  If $\mathbf{C}(t)$ is invertible for all $t$, then there exists a unique process $\mathbf{\theta}(t) = (\theta_1(t), \ldots, \theta_d(t))$ such that

  \begin{align*}
    \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t)
  \end{align*}

  for all $j=1,\ldots,d$.

  \begin{proof}
    Indeed, notice that the equation can be rewritten in vector form as

    \begin{align*}
      \mathbf{C}(t) \mathbf{\theta}(t) = \mathbf{\mu}(t) - r(t).
    \end{align*}

    It follows at once that

    \begin{align*}
      \mathbf{\theta}(t) = \mathbf{C}^{-1}(t) (\mathbf{\mu}(t) - r(t)).
    \end{align*}
  \end{proof}
\end{proposition}

%%%%%%%%%%%%%
\comment{what follows seems unfinished so I will stop here. Here are some final remarks:
\begin{itemize}
\item This is a very good first draft!
\item We must be more careful about paragraphs. The empty lines in latex code create a mess in the pdf file. If you fix this the draft will automatically become {\bf much} cleaner and obtain a professional look.
\item All theorems lemmas and propositions must come with a citation to a bibliography item.
\item There are a lot of FIXMEs. At the beginning this is fine, but I strongly encourage you not to continue like this. A draft of 20 pages which requires a lot of fixes will become an editorial nightmare. As you fix in one place, possibly some changes will be required in other places etc. Things will get out of hand. The best rule is: ``slow and steady wins the race''. It is best to finish sections, so that they are ready. They can always be expanded, but a draft should consist of work that is print ready (or as close as possible, at least). Otherwise you will have a lot of editorial problems as you proceed further.
\end{itemize}
}
%%%%%%%%%%%%%

FIXME: Corollary with C(t) invertible and the drift and volatility coefficients deterministic. Use previous theorem with lemma from preliminaries.

\subsection{Strategies}

FIXME: Definition of contigent claim

FIXME: Definition of strategy

FIXME: Self-financing condition

FIXME: Definition of martingale strategy

FIXME: Definition of admissible strategy

FIXME: Explanation as to why we need admissiblity?

FIXME: Definition of replicating strategy

FIXME: Representation of contigent claims as processes assumed to be It\^o

\subsection{Completeness}

FIXME: Completeness of the model. (Is it though?)

\subsection{Pricing of derivatives}

FIXME: Pricing with risk-neutral expectations + comment about unknown joint distribution

FIXME: Black-Scholes PDE

\pagebreak
\section{The Fokker-Planck equation}

% Introduce the Fokker-Planck equation. Use [4] as a reference.

FIXME: Intro citing \textcite{pavliotis_stochastic_2014}

FIXME: We work in $(\Omega, \mathcal{F}, \mathcal{F}_t, P)$.

\begin{definition}
  A \textbf{Markov process} is a stochastic process $\mathbf{X}(t)$ that satisfies the \textit{Markov condition}

  \begin{align*}
    \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_{s})
    = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  \end{align*}

  for $0 \le s < t \le T$ and for all Borel bounded functions $f$.
\end{definition}

These processes are relevant to our discussion since It\^o processes -- and in particular Wiener processes -- are in fact Markov processes.

FIXME: It\^o processes are Markov processes

Informally, the future evolution of a Markov process only depends on its current state, independently from its past evolution.

For $s < t$ and $\Gamma \in \mathcal{B}({\mathbb{R}^d})$ -- where $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subsets of $\mathbb{R}^d$ -- the Markov condition implies that

\begin{align*}
  P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s)
  &= \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_s)
  = \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  = \phi(\mathbf{X}(s))
\end{align*}

for some Borel function $\phi$ by the Doob-Dynkin lemma. This justifies the following definition since it ensures that it exists.

\begin{definition}
  Let $\mathbf{X}(t)$ be a $d$-dimensional Markov process. A \textbf{transition (probability) function} of $\mathbf{X}$ is a Borel function $\mu(\Gamma, t; \mathbf{x}, s)$ for $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$ and $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ such that

  \begin{align*}
    \mu(\Gamma, t; \mathbf{X}(s), s) = P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s).
  \end{align*}
\end{definition}

Note that, viewing $\mu(\Gamma, t; \mathbf{x}, s)$ as a function of $\Gamma$ with the other arguments fixed, it forms a probability measure. In the rest of this section, we focus on such transition function that have a density, leading to the following definition.

\begin{definition}
  Let $\mathbf{X}(t)$ be a Markov process with transition function $\mu(\Gamma,t;\mathbf{x},s)$ that admits a density $\rho$ with respect to the Lebesgue measure, i.e.

  \begin{align*}
    \mu(\Gamma,t;\mathbf{x},s) = \int_\Gamma \rho(\mathbf{y},t;\mathbf{x},s) \mathrm{d}\mathbf{y}
  \end{align*}

  for all $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < t \le T$.

  We say that $\rho$ is a \textbf{transition (probability) density} of the Markov process.
\end{definition}

In order to prove the main result of this section, we will make use of the following property of the transition probability density. There exist several versions of this equation including one in terms of the transition function not requiring the existence of a density. However, we only state a single version here for brevity.

\begin{theorem}\label{thm:chapman-kolmogorov}
  (\textbf{Chapman-Kolmogorov equation}) Let $\mathbf{X}(t)$ be a Markov process with transition probability density $\rho$. Then

  \begin{align*}
    \rho(\mathbf{y}, t; \mathbf{x}, s) = \int_{\mathbb{R}^d} \rho(\mathbf{y}, t; \mathbf{z}, u) \rho(\mathbf{z}, u; \mathbf{x}, s) \mathrm{d}\mathbf{z}
  \end{align*}

  for all $0 \le s < u < t \le T$ and for almost all $\mathbf{x} \in \mathrm{Im}(\mathbf{X}(s)), \mathbf{y} \in \mathbb{R}^d$.

  \begin{proof}
    Let $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < u < t \le T$. Then

    \begin{align*}
      \int_\Gamma \rho(\mathbf{y},t;\mathbf{X}(s),s) \mathrm{d}\mathbf{y}
      &= \mu(\Gamma,t;\mathbf{X}(s),s)\\
      &= P(X(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbf{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mu(\Gamma,t;\mathbf{X}(u),u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mu(\Gamma,t;\mathbf{X}(u),u) \mid \mathcal{F}_{\mathbf{X}(s)})\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \mu(\mathrm{d}\mathbf{z},u;\mathbf{X}(s),s)\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}\\
      &= \int_{\mathbb{R}^d} \int_\Gamma \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{z}\\
      &= \int_\Gamma \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z} \mathrm{d}\mathbf{y}.
    \end{align*}

    Since this holds for all $\Gamma$, the integrand must be equal for almost $\mathbf{y}$, i.e.

    \begin{align*}
      \rho(\mathbf{y},t;\mathbf{X}(s),s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}.
    \end{align*}

    And since this itself holds for almost all $\omega \in \Omega$, then for almost all $\mathbf{x} \in \mathrm{Im}(\mathbf{X}(s))$ such that $\mathbf{x} = \mathbf{X}(s,\omega)$, we have

    \begin{align*}
      \rho(\mathbf{y},t;\mathbf{x},s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{x},s) \mathrm{d}\mathbf{z}.
    \end{align*}
  \end{proof}
\end{theorem}

We can finally state and prove the main result of this section. Note that it is also called the \textit{forward Kolmogorov equation} in other resources.

\begin{theorem}
  (\textbf{Fokker-Planck equation}) Let $\mathbf{X}(t)$ be an It\^o process satisfying

  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t) \mathrm{d}t + \sum_{j=1}^d b_{ij}(t) \mathrm{d}W_j(t)
  \end{align*}

  for $i=1,\ldots,d$ and with transition probability density $\rho$. Then

  \begin{align*}
    \frac{\partial \rho}{\partial t} = - \frac{\partial}{\partial y} (a(t,y) \rho) + \frac{1}{2} \frac{\partial^2}{\partial y^2} (b(t,y) \rho).
  \end{align*}

  (FIXME)
\end{theorem}

\pagebreak
\section{Dupire's equation}

% Give a detailed derivation of the Dupire’s equation (equation starting with ∂C = on page 171 in [2]). ∂T
% Use section 2 from [2] and the section ‘The continuous time theory’ from [3] as a source for the proof.

FIXME

\section{Generalisation to multiple assets}

% Provide the setup and give a detailed proof of Theorem 1 from [2]. This should be based on section 3 from [2]

FIXME

\section{Alternative proof}

% Present the alternative proof of Theorem 1, anded on Appendix A from [2].

FIXME

\section{Numerical example}

% Give a numerical example of how Theorem 1 can be applied to recover aij. You can restrict to the simplest setting of a two dimensional Black-Scholes model.

FIXME

\pagebreak
\printbibliography

\end{document}
