%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage{color}
\definecolor{note_fontcolor}{rgb}{0.800781, 0.800781, 0.800781}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{mathtools}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter

\newcommand{\comment}[1]{\color{blue}#1\color{black}}
\newcommand{\tomcomment}[1]{\color{orange}#1\color{black}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\DeclareRobustCommand*{\lyxarrow}{%
\@ifstar
{\leavevmode\,$\triangleleft$\,\allowbreak}
{\leavevmode\,$\triangleright$\,\allowbreak}}
%% The greyedout annotation environment
\newenvironment{lyxgreyedout}
  {\textcolor{note_fontcolor}\bgroup\ignorespaces}
  {\ignorespacesafterend\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\newtheoremstyle{bolddescit}{}{}{\itshape}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\newtheoremstyle{bolddesc}{}{}{}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{bolddescit}
\newtheorem{theorem}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[theorem]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
\theoremstyle{bolddesc}
\newtheorem{assumption}[theorem]{\protect\assumptionname}
\theoremstyle{plain}
\newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\usepackage{amsfonts}
\newcommand{\commentMJC}[1]{{\color{red}#1}}

\makeatother

\providecommand{\assumptionname}{Assumption}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Title}
\author{Thomas Feron}
\date{~}

\maketitle
\vspace{2.5in}

\noindent \begin{center}
Dissertation submitted for the MSc in Mathematical Finance
\par\end{center}

\begin{center}
\bigskip{}
\par\end{center}

\begin{center}
Department of Mathematics

University of York\bigskip{}
\par\end{center}

\begin{center}
\today
\par\end{center}

\vspace{1in}

\begin{center}
Supervisor: Maciej J. Capi\'nski
\par\end{center}

\newpage{}

\tableofcontents{}\newpage{}

\pagebreak

FIXME: Quick intro to the topic and explanation of what is coming.

\section{Preliminaries}

% Develop a preliminaries section, in which the tools of stochastic Ito calculus are introduced. Assume that your reader has background in the standard undergraduate modules in mathematics (including analysis, measure theory, probability theory and statistics), but introduce all the notions concerning stochastic processes from scratch. This section should not contain any proofs. Simply set up and state the needed results, providing citations to sources. While developing this section, restrict to the setup needed for the model from [2].

This section establishes the preliminaries of stochastic calculus required by further sections without proofs. See \textcite{capinski_stochastic_2012} and \textcite{capinski_blackscholes_2012} for further details.

In the following, we implicitly assume that we work in a probability space $(\Omega, \mathcal{F}, P)$ unless stated otherwise. We also restrict ourselves to
%%%%%%%%%%%%%
\comment{maybe mentioning processes here is not needed. You restrict to a time interval.}
%%%%%%%%%%%%%
the time interval $[0,T]$ for some $T$ as it is sufficient in this context.

\tomcomment{Moved assumption about filtered probability space after definition of filtration.}
 %%%%%%%%%%%%%
\comment{we could add a definition of a filtration.}
%%%%%%%%%%%%%

We will denote the Borel subsets of $A$ by $\mathcal{B}(A)$, e.g. $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subets of $\mathbb{R}^d$.

\begin{definition}
  A \textbf{stochastic process} is a measurable function $X : [0,T] \times \Omega \to \mathbb{R}^d$ with respect to the $\sigma$-field $\mathcal{B}([0,T]) \times \mathcal{F}$.
%%%%%%%%%%%%%
%\comment{We should explain the notation $\mathcal{B}([0,T])$}
%%%%%%%%%%%%%
\end{definition}

In the following, when we write $X(t)$ for $t \in [0,T]$, it denotes the random variable $\omega \mapsto X(t, \omega)$.
%%%%%%%%%%%%%
%\comment{Formally $X(t)$ is not defined, since $X : [0,T] \times \Omega \to \mathbb{R}^d$. The choice of the phrase ``Note that ...'' might not be the best.}
%%%%%%%%%%%%%

\begin{definition}
  For any fixed $\omega \in \Omega$, we say that the function $t \mapsto X(t,\omega)$ is a \textbf{path} of the stochastic process $X$.
\end{definition}

\begin{definition}
  A \textbf{filtration} is a family $(\mathcal{F}_t)_{t \in [0,T]}$ of sub-$\sigma$-fields of $\mathcal{F}$ such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $0 \le s < t \le T$.
\end{definition}

From here onwards, we further assume when relevant that we work with a filtration $(\mathcal{F}_t)_{t \in [0,T]}$.

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\}
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$ for any $\mathcal{F}$-measurable random variable $Y$.

\begin{definition}
  We say that a filtration $\mathcal{F}_t$ is \textbf{coarser} than a filtration $\mathcal{G}_t$ if $\mathcal{F}_t \subseteq \mathcal{G}_t$ for all $t \in [0,T]$. Equivalently, we say that $\mathcal{G}_t$ is \textbf{finer} than $\mathcal{F}_t$.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be \textbf{adapted} to a filtration $(\mathcal{F}_t)_{t \in [0,T]}$ if for all $t \in [0,T]$, $X(t)$ is $\mathcal{F}_t$-measurable.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be a \textbf{martingale} for a filtration $\mathcal{F}_t$ if $X(t)$ is integrable for each $t \in [0,T]$ and
  \begin{align*}
    \mathbb{E}(X(t) \mid \mathcal{F}_s) = X(s)
  \end{align*}
  for all $0 \le s < t \le T$.
\end{definition}

The Wiener process is a central stochastic process to stochastic calculus. Here, we give an axiomatic definition. For a construction of such a process and thus a proof of existence, see \textcite{capinski_stochastic_2012}.

\begin{definition}
  A \textbf{Wiener process}, also called Brownian motion, is a stochastic process $(W(t))_{t \in [0,T]}$ that satisfies
  \begin{itemize}
    \item $W(0) = 0$ almost surely,
    \item for all $0 \le s < t \le T$, the increment $W(t) - W(s)$ follows a normal distribution with mean 0 and variance $t - s$,
    \item for all $0 \le t_1 < t_2 < \cdots < t_m$, the increments $W(t_k) - W(t_{k-1}), k=2,\ldots,m$ are independent,
    \item almost all paths are continuous, i.e. $t \mapsto W(t,\omega)$ are continuous functions for almost all $\omega \in \Omega$.
  \end{itemize}
\end{definition}

\begin{definition}
  A \textbf{$d$-dimensional Wiener process} is a stochastic process $\mathbf{W}(t) = (W_1(t), W_2(t), \ldots, W_d(t))$ where $W_j(t), j=1,\ldots,n$ are independent Wiener processes.
\end{definition}

\begin{definition}
  We say that a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ is a \textbf{simple process}, and denote it by $X \in \mathcal{S}^2$, if
  \begin{align*}
    X(t,\omega) = \xi_0 \mathbf{1}_{\{0\}}(t) + \sum_{k=0}^{n-1} \xi_k(\omega) \mathbf{1}_{(t_k,t_{k+1}]}(t)
  \end{align*}
  for some $n > 0$, $0 = t_0 < t_1 < \cdots < t_n = T$ and $\mathcal{F}^W_{t_k}$-measurable random variables $\xi_k$ such that $\mathbb{E}(\xi_k^2) < \infty$ for $k = 0,1,\ldots,n-1$.
\end{definition}

\begin{definition}
  The \textbf{stochastic integral}, also called It\^o integral, of a process $X \in \mathcal{S}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \sum_{k=0}^{n-1} \xi_k (W(t_{k+1}) - W(t_k)).
  \end{align*}
\end{definition}

\begin{definition}
  The set $\mathcal{M}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t \right) < \infty.
  \end{align*}
\end{definition}

\begin{theorem}
%%%%%%%%%%%%%
\comment{maybe this could be a theorem, not proposition.}
%%%%%%%%%%%%%
(\cite{capinski_stochastic_2012}, Theorem 3.4)\label{thm:s2-m2-conv}
  For all $X \in \mathcal{M}^2$, there exists a sequence $(X_n)_{n \ge 1}$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{theorem}

% FIXME: Move comment after definition in a theorem here.

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \lim_{n \to \infty} \int_0^T X_n(t) \mathrm{d}W(t).
  \end{align*}
  for a sequence $(X_n)$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{definition}

The limit in this definition exists and does not in fact depend on which specific sequence $(X_n)$ convergent to $X$ we choose
%%%%%%%%%%%%%
\comment{this is not evident so we could formulate an appropriate theorem and cite a source.}
%%%%%%%%%%%%%
. This together with Theorem \ref{thm:s2-m2-conv}
%%%%%%%%%%%%%
\comment{Proposition \ref{thm:s2-m2-conv} (we should use capital letter.)}
%%%%%%%%%%%%%
ensures that the stochastic integral is well-defined on $\mathcal{M}^2$. See \cite{oksendal_stochastic_2003}, Definition 3.1.6.

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[a,b]$ for $0 \le a < b \le T$ is defined as
  \begin{align*}
    \int_a^b X(t) \mathrm{d}W(t)
    = \int_0^T \mathbf{1}_{[a,b]} X(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.15)\label{prop:stochastic-integral-martingale}
  For all $X \in \mathcal{M}^2$, there exists a martingale $M : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that
  \begin{align*}
    M(t) = \int_0^T \mathbf{1}_{[0,t]}(s) X(s) \mathrm{d}W(s)
  \end{align*}
  almost surely for all $t \in [0,T]$.
\end{proposition}

\begin{definition}\label{def:stochastic-integral-as-process}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ is defined as the process
  \begin{align}\label{eq:stochastic-integral-process}
    \int_0^t X(s) \mathrm{d}W(s) = M(t)
  \end{align}
  for $t \in [0,T]$ where $M$ is the martingale given by Proposition \ref{prop:stochastic-integral-martingale}.
\end{definition}

\begin{theorem}\label{thm:stochastic-integral-expectation-m2}
  (\cite{capinski_stochastic_2012}, Theorem 3.14)
  If $X \in \mathcal{M}^2$, then
  \begin{align*}
    \mathbb{E}\left(\int_0^t X(s) \mathrm{d}s\right) = 0,
  \end{align*}
  for all $t \in [0,T]$.
\end{theorem}

\begin{definition}
  The set $\mathcal{P}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \int_0^T X(t)^2 \mathrm{d}t < \infty
  \end{align*}
  almost surely.
\end{definition}

\begin{proposition}(\cite{capinski_stochastic_2012}, Proposition 4.14 and Theorem 4.16)\label{prop:p2-localising-sequence}
  Let $X \in \mathcal{P}^2$ and let $(X_n)_{n \ge 1}$ be the sequence of stochastic processes given by
  \begin{align*}
    X_n(t) = \mathbf{1}_{[0,\tau_n]}(t) X(t)
  \end{align*}
  with
  \begin{align*}
    \tau_n = \inf \left\{ t \in [0,T] : \int_0^t X(s)^2 \mathrm{d}s \ge n \right\}
  \end{align*}
  where we take $\inf \emptyset = T$.
  Then $X_n \in \mathcal{M}^2$ for all $n$ and the sequence of continuous martingales
  \begin{align*}
    M_n(t) = \int_0^t X_n(s) \mathrm{d}W(s)
  \end{align*}
  as in \eqref{eq:stochastic-integral-process} converges almost surely to a stochastic process $Y$ with continuous paths.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{P}^2$ is the process
  \begin{align*}
    \int_0^t X(s) \mathrm{d}W(s) = \lim_{n \to \infty} M_n(t) = Y(t)
  \end{align*}
  with $M_n$ and $Y$ as in Proposition \ref{prop:p2-localising-sequence}.
\end{definition}

%%%%%%%%%%%%%
\comment{it would seem more natural to give first a definition of a one dimensional Ito process. Possibly setting up one dimensional versions first, and then multidimensional ones could be a choice. I understand though why you have decided to go for the general case first to make things shorter. So, this is just a choice that can be considered. }
%%%%%%%%%%%%%

For the rest of this section, we turn our attention to multidimensional stochastic processes.

\begin{definition}
  The \textbf{stochastic integral} of the $n \times d$ matrix $\mathbf{X} = [X_{ij}]$ of processes in $\mathcal{P}^2$ with respect to a $d$-dimensional Wiener process $\mathbf{W}$ is defined as
  \begin{align*}
    \int_0^t \mathbf{B}(s,\mathbf{X}(s)) \mathrm{d}\mathbf{W}(s) = \left[
      \sum_{j=1}^{n} \int_0^t b_{ij}(s) \mathrm{d}W_j(s)
    \right]_{n=1,\ldots,d}.
  \end{align*}
\end{definition}

If $n=1$, we understand the integral to be the entry as defined above rather than a 1-component vector.

Moreover, we can denote some sums of stochastic integrals with the notation
\begin{align*}
  \int_0^t \mathbf{X}(s) \cdot \mathrm{d}\mathbf{W}(s) = \sum_{j=1}^{d} \int_0^t X_j(s) \mathrm{d}W_j(s)
\end{align*}
for a stochastic process $\mathbf{X}(t) = (X_1(t), \ldots, X_d(t))$ with components in $\mathcal{P}^2$.

\begin{definition}
  We say that a process $\mathbf{X}(t) = (X_1(t), X_2(t), \ldots, X_d(t))$ is an \textbf{It\^o process} if it has the form
  %%%%%%%%%%%%%
\comment{we should force line brakes. Pleas remove all `double slashes'. If something does not fit, possibly it should be centred.}
%%%%%%%%%%%%%
  \begin{align}\label{eq:ito-process}
    X_i(t) = X_i(0) + \int_0^t a_i(s) \mathrm{d}s + \sum_{j=1}^n \int_0^t b_{ij}(s) \mathrm{d}W_j(s)
  \end{align}
  for $i=1,\ldots,d$ where $W_j(t), j=1,\ldots,n$ are $n$ Wiener processes with $\mathbf{W}(t) = (W_1(t),\ldots,W_n(t))$, $a_i(t), i=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes such that $\int_0^T |a_i(t)| \mathrm{d}t < \infty$ and $b_{ij} \in \mathcal{P}^2$ for $i=1,\ldots,d$ and $j=1,\ldots,n$.
\end{definition}

Taking $\mathbf{a}(t) = (a_1(t),\ldots,a_d(t))$, $\mathbf{B}(t) = [b_{ij}(t)]_{i=1,\ldots,d;j=1,\ldots,n}$, we can write \eqref{eq:ito-process} as
\begin{align*}
  \mathbf{X}(t) = \mathbf{X}(0) + \int_0^t \mathbf{a}(s) \mathrm{d}s + \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s)
\end{align*}
or to be even terser, in its so-called \textbf{stochastic differential} notation:
\begin{align*}
  \mathrm{d}\mathbf{X}(t) = \mathbf{a}(t) \mathrm{d}t + \mathbf{B}(t) \mathrm{d}\mathbf{W}(t).
\end{align*}

We call $\mathbf{a}$ the \textbf{drift} and $\mathbf{B}$ the \textbf{volatility}. Together, they are referred to as the \textbf{characteristics} of the It\^o process.

\begin{definition}
  A \textbf{stochastic differential equation} (or \textbf{SDE} for short) is an equation of the form
  \begin{align}\label{eq:sde-init-value}
    \mathbf{X}(t) &= \mathbf{X}(0) + \int_0^t a(s, \mathbf{X}(s)) \mathrm{d}s + \int_0^t b(s, \mathbf{X}(s)) \mathrm{d}\mathbf{W}(s),\\
    \mathbf{X}(0) &= \mathbf{x}_0,\notag
  \end{align}
  for some $a : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^d$, $b : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^{d \times n}$ and $\mathbf{x}_0 \in \mathbb{R}^d$.
\end{definition}

\begin{theorem}\label{thm:sde-solution}
  (\cite{oksendal_stochastic_2003}, Theorem 5.2.1)
  Provided that both the coefficients $a(t,\mathbf{x})$ and $b(t,\mathbf{x})$ satisfy the following conditions where $\|\cdot\|$ denotes the Euclidean norm in the relevant space.
  \begin{itemize}
    \item Linear growth: there exists $C > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x})\| + \|b(t,\mathbf{x})\| \le C (1 + \|\mathbf{x}\|)
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x} \in \mathbb{R}^d$.

    \item Lipschitz continuity: there exists $K > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x}) - a(t,\mathbf{y})\| + \|b(t,\mathbf{x}) - b(t,\mathbf{y})\| \le K \|\mathbf{x}-\mathbf{y}\|
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^d$.
  \end{itemize}
  Then \eqref{eq:sde-init-value} has a unique solution in $\mathcal{M}^2$ with continuous paths.
\end{theorem}

\begin{theorem}
  An It\^o process of the form \eqref{eq:sde-init-value} whose characteristics satisfy the conditions of Theorem \ref{thm:sde-solution}, is called an \textbf{It\^o diffusion}.
\end{theorem}

\begin{theorem}[It\^o Formula]
  (\cite{capinski_blackscholes_2012}, Theorem 6.10)\label{thm:ito-formula}
  Let $F : [0,T] \times \mathbb{R}^d \to \mathbb{R}$ and let $\mathbf{X}(t)$ be a $d$-dimensional It\^o process driven by $n$ independent Wiener processes. For short, we write
  \begin{align*}
    F(t,\mathbf{X}(t)) = F(t,X_1(t),X_2(t),\ldots,X_d(t)).
  \end{align*}
  If $F$ is continuously differentiable in the first argument and twice-continuously differentiable in the others, then $F(t,\mathbf{X}(t))$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}F(t,\mathbf{X}(t))
    &= F_t(t,\mathbf{X}(t)) \mathrm{d}t + \sum_{i=1}^d F_{x_i}(t,\mathbf{X}(t)) a_i(t) \mathrm{d}t\\
    &\ \ \ \ + \sum_{i=1}^d \left(F_{x_i}(t,\mathbf{X}(t)) \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t) \right)\\
    &\ \ \ \ + \frac{1}{2} \sum_{j=1}^n \sum_{i,l=1}^d F_{x_i x_l}(t,\mathbf{X}(t)) b_{ij}(t)b_{lj}(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

\begin{definition}
  Given two It\^o processes $X, Y$ such that $Y$ has stochastic differential
  \begin{align*}
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  the \textbf{stochastic integral} of $X$ with respect to $Y$ is defined as
  \begin{align*}
    \int_0^t X(s) \mathrm{d}Y(s) = \int_0^t X(s) a_Y(s) \mathrm{d}s + \int_0^t X(s) b_Y(s) \mathrm{d}W(s).
  \end{align*}
  We also write
  \begin{align*}
    X(t) \mathrm{d}Y(t) = X(t) a_Y(t) \mathrm{d}t + X(t) b_Y(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

\begin{theorem}[It\^o Product Rule]\label{thm:ito-product-rule}
  (\cite{capinski_stochastic_2012}, Theorem 4.36)
  Given two It\^o processes $X, Y$ with stochastic differential
  \begin{align*}
    \mathrm{d}X(t) &= a_X(t) \mathrm{d}t + b_X(t) \mathrm{d}W(t),\\
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  their product $XY$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}[XY](t) = X(t) \mathrm{d}Y(t) + Y(t) \mathrm{d}X(t) + b_X(t) b_Y(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

\begin{theorem}[Girsanov Theorem]\label{thm:girsanov}
  (\cite{capinski_blackscholes_2012}, Theorem 6.15)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $\theta_j, j=1,\ldots,d$ be $\mathcal{F}^\mathbf{W}_t$-adapted processes such that
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale under $P$ and let $Q$ be the measure with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$, i.e.
  \begin{align*}
    Q(A) = \int_A M(T) \mathrm{d}P
  \end{align*}
  for all $A \in \mathcal{F}$.
  Then the process $\mathbf{W}^Q(t) = (W^Q_1(t), W^Q_2(t), \ldots, W^Q_d(t))$ with
  \begin{align*}
    W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
  \end{align*}
  is a $d$-dimensional Wiener process under $Q$.
\end{theorem}

\begin{theorem}[Novikov Condition]\label{thm:novikov}
  (\cite{karatzas_brownian_1998}, Corollary 5.13)
  If $a_j(t)$, $j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes satisfying
  \begin{align*}
    \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right) < \infty,
  \end{align*}
  then
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.
\end{theorem}

\begin{theorem}[Martingale Representation Theorem]\label{thm:martingale-representation}
  (\cite{oksendal_stochastic_2003}, Theorem 4.3.4)
  Let $\mathbf{W}(t)$ be a $d$-dimensional Wiener process and $M(t)$ be a square-integrable martingale. Then there exists a unique $\mathcal{F}^\mathbf{W}_t$-adapted process $\mathbf{\Gamma}(t)$ such that
  \begin{align*}
    M(t) = M(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}(s)
  \end{align*}
  and such that $\Gamma_i \in \mathcal{M}^2$ for all $i=1,\ldots,d$.
\end{theorem}

\begin{theorem}\label{thm:cond-exp-measurable-independent}
  (\cite{kopp_probability_2013}, Theorem 4.27)
  Let $0 \le s < t \le T$ and $X, Y$ be two random variables such that
  \begin{itemize}
    \item $X$ is $\mathcal{F}^W_s$-measurable and
    \item $Y$ is $\mathcal{F}^W_t$-measurable and independent from $\mathcal{F}^W_s$.
  \end{itemize}
  For any bounded Borel function $f$, we have
  \begin{align*}
    \mathbb{E}(f(X,Y) \mid \mathcal{F}^W_s) = g_f(X),
  \end{align*}
  almost surely, where $g_f$ is a bounded Borel function given by
  \begin{align*}
    g_f(x) = \mathbb{E}(f(x,Y)).
  \end{align*}
\end{theorem}

\begin{definition}
  The \textbf{support} of a random variable $X$ with density $f$ is defined as
  \begin{align*}
    \mathrm{supp}(X) = \{x : f(x) > 0\}.
  \end{align*}
\end{definition}

%%%%%%%%%%%%%
\comment{I do not think that we should manually brake pages. It is best to allow latex to do the formatting.}
%%%%%%%%%%%%%
\section{Multi-asset Black-Scholes model}

In this section, we discuss a market model -- often referred to as the \textit{multi-asset Black-Scholes model with variable coefficients} -- consisting of one risk-free asset with price $A(t)$ satisfying
\begin{align}\label{eq:multi-bs-eq-risk-free}
  \mathrm{d}A(t) = r(t) A(t) \mathrm{d}t, && A(0) = 1,
\end{align}
where $r(t)$ is a deterministic function representing the continuous risk-free rate, and many risky assets with prices $S_i(t), i=1,\ldots,d$ satisfying
\begin{align}
  \mathrm{d}S_i(t) &= \mu_i(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W_j(t), \text{ for } i = 1,\ldots,d.\label{eq:multi-bs-eq}
\end{align}
for $\mathbf{W}(t) = (W_1(t), \ldots, W_d(t))$ a Wiener process with respect to the probability $P$ in the filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$, and some processes $\mu_i, i=1,\ldots,d$ and $c_{ij}, i,j=1,\ldots,d$.

Some further assumptions are made by the model.

\begin{assumption}\label{ass:drift-vol-regularity}
  The processes $\mu_i$ and $c_{ij}$ for $i,j=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted with continuous paths and are bounded by a deterministic constant.
\end{assumption}

\begin{assumption}\label{ass:vol-matrix-invertible}
  The matrix of volatily coefficients $\mathbf{C}(t)= [c_{ij}(t)]_{i,j=1,\ldots,d}$ is invertible.
\end{assumption}

\begin{theorem}\label{thm:bs-solution}
  Equation \eqref{eq:multi-bs-eq} has solution
  \begin{align*}
    S_i(t) &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Let $i = 1,\ldots,d$. By the It\^o Formula (Theorem \ref{thm:ito-formula}) with $F(t,x) = \ln x$ so that $F_t(t,x) = 0, F_x(t,x) = \frac{1}{x}$ and $F_{xx} = \frac{-1}{x^2}$, we have
    \begin{align*}
      \mathrm{d}F(t,S_i(t))
      &= F_t(t,S_i(t)) \mathrm{d}t + F_x(t,S_i(t)) \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + F_x(t,S_i(t)) \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d F_{xx}(t,S_i(t)) (c_{ij}(t) S_i(t))^2 \mathrm{d}t\\
      &= 0 \mathrm{d}t + \frac{1}{S_i(t)} \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + \frac{1}{S_i(t)} \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d \frac{-1}{S_i(t)^2} c_{ij}(t)^2 S_i(t)^2 \mathrm{d}t\\
      &= \mu_i(t) \mathrm{d}t
      - \frac{1}{2} \sum_{j=1}^d c_{ij}(t)^2 \mathrm{d}t
      + \sum_{j=1}^d c_{ij}(t) \mathrm{d}W_j(t).
    \end{align*}

    Expanding the stochastic differential notation into its integral form and since $\ln S_i(t) - \ln S_i(0) = \ln \frac{S_i(t)}{S_i(0)}$, we obtain
    \begin{align*}
      \ln \frac{S_i(t)}{S_i(0)}
      &= \int_0^t \mu_i(s) \mathrm{d}s
      - \frac{1}{2} \sum_{j=1}^d \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s).
    \end{align*}

    Applying the exponential function to both sides yields the required solution.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-solution-risk-free}
  Equation \eqref{eq:multi-bs-eq-risk-free} has the unique solution
  \begin{align*}
    A(t) &= \exp \left( \int_0^t r(s) \mathrm{d}s \right).
  \end{align*}

  \begin{proof}
    We can see \eqref{eq:multi-bs-eq-risk-free} as
    \begin{align*}
      \mathrm{d}A(t) = \mu_0(t) A(t) \mathrm{d}t + \sum_{j=1}^{d} c_{0j}(t) A(t) \mathrm{d}W_j(t)
    \end{align*}
    with $\mu_0(t) = r(t)$ and $c_{0j}(t) = 0$ for all $t \in [0,T]$ so, by the previous theorem,
    \begin{align*}
      A(t) &= A(0) \exp \left( \int_0^t \mu_0(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{0j}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{0j}(s) \mathrm{d}W_j(s) \right)\\
      &= \exp \left( \int_0^t r(s) \mathrm{d}s\right)
    \end{align*}
    substituting $\mu_0$ and $c_{0j}$ and since $A(0) = 1$.
  \end{proof}
\end{theorem}

\subsection{Risk-neutral probability}

\begin{definition}
  A \textbf{risk-neutral probability} is a probability measure $Q$ that is equivalent to $P$ and under which all assets are martingales.
\end{definition}

\begin{definition}
  The \textbf{discounted stock prices} are given by
  \begin{align*}
    \widetilde{S}_i(t) = e^{- \int_0^t r(s)\mathrm{d}s} S_i(t)
  \end{align*}
  for all $i=1,\ldots,d$.
\end{definition}

\begin{lemma}\label{lem:bs-exponential-martingale}
  If $a_j(t), j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes bounded by a deterministic constant $K > 0$, then the stochastic process
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.

  \begin{proof}
    The Novikov condition holds since
    \begin{align*}
      \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right)
      &\le \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\right)\\
      &= \exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\\
      &= \exp \left(\frac{1}{2} d K^2 T \right) < \infty.
    \end{align*}
    Hence, by Theorem \ref{thm:novikov}, $M(t)$ is a martingale.
  \end{proof}
\end{lemma}

\begin{theorem}
  Let $\mathbf{\theta}(t) = [\theta_j(t)]_{j=1,\ldots,d}$ be given by
  \begin{align}\label{eq:bs-def-theta}
    \mathbf{\theta}(t) = \mathbf{C}^{-1}(t) [\mu(t) - r(t)]
  \end{align}
  and let
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right).
  \end{align*}
  If $M(t)$ is a martingale under $P$, then the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$ is a risk-neutral probability, i.e. it is equivalent to $P$ and the processes $\widetilde{S}_i, i=1,\ldots,d$ are martingales under $Q$.
  %%%%%%%%%%%%%
\comment{we should not be making definitions inside of a statement of a theorem.}
%%%%%%%%%%%%%

  \begin{proof}
    For all $i=1,\ldots,d$, by Theorem \ref{thm:bs-solution},
    \begin{align*}
      \widetilde{S}_i(t)
      &= \exp\left(- \int_0^t r(s)\mathrm{d}s\right) S_i(t)\\
      &= S_i(0) \exp \left( \int_0^t (\mu_i(s) - r(s)) \mathrm{d}s \right.\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right).
    \end{align*}

    By \eqref{eq:bs-def-theta}, we have
    \begin{align*}
      \mathbf{\mu}(t) - r(t) = \mathbf{C}(t) \mathbf{\theta}(t)
    \end{align*}
    which in turn implies that
    \begin{align*}
      \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t)
    \end{align*}
    for all $i=1,\ldots,d$.

    Substituting this into the previous equation and by linearity of the integral, we obtain
    \begin{align}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( \sum_{j=1}^d \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\notag\\
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. + \sum_{j=1}^d \left[\int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right]\right).\label{eq:bs-risk-neutral-before-product-rule}
    \end{align}

    Since we assume that $M(t)$ is a martingale, then the Girsanov Theorem \ref{thm:girsanov} implies that the process $\mathbf{W}^Q(t) = (W^Q_1(t), \ldots, W^Q_d(t))$ with
    \begin{align*}
      \mathrm{d}W^Q_j(t) = \theta_j(t) \mathrm{d}t + \mathrm{d}W(t)
    \end{align*}
    is a Wiener process under Q.

    By the It\^o Product Rule (Theorem \ref{thm:ito-product-rule}), we have that for all $j=1,\ldots,d$,
    \begin{align*}
      \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s)
      = c_{ij}(t) \mathrm{d}W^Q(t).
    \end{align*}

    Substituting in equation \eqref{eq:bs-risk-neutral-before-product-rule}, we obtain
    \begin{align}\label{eq:bs-discounted-stock-price}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align}

    By Lemma \ref{lem:bs-exponential-martingale} with $a_j(t) \coloneq - c_{ij}(t)$ and Assumption \ref{ass:drift-vol-regularity}, $\widetilde{S}_i(t)$ is thus a martingale under $Q$ for all $i=1,\ldots,d$ as required.

    Moreover, since
    \begin{align*}
      Q(A) = \int_A M(T) \mathrm{d}P
    \end{align*}
    for all $A \in \mathcal{F}$, then $P(A) = 0$ implies that $Q(A) = 0$. But since $M(T) > 0$, then $Q(A) = 0$ also implies that $P(A) = 0$. That is, the measures $P$ and $Q$ are equivalent.

    In conclusion, we have constructed a measure $Q$ that is indeed a risk-neutral probability for the model.
    %%%%%%%%%%%%%
\comment{the conclusion does not seem justified since we have not defined what is a risk-neutral probability}
%%%%%%%%%%%%%
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-risk-neutral-dynamics}
  The risky assets satisfy
  \begin{align*}
    \mathrm{d}S_i(t) = r(t) S_i(t) \mathrm{d}S_i(t) + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t)
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Taking $r(t)$ as the drift and $\mathrm{W}^Q$ as the Wiener process in Theorem \ref{thm:bs-solution}, we obtain the unique solution for this stochastic differential
    \begin{align*}
      S_i(t)
      &= S_i(0) \exp \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align*}

    Multiplying both sides by the discounted factor $e^{-\int_0^t r(s) \mathrm{d}s}$, we can see that this is equivalent to \eqref{eq:bs-discounted-stock-price} which has been established earlier so the stochastic differential is indeed satisfied by $S_i$ for all $i=1,\ldots,d$.
  \end{proof}
\end{theorem}

\begin{assumption}\label{ass:bs-stock-price-square-integrability}
  The risky assets $S_i$ are assumed to be square-integrable under $Q$ at time $T$, i.e.
  \begin{align*}
    \mathbb{E}_Q(S_i(T)^2) < \infty,
  \end{align*}
  for $i=1,\ldots,d$.
\end{assumption}

\subsection{Strategies}

\begin{definition}
  A (European) \textbf{derivative security} with expiry time $T$ is a security whose value is a stochastic process $H : [0,T] \times \Omega \to \mathbb{R}$ called its \textbf{price process} and such that $H(T) = H_\text{payoff}$ for some $\mathcal{F}^\mathbf{W}_T$-measurable random variable $H_\text{payoff}$ called the \textbf{payoff} of the derivative.
  %%%%%%%%%%%%%
\comment{you could acknowledge that there is some abuse of notation here.}
%%%%%%%%%%%%%
\end{definition}

\begin{definition}
  The \textbf{discounted process} of a process $X$ is defined as
  \begin{align*}
    \widetilde{X}(t) = \frac{X(t)}{A(t)} = e^{-\int_0^t r(s) \mathrm{d}s} X(t).
  \end{align*}
\end{definition}

Note that since $e^{-\int_0^0 r(s) \mathrm{d}s} = 1$, a process agrees with its dicounted process at time 0, i.e. $X(0) = \widetilde{X}(0)$.

\begin{definition}
  The \textbf{extended market} for a derivative $H$ consists of the assets $A$ and $S_i, i=1,\ldots,d$ as described above (the basic market) as well as the derivative $H$.
\end{definition}

\begin{definition}
  A \textbf{strategy} is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form\\ $(x_1(t),\ldots,x_d(t),y(t))$ for $t \in [0,T]$ or simply $(x_1,\ldots,x_d,y)$ for short.
\end{definition}

\begin{definition}
  The \textbf{value (process)} of a strategy $(x_1,\ldots,x_d,y)$ is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y)}(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t).
  \end{align*}
  When there is no ambiguity, we simply denote it by $V(t)$.
\end{definition}

\begin{definition}
  An \textbf{extended strategy}, or a strategy in the extended market, is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form $(x_1,\ldots,x_d,y,z)$. Its value is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y,z)}(t)
    = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t) + z(t) H(t).
  \end{align*}
\end{definition}

\begin{definition}
  We say that a strategy is \textbf{self-financing} if it is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
  \end{align*}
\end{definition}

\begin{definition}
  We say that an extended strategy is \textbf{self-financing} if it is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t) + z(t) \mathrm{d}H(t).
  \end{align*}
\end{definition}

The following definitions apply to strategies as well as extended strategies.

\begin{definition}
  We say that a strategy is \textbf{admissible} if there exists a constant $L > 0$ such that its value process satisfies $V(t) \ge -L$ for all $t \in [0,T]$, almost surely.
\end{definition}

\begin{definition}
  An \textbf{arbitrage opportunity} is an admissible self-financing strategy such that
  \begin{itemize}
    \item $V(0) = 0$,
    \item $V(T) \ge 0$ almost surely and
    \item $V(T) > 0$ with positive probability.
  \end{itemize}
\end{definition}

\begin{remark}
  In these definitions, we do not have to specify under which probability statements hold \textit{almost surely} or \textit{with positive probability}. This is because $P$ and $Q$ are equivalent so saying that a proposition holds almost surely / with positive probability under $P$ is equivalent to saying that it holds under $Q$.
\end{remark}

\begin{assumption}[No-Arbitrage Principle]\label{ass:no-arbitrage-principle}
  There exist no arbitrage opportunities in the extended market.
\end{assumption}

\begin{assumption}\label{ass:bs-derivative-regularity}
  The price process of a derivative security with payoff $H_\text{payoff}$ is an It\^o process $H : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that $H(T) = H_\text{payoff}$ and such that there are no arbitrage opportunities in the extended market.
\end{assumption}

\subsection{Pricing of derivatives}

%%%%%%%%%%%%%
\comment{OK. I will stop commenting but just browse through the content.}
%%%%%%%%%%%%%

\begin{definition}
  A \textbf{replicating strategy} for a derivative with payoff $H_\text{payoff}$ is an admissible self-financing strategy (in the basic market) with value process $V$ such that
  \begin{align*}
    V(T) = H_\text{payoff}
  \end{align*}
  and such that its discounted value process $\widetilde{V}$ is a martingale under $Q$.
\end{definition}

If there exists a replicating strategy for a derivative, we say that the strategy \textbf{replicates} the derivative and that the derivative is \textbf{replicable}.

\begin{lemma}\label{lem:bs-non-negative-strat}
  Let $(x_1, \ldots, x_d, y)$ be a self-financing strategy with value process $V$ that has continuous paths. If $V(T) \ge 0$, then $V(t) \ge 0$ almost surely for all $t \in [0,T]$.

  \begin{proof}
    Assume to the contrary that there is a $t_0 \in [0,T)$ such that $V(t_0) < 0$ with positive probability. We prove this lemma by contradiction by constructing an arbitrage opportunity $(x_1^*, \ldots, x_d^*, y^*)$.

    On the region $A = \{V(t_0) \ge 0\}$, we simply take the strategy $(0,0,\ldots,0)$ and we now focus on the region $A^C = \{V(t_0) < 0\}$.

    For $t < t_0$, we take $x_1^*(t) = \cdots = x_d^*(t) = y^*(t) = 0$. For $t \ge t_0$, we take

    \begin{align*}
      x_i^*(t) &= \left\{ \begin{array}{ll}
        x_i(t) & \text{if } -A(t) < V(t) < 0,\\
        0 & \text{otherwise},
      \end{array}\right. \text{for } i=1,\ldots,d,\\
      y^*(t) &= \left\{ \begin{array}{ll}
        - \frac{V(t_0)}{A(t_0)} - 1 & \text{if } V(t) \le - A(t),\\
        - \frac{V(t_0)}{A(t_0)} + y(t) & \text{if } -A(t) < V(t) < 0,\\
        - \frac{V(t_0)}{A(t_0)} & \text{if } V(t) \ge 0.
      \end{array}\right.
    \end{align*}

    \textbf{Admissibility}: For $t < t_0$, we have $V^*(t) = 0$ and for $t \ge t_0$, we have
      \begin{align*}
        V^*(t)
        &= \sum_{i=1}^d x_i^*(s) S_i(s) + y^*(s) A(s)\\
        &= -\frac{V(t_0)}{A(t_0)} A(t) + \left\{
          \begin{array}{ll}
            -A(t) & \text{if } V(t) \le -A(t),\\
            V(t) & \text{if } -A(t) < V(t) < 0,\\
            0 & \text{if } V(t) \ge 0,
          \end{array}
        \right.\\
        &\ge -\frac{V(t_0)}{A(t_0)} A(t) - A(t)
        \ge - A(t) \tag{$-\frac{V(t_0)}{A(t_0)}A(t)$ > 0}\\
        &\ge -  A(T)
      \end{align*}
      which is a deterministic constant as required. Hence, the strategy $(x^*_1,\ldots,x^*_d,y^*)$ is admissible.

    \textbf{Self-financing condition}:
      For $t < t_0$, the self-financing condition holds since
      \begin{align*}
      V^*(0) &+ \sum_{i=1}^d \int_0^t x_i^*(s)\mathrm{d}S_i(s) + \int_0^t y^*(s) \mathrm{d}A(s)\\
      &= 0 + \sum_{i=1}^d \int_0^t 0 \mathrm{d}S_i(s) + \int_0^t 0 \mathrm{d}A(s)
      = 0 = V^*(t).
      \end{align*}

      At $t = t_0$, we have $V^*(t) = V(t_0) - \frac{V(t_0)}{A(t_0)} A(t_0) = 0$ so the condition also holds at this time.

      For a fixed $\omega \in \Omega$, we can divide the graph of $V(t)$ into three regions where $V(t) < -A(t)$, $-A(t) < V(t) < 0$ or $V(t) \ge 0$ respectively. While the graph of $V(t)$ remains in any of these regions, the strategy only changes as a result of $(x_1,\ldots,x_d,y)$ changing and so the self-financing condition holds since $(x_1,\ldots,x_d,y)$ is itself self-financing. Since both $V(t)$ and $A(t)$ have continuous paths, the graph of $V(t)$ can only cross from one region to another at a point where $V(t) = -A(t)$ or $V(t) = 0$. It is therefore sufficient to verify that the self-financing condition holds at these points. When $V(t) = -A(t)$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = -A(t) - \frac{V(t_0)}{A(t_0)} A(t)\\
        &= \left(- \frac{V(t_0)}{A(t_0)} - 1\right) A(t)
      \end{align*}
      and when $V(t) = 0$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = - \frac{V(t_0)}{A(t_0)} A(t)
      \end{align*}
      so this is indeed the case.

    \textbf{Arbitrage conditions}: As shown above, we have $V^*(0)$. On $A$, $V^*(T) = 0 \ge 0$ and on $A^C$ which has positive probability, we have
    \begin{align*}
      V^*(T) = -\frac{V(t_0)}{A(t_0)} A(T) > 0
    \end{align*}
    since $V(T) \ge 0$. Hence $V^*(T)$ is non-negative almost surely and strictly positive with positive probability as required.

    That is, we have constructed an arbitrage opportunity but this leads to a contradiction by the No-Arbitrage Principle (Assumption \ref{ass:no-arbitrage-principle}), thus completing the proof.
  \end{proof}
\end{lemma}

Note that by inspecting the proof, we can see that this lemma equally applies to strategies in the extended market.

\begin{theorem}\label{thm:bs-repl-strat-derivative-prices}
  If a derivate with payoff $H_\text{payoff}$ is replicable by a strategy with value process $V$ that has continuous paths, then $V(t) = H(t)$ for all $t \in [0,T]$.

  \begin{proof}
    Let $(x_1,\ldots,x_d,y)$ be the replicating strategy with value process $V$. Consider the two strategies in the extended market $(x_1^+,\ldots,x_d^+,y^+,z^+)$ and $(x_1^-,\ldots,x_d^-,y^-,z^-)$ with value process $V^\pm$ and with $x_i^\pm(t) = \pm x_i(t)$ for $i=1,\ldots,d$, $y^\pm(t) = \pm y(t)$ and $z^\pm(t) = (-1) (\pm z(t))$ for all $t \in [0,T]$.  Then their values satisfy $V^+(t) = - V^-(t) = V(t) - H(t)$ for all $t \in [0,T]$.

    Note that since both $V(t)$ and $H(t)$ have continuous paths (by assumption of this theorem and by Assumption \ref{ass:bs-derivative-regularity} respectively), then so do $V^\pm$. Moreover, $V^\pm(T) = V(T) - H(T) = 0 \ge 0$. Hence, by Lemma \ref{lem:bs-non-negative-strat} twice, we have
    \begin{align*}
      V^+(t) \ge 0, && V^-(t) \ge 0,
    \end{align*}
    almost surely for all $t \in [0,T]$. But then $V^+(t) = - V^+(t) \le 0$ so $V^+(t) = 0$.

    In other words, $V(t) - H(t) = 0$ or, equivalently, $V(t) = H(t)$ almost surely for all $t \in [0,T]$ as required.
  \end{proof}
\end{theorem}

  %%%%%%%%%%%%%
\comment{It seems to me that this can be proven. In your email you have written that a proof would be technical. I am wondering if this indeed is the case. We could discuss this.}
%%%%%%%%%%%%%

\begin{theorem}\label{thm:bs-replicability}
  Let $H$ be a derivative with square-integrable payoff. If there exists a deterministic constant $L > 0$ such that $H(T) \ge -L$ almost surely, then the derivative is replicable by a strategy whose value process has continuous paths.

  \begin{proof}
    We construct a replicating strategy $(x_1,\ldots,x_d,y)$ with value process $V$ for the derivative. Since it replicates the derivative, we have that $V(T) = H(T)$ and since a replicating strategy, then its discounted value has to be a $Q$-martingale so, for all $t \in [0,T]$,
    \begin{align*}
      \widetilde{V}(t)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} V(T) \mid \mathcal{F}^\mathbf{W}_t\right)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right).
    \end{align*}

    In particular,
    \begin{align*}
      \widetilde{V}(T)
      &= \mathbb{E}_Q\left(e^{-\int_0^T r(s)\mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_T\right) \\
      &= e^{-\int_0^T r(s)\mathrm{d}s} \mathbb{E}_Q\left(H(T) \mid \mathcal{F}^\mathbf{W}_T\right) \\
      &= e^{-\int_0^T r(s)\mathrm{d}s} H(T)
    \end{align*}
    since $H(T)$ is $\mathcal{F}^\mathbf{W}_T$-measurable. Hence, since $H(T)$ is square-integrable, then so is $\widetilde{V}(T) = e^{-\int_0^T r(s)\mathrm{d}s} H(T)$.

    By the Martingale Representation Theorem \ref{thm:martingale-representation}, there exists $\mathbf{\Gamma}$ such that
    \begin{align}\label{eq:bs-repl-deriv-gamma}
      \widetilde{V}(t)
      &= \widetilde{V}(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}^Q(s).
    \end{align}

    Since $\Gamma_i \in \mathcal{M}^2$, the stochastic integrals have continuous paths by Definition \ref{def:stochastic-integral-as-process} and, since $A(t)$ is also continuous, then $V(t) = A(t) \widetilde{V}(t)$ has continuous paths.

    Note that this process $\widetilde{V}$ exists whether there exists a replicating strategy or not, being the conditional expectation under $Q$ of the discounted payoff, so we are not in fact using the existence of a replicating strategy in order to construct one, however it may seem at first sight.

    In order to make the strategy self-financing, it needs to satisfy
    \begin{align}\label{eq:bs-repl-deriv-self-financing}
      \mathrm{d}V(t) &= \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
    \end{align}
    By Theorem \ref{thm:bs-risk-neutral-dynamics}, we have
    \begin{align*}
      \mathrm{d}S_i(t) = r(t) S_i(t) \mathrm{d}(t) + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t)
    \end{align*}
    and, by the It\^o Formula (Theorem \ref{thm:ito-formula}) with $F(t,x) = \frac{1}{x}$,
    \begin{align}\label{eq:bs-repl-deriv-risk-free-inv}
      \mathrm{d}A(t)^{-1} = -A(t)^{-2} r(t) A(t) \mathrm{d}t = - r(t) A(t)^{-1} \mathrm{d}t,
    \end{align}
    so, by the It\^o Product Rule (Theorem \ref{thm:ito-product-rule}), we obtain
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= A(t)^{-1} \mathrm{d}V(t) + V(t) \mathrm{d}A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) + V(t) \mathrm{d}A(t)^{-1}\tag{by \eqref{eq:bs-repl-deriv-self-financing}}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) - V(t) r(t) A(t)^{-1} \mathrm{d}t \tag{by \eqref{eq:bs-repl-deriv-risk-free-inv}}.
    \end{align*}
    Substituting $V(t)$ and by \eqref{eq:multi-bs-eq-risk-free},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) r(t) A(t) \mathrm{d}t - y(t) A(t) r(t) A(t)^{-1} \mathrm{d}t\\
        &\ \ \ \ - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t.
    \end{align*}
    Then, by Theorem \ref{thm:bs-solution},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d [ A(t)^{-1} x_i(t) r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^d A(t)^{-1} x_i(t) c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t) \\
        &\ \ \ \ \ \ \ \ \ - x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t ]\\
      &= \sum_{i,j=1}^d x_i(t) c_{ij}(t) \widetilde{S}_i(t) \mathrm{d}W^Q_j(t)
      = \mathbf{C}(t) \mathbf{\theta}(t) \cdot \mathrm{d}\mathbf{W}^Q(t)
    \end{align*}
    where $\theta_i(t) = x_i(t) \widetilde{S}_i(t)$ for $i=1,\ldots,d$.

    Both this last equation and \eqref{eq:bs-repl-deriv-gamma} can be satisfied if $\mathbf{C} \mathbf{\theta} = \mathbf{\Gamma}$. This implies that $\mathbf{\theta} = \mathbf{C}^{-1} \mathbf{\Gamma}$. But then,
    \begin{align*}
      x_i = \frac{(\mathbf{C}^{-1} \mathbf{\Gamma})_i}{\widetilde{S}_i},
    \end{align*}
    for all $i=1,\ldots,d$. Moreover, since $V(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t)$, we also have
    \begin{align*}
      y(t) = \frac{1}{A(t)} \left(\sum_{i=1}^d x_i(t) S_i(t) - V(t)\right).
    \end{align*}

    Hence, the strategy constructed above is self-financing with a discounted value process that forms a $Q$-martingale and such that $V(T) = H(T)$. We are only left to show admissibility for that strategy to replicate $H$. But for all $t \in [0,T]$, we have that, almost surely,
    \begin{align*}
      V(t) &= \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right)\\
      &\ge \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} L \mid \mathcal{F}^\mathbf{W}_t\right)
      = e^{-\int_t^T r(s) \mathrm{d}s} L
    \end{align*}
    which is a deterministic constant.

    Therefore, we have constructed a replicating strategy, thus proving that the derivative is indeed replicable. Moreover, we have also shown that its value process has continuous paths.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-derivative-pricing}
  If a derivative $H$ is replicable by a strategy whose value process $V$ has continuous paths, then
  \begin{align}\label{eq:bs-derivative-pricing}
    \widetilde{H}(t) = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t)
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    By Theorem \ref{thm:bs-repl-strat-derivative-prices}, we have that $H(t) = V(t)$ for all $t \in [0,T]$. Multiplying both sides by the discounting factor $\exp\left(-\int_0^t r(s) \mathrm{d}s\right)$, we also have that $\widetilde{H}(t) = \widetilde{V}(t)$ for all $t \in [0,T]$.

    Since $(x_1, \ldots, x_d, y)$ is a replicating strategy, its discounted value $\widetilde{V}$ is a Q-martingale. Therefore,
    \begin{align*}
      \widetilde{H}(t) = \widetilde{V}(t)
      = \mathbb{E}_Q(\widetilde{V}(T) \mid \mathcal{F}^\mathbf{W}_t)
      = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t).
    \end{align*}
  \end{proof}
\end{theorem}

Alternatively, we can write equation \eqref{eq:bs-derivative-pricing} as
\begin{align*}
  H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right)
\end{align*}
for all $t \in [0,T]$. In particular at time 0, since $\mathcal{F}^\mathbf{W}_0$ is the trivial $\sigma$-field, we have
\begin{align*}
  H(0) = \mathbb{E}_Q\left(\widetilde{H}(T)\right)
  = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T)\right).
\end{align*}

\subsection{European options}

\begin{definition}
  The \textbf{positive part function} $(\cdot)^+ : \mathbb{R} \to \mathbb{R}$ is given by
  \begin{align*}
    x^+ = \left\{\begin{array}{ll}
      0 & \text{if } x < 0,\\
      x & \text{if } x \ge 0.
    \end{array}\right.
  \end{align*}
\end{definition}

\begin{definition}
  European \textbf{call and put options} with expiry time $T$, strike price $K > 0$ and underlying $U$ a replicable $\mathcal{F}^\mathbf{W}_t$-adapted process with continuous paths, are derivative securities with payoffs
  \begin{align*}
    H_\text{call} = (U(T) - K)^+, && H_\text{put} = (K - U(T))^+.
  \end{align*}
\end{definition}

\begin{theorem}[Put-call parity]
  Let $C(t)$ and $P(t)$ be the price processes of European call and put options respectively with expiry time $T$, strike prike $K$ and underlying $U$. Then
  \begin{align}\label{eq:put-call-parity}
    C(t) - P(t) = U(t) - e^{-\int_t^T r(s) \mathrm{d}s} K
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    Taking $(x_1^U,\ldots,x_d^U,y^U)$ the strategy with value $V^U$ replicating the underlying $U$.

    A portfolio consisting of one long such call option and one short such put option is equivalent to a derivative with payoff
    \begin{align*}
      (U(T) - K)^+ - (K - U(T))^+
      &= U(T) - K.
    \end{align*}

    We thus can replicate it by the strategy $(x_1,\ldots,x_d,y)$ where
    \begin{align*}
      x_i(t) &= x_i^U(t), \text{ for } i=1,\ldots,d,\\
      y(t) &= y^U(t) - \frac{K}{A(T)}.
    \end{align*}
    Note that $y(t)$ is $\mathcal{F}^\mathbf{W}_t$-adapted since $y^U(t)$ is adapted and $r$ is a deterministic function. The strategy's value at time $T$ is
    \begin{align*}
      V(T) = \sum_{i=1}^d x_i^U(t) S_i(t) + y^U(t) A(t) - \frac{K}{A(T)} A(T)
      = U(T) - K
    \end{align*}

    Hence, Theorem \ref{thm:bs-repl-strat-derivative-prices} implies that equation \eqref{eq:put-call-parity} indeed holds.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-option-pricing}
  A European option with price process $H$ and payoff $H_\text{payoff}$ on an underlying $U$ such that $U(T)$ is square-integrable, is replicable by a strategy whose value process has continuous paths. Moreover,
  \begin{align}\label{eq:bs-option-pricing}
    H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H_\text{payoff} \mid \mathcal{F}^\mathbf{W}_t\right).
  \end{align}

  \begin{proof}
    First note that
    \begin{align*}
      ((U(T) - K)^+)^2 &\le (U(T) - K)^2,\\
      ((K - U(T))^+)^2 &\le (U(T) - K)^2,
    \end{align*}
    so in both cases, we have
    \begin{align*}
      \mathbb{E}_Q(H_\text{payoff}^2)
      \le \mathbb{E}_Q((U(T) - K)^2)
      = \mathbb{E}_Q(U(T)^2) - 2K\mathbb{E}_Q(U(T))) + K^2
      < \infty,
    \end{align*}
    since $U(T) \in L^2(\Omega, Q)$ and hence $U(T) \in L^1(\Omega, Q)$ as well.

    In addition, since the payoff of an option is non-negative, it is bounded below by the deterministic constant $0$. Thus, by Theorem \ref{thm:bs-replicability}, there exists a replicating strategy whose value process has continuous paths.

    By Theorem \ref{thm:bs-derivative-pricing}, the existence of such a strategy in turn implies that \eqref{eq:bs-option-pricing} holds.
  \end{proof}
\end{theorem}

\begin{definition}
  A European \textbf{basket option} is a European option on an underlying of the form
  \begin{align*}
    B_\mathbf{w}(t) = \sum_{i=1}^{d} w_i S_i(t),
  \end{align*}
  with $\mathbf{w} = (w_1, \ldots, w_d)$, called the \textbf{weights}, such that $\sum_{i=1}^{d} w_i = 1$.
\end{definition}

Note that this is well-defined since $U$ is clearly replicable by the constant strategy $(w_1, \ldots, w_d, 0)$.

\begin{theorem}
  Consider a European basket option with underlying $U$ and weights $\mathbf{w}$. Then $U(T)$ is square-integrable.

  \begin{proof}
    By Assumption \ref{ass:bs-stock-price-square-integrability}, $S_i \in L^2(\Omega, Q)$ for all $i=1,\ldots,d$. Since $L^2(\Omega, Q)$ is a vector space over $\mathbb{R}$, the linear combination $U(T) = \sum_{i=1}^{d} w_i S_i(T)$ is also in $L^2(\Omega, Q)$, thus completing the proof.
  \end{proof}
\end{theorem}

\begin{corollary}
  The price process of a European basket option satisfies \eqref{eq:bs-option-pricing}.

  \begin{proof}
    The result immediately follows from the previous theorem together with Theorem \ref{thm:bs-option-pricing}.
  \end{proof}
\end{corollary}

\section{The Fokker-Planck equation}

% Introduce the Fokker-Planck equation. Use [4] as a reference.

This section is based on \textcite{pavliotis_stochastic_2014}. We work in a filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$ without stating it explicitly as to not clutter the argument unecessarily.

\begin{definition}
  A \textbf{Markov process} is a stochastic process $\mathbf{X}(t)$ that satisfies the \textit{Markov condition}
  \begin{align*}
    \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
    = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}),
  \end{align*}
  for $0 \le s < t \le T$ and for all bounded Borel functions $f$.
\end{definition}

We also say that a Markov process is Markov, using the name as an adjective. Informally, the future evolution of a Markov process only depends on its current state, independently from its past evolution.

For $s < t$ and $\Gamma \in \mathcal{B}({\mathbb{R}^d})$, the Markov condition implies that
\begin{align*}
  P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{W}_s)
  &= \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
  = \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  = \phi(\mathbf{X}(s)),
\end{align*}
for some Borel function $\phi$ by the Doob-Dynkin Lemma (\cite{shreve_stochastic_2004}, Lemma 2.1.2). This justifies the following definition since it ensures that it exists.

\begin{definition}
  Let $\mathbf{X}(t)$ be a $d$-dimensional Markov process. A \textbf{transition (probability) function} of $\mathbf{X}$ is a Borel function $\mu(\Gamma, t; \mathbf{x}, s)$ for $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$ and $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ such that
  \begin{align*}
    \mu(\Gamma, t; \mathbf{X}(s), s) = P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{W}_s).
  \end{align*}
\end{definition}

Note that, viewing $\mu(\Gamma, t; \mathbf{x}, s)$ as a function of $\Gamma$ with the other arguments fixed, it forms a probability measure. In the rest of this section, we focus on such transition function that have a density, leading to the following definition.

\begin{definition}
  Let $\mathbf{X}(t)$ be a Markov process such that it has a transition function $\mu(\Gamma,t;\mathbf{x},s)$ that admits a density $\rho$ with respect to the Lebesgue measure, i.e.
  \begin{align*}
    \mu(\Gamma,t;\mathbf{y},s) = \int_\Gamma \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}
  \end{align*}
  for all $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < t \le T$.
  We say that $\rho$ is a \textbf{transition (probability) density} of the Markov process.
\end{definition}

In order to prove the main result of this section, we will make use of the following property of the transition probability density. There exist several versions of this equation including one in terms of the transition function that does not require the existence of a density. However, we only state a single version here for brevity.

\begin{theorem}[Chapman-Kolmogorov Equation]\label{thm:fp-chapman-kolmogorov}
  Let $\mathbf{X}(t)$ be a Markov process with transition probability density $\rho$. Then
  \begin{align*}
    \rho(\mathbf{x}, t; \mathbf{y}, s) = \int_{\mathbb{R}^d} \rho(\mathbf{x}, t; \mathbf{z}, u) \rho(\mathbf{z}, u; \mathbf{y}, s) \mathrm{d}\mathbf{z}
  \end{align*}
  for all $0 \le s < u < t \le T$ and for almost all $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$.

  \begin{proof}
    Let $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < u < t \le T$. Then
    \begin{align*}
      \int_\Gamma \rho(\mathbf{x},t;\mathbf{X}(s),s) \mathrm{d}\mathbf{x}
      &= \mu(\Gamma,t;\mathbf{X}(s),s)
      = P(X(t) \in \Gamma \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_u) \mid \mathcal{F}^\mathbf{W}_s) \tag{tower property}\\
      &= \mathbb{E}(\mu(\Gamma,t;\mathbf{X}(u),u) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \mu(\mathrm{d}\mathbf{z},u;\mathbf{X}(s),s)\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}\\
      &= \int_{\mathbb{R}^d} \int_\Gamma \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{z}\\
      &= \int_\Gamma \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z} \mathrm{d}\mathbf{x},
    \end{align*}
    by Fubini's Theorem (\cite{kopp_probability_2013}, Theorem 3.18) since the integral of a density is finite.

    Since this holds for all $\Gamma$, the integrand must be equal for almost $\mathbf{x}$, i.e.
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{X}(s),s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}.
    \end{align*}

    And since this itself holds for almost all $\omega \in \Omega$, then for almost all $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$ such that $\mathbf{y} = \mathbf{X}(s,\omega)$, we have
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{y},s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{y},s) \mathrm{d}\mathbf{z}.
    \end{align*}
  \end{proof}
\end{theorem}

\begin{lemma}\label{lem:fp-ito-diffusion-indep-increments}
  If $\mathbf{X}$ be an It\^o diffusion with deterministic drift, i.e. satisfying
  \begin{align}\label{eq:fp-ito-diffusion-deterministic-drift}
    \mathrm{d}\mathbf{X}(t) = a(t) \mathrm{d}t + b(t,\mathbf{X}(t)) \mathrm{d}\mathbf{W}(t),
  \end{align}
  where $a$ is a deterministic function, then the increment $\mathbf{X}(t) - \mathbf{X}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$ for all $0 \le s < t \le T$.

  \begin{proof}
    Let $0 \le s < t \le T$. Firstly,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u \mid \mathcal{F}^\mathbf{W}_s\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right) \tag{$a$ deterministic}\\
      &= \int_s^t a(u) \mathrm{d}u + \int_s^s b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \\
      &= \int_s^t a(u) \mathrm{d}u,
    \end{align*}
    since the stochastic integral is a martingale. Indeed, for all $i=1,\ldots,d$, $X_i \in \mathcal{M}^2$ by Theorem \ref{thm:sde-solution} and $b$ is a bounded Borel function so the integrands are also in $\mathcal{M}^2$.

    Secondly,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s))\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u)\right)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\right) \tag{tower property}\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(0\right) \tag{as before}\\
      &= \int_s^t a(u) \mathrm{d}u.
    \end{align*}

    But then,
    \begin{align*}
      \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)
      = \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s)),
    \end{align*}
    so the required independence follows.
  \end{proof}
\end{lemma}

\begin{theorem}\label{thm:local-ito-diffusion-markov}
  If $\mathbf{X}$ be an It\^o diffusion with deterministic drift with equation \eqref{eq:fp-ito-diffusion-deterministic-drift}, then $\mathbf{X}$ is Markov.

  \begin{proof}
    Let $f$ be a bounded Borel function. Take $f^*(x,y) = f(x+y)$. It is a bounded Borel function since $f$ is as well. Since $\mathbf{X}(s)$ is $\mathcal{F}^\mathbf{W}_s$-measurable and $\mathbf{X}(t) - \mathbf{X}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$ by Lemma \ref{lem:fp-ito-diffusion-indep-increments}, then, by Theorem \ref{thm:cond-exp-measurable-independent},
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
      &= \mathbb{E}(f^*(\mathbf{X}(s), \mathbf{X}(t) - \mathbf{X}(s)) \mid \mathcal{F}^\mathbf{W}_s)
      = g_{f^*}(\mathbf{X}(s))
    \end{align*}
    where $g_{f^*}$ is a Borel function.

    In other words, $\mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)$ is $\mathcal{F}_{\mathbf{X}(s)}$-measurable and thus
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
      = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}).
    \end{align*}
    That is, $\mathbf{X}$ is indeed a Markov process.
  \end{proof}
\end{theorem}

We can finally state and prove the main result of this section. Note that it is also called the \textit{Forward Kolmogorov Equation} in other resources.

% FIXME: Explain notation $C^2_0(\mathbb{R}^d)$

\begin{theorem}[Fokker-Planck Equation]\label{thm:fokker-planck}
  Let $\mathbf{X}(t)$ be an It\^o diffusion that is Markov, with transition probability density $\rho(\mathbf{x},t;\mathbf{y},s)$ that is in $C^2$ as a function of $\mathbf{x}$ and $t$, satisfying equations of the form
  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t, \mathbf{X}(t)) \mathrm{d}t + \sum_{j=1}^d b_{ij}(t, \mathbf{X}(t)) \mathrm{d}W_j(t),
  \end{align*}
  for $i=1,\ldots,d$. Then, for all $0 \le s < t \le T$ and $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$, we have
  \begin{align*}
    \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
    &= \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)]\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)].
  \end{align*}

  \begin{proof}
    Let $f \in C^2_0(\mathbb{R}^d)$, let $0 \le s < t \le T$ and let $h$ be such that $0 \le t < t + h \le T$. Then, by the It\^o Formula (Theorem \ref{thm:ito-formula}),
    \begin{align*}
      f(\mathbf{X}&(t+h)) - f(\mathbf{X}(t))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\\
        &\ \ \ \ \ + \sum_{i,j=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) \mathrm{d}W_j(s)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s.
    \end{align*}
    Since $f$ is twice-continuously differentiable, all these partial derivatives are continuous on the closed bounded interval $[0,T]$ and so are the characteristics of $\mathbf{X}$ by definition of It\^o diffusion. Hence, they are bounded by the Boundedness Theorem. Therefore, the integrands are in $\mathcal{M}^2$ and, by Theorem \ref{thm:stochastic-integral-expectation-m2} and by linearity of the expectation,
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\right)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s\right).
    \end{align*}
    Using the boundedness of the integrands once again, by the Fubini Theorem, we have
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s.
    \end{align*}

    Therefore, we have
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \lim_{h \to 0} \frac{1}{h} (\mathbb{E}(f(\mathbf{X}(t+h))) - \mathbb{E}(f(\mathbf{X}(t))))\\
      &= \lim_{h \to 0} \frac{1}{h} \mathbb{E}(f(\mathbf{X}(t+h)) - f(\mathbf{X}(t))) \tag{linearity}\\
      &= \sum_{i=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s\\
      &= \sum_{i=1}^{d} \mathbb{E}(f_{x_i}(\mathbf{X}(t)) a_i(t,\mathbf{X}(t)))\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(t)) b_{ij}(t,\mathbf{X}(t)) b_{lj}(t,\mathbf{X}(t))\right)\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}.
    \end{align*}

    Using the Chapman-Kolmogorov Equation (Theorem \ref{thm:fp-chapman-kolmogorov}), we get
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}
    \end{align*}
    and, by the Fubini Theorem since the integrands are bounded,
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d}  \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.
    \end{align*}

    By integration by parts and since $f$ and its partial derivatives vanish at $\pm \infty$, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}
      &= \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x},
    \end{align*}
    for all $i=1,\ldots,d$. Similarly using integration by parts twice, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) &b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}\\
      &= \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) \frac{\partial}{\partial x_l}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}\\
      &= \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}.
    \end{align*}
    Hence,
    \begin{align}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\notag\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\notag\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \left.\int_{\mathbb{R}^d} \right(\rho(\mathbf{y},s;\mathbf{X}(0),0) \notag\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.\int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \right)\mathrm{d}\mathbf{y}\notag\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \left(\sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \right.\notag\\
        &\ \ \ \ \ + \left.\frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \right) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.\label{eq:fp-proof-1}
    \end{align}

    But
    \begin{align}
      \frac{\partial}{\partial t}\mathbb{E}(f(\mathbf{X}(t)))
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{y}\notag\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\tag{Chapman-Kolmogorov}\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\tag{Fubini}\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\partial}{\partial t}[f(\mathbf{y}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0)] \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \tag{by Theorem \ref{thm:app-deriv-int-swap} since $f\rho \in C^2$}\\
      &= \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\label{eq:fp-proof-2}
    \end{align}
    since $f$ does not depend on $t$.

    But then, since \eqref{eq:fp-proof-1} and \eqref{eq:fp-proof-2} are equal for any choice of $f \in C^2_0(\mathbb{R}^d)$, we have
    \begin{align*}
      \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
      &= \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)]\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)],
    \end{align*}
    for all $\mathbf{y} \in \mathbb{R}^d$ such that $\rho(\mathbf{y},s;\mathbf{X}(0),0) \neq 0$, i.e. for all $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$, thus completing the proof.
  \end{proof}
\end{theorem}

\section{Local volatility model}

% FIXME: Provide counterexample to stock prices being Markov to justify the restriction to a local volatility model.

We restrict the multi-asset Black-Scholes model developed before by making further assumptions about the coefficients $\mu_i$ and $c_{ij}$ for $i=1,\ldots,d$, namely we constrain them to be Borel functions of time and the values of the stock prices. We slightly abuse notation for convenience and write $\mu_i(t) = \mu_i(t,\mathbf{S}(t)), i=1,\ldots,d$ where $\mu_i$ can denote both the process and the Borel function and similarly for $c_{ij}, i,j=1,\ldots,d$. In other words,
\begin{align*}
  \mathrm{d}S_i(t) = \mu_i(t,\mathbf{S}(t)) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t,\mathbf{S}(t)) S_i(t) \mathrm{d}W_j(t)
\end{align*}
for all $i=1,\ldots,d$.

Such a model is called is a \textit{local volatility model} and, as we shall now see, it guarantees that the discounted price of a European basket option is a Markov process.

Taking $u(t,\mathbf{x})$ and $v(t,\mathbf{x})$ such that for all $i,j=1,\ldots,d$,
\begin{align*}
  u_i(t,\mathbf{x}) &= \mu_i(t,\mathbf{x}) x_i,\\
  v_{ij}(t,\mathbf{x}) &= c_{ij}(t,\mathbf{x}) x_i,
\end{align*}
we can express the previous equation as
\begin{align*}
  \mathrm{d}\mathbf{S}(t) = u(t,\mathbf{S(t)}) \mathrm{d}t + v(t,\mathbf{S}(t)) \mathrm{d}\mathbf{W}(t).
\end{align*}

Since $\mu_i$ and $c_{ij}$ are bounded Borel functions for all $i,j=1,\ldots,d$, both $u$ and $v$ are Lipschitz with linear growth with respect to $\mathbf{x}$. Therefore, by Theorem \ref{thm:sde-solution}, we have that the solution $\mathbf{S}(t)$ has continuous paths and is in $\mathcal{M}^2$.

\subsection{European basket options}

\begin{lemma}\label{lem:local-stock-increments}
  Let $B_\mathbf{w}$ be the price process of the underlying of a European basket option with weights $\mathbf{w}$. For any $0 \le s < t \le T$, the increment $\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$.

  \begin{proof}
    First notice that for all $0 \le s < t \le T$,
    \begin{align*}
      \mathbb{E}_Q\left(\widetilde{B}_\mathbf{w}(t) \mid \mathcal{F}^\mathbf{W}_s\right)
      &= \mathbb{E}_Q\left(e^{-\int_0^t r(s) \mathrm{d}s} \sum_{i=1}^{d} w_i S_i(t) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \mathbb{E}_Q\left(\sum_{i=1}^{d} w_i \widetilde{S}_i(t) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \sum_{i=1}^{d} w_i \mathbb{E}_Q\left( \widetilde{S}_i(t) \mid \mathcal{F}^\mathbf{W}_s\right) \tag{linearity}\\
      &= \sum_{i=1}^{d} w_i \widetilde{S}_i(s) \tag{$\widetilde{S}_i, i=1,\ldots,d$ are $Q$-martingales}\\
      &= \widetilde{B}_\mathbf{w}(s),
    \end{align*}
    so $\widetilde{B}_\mathbf{w}$ is a $Q$-martingale.

    Therefore,
    \begin{align*}
      \mathbb{E}_Q(\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s))
      &= \mathbb{E}_Q(\mathbb{E}_Q(\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s) \mid \mathcal{F}^W_s)) \tag{tower property}\\
      &= \mathbb{E}_Q(\mathbb{E}_Q(\widetilde{B}_\mathbf{w}(t) \mid \mathcal{F}^W_s) - \widetilde{B}_\mathbf{w}(s) ) \tag{linearity and $\widetilde{B}_\mathbf{s}$ is $\mathcal{F}^W_s$-measurable}\\
      &= \mathbb{E}_Q(\widetilde{B}_\mathbf{w}(s) - \widetilde{B}_\mathbf{w}(s) ) \tag{martingale}\\
      &= \mathbb{E}_Q(0) = 0.
    \end{align*}

    Moreover,
    \begin{align*}
      \mathbb{E}_Q&\left(\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \mathbb{E}_Q\left(\sum_{i=1}^{d} w_i [\widetilde{S}_i(t) - \widetilde{S}_i(s)] \mid \mathcal{F}^\mathbf{W}_s\right)
      = \sum_{i=1}^{d} w_i \mathbb{E}_Q\left(\widetilde{S}_i(t) - \widetilde{S}_i(s) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \sum_{i=1}^d w_i \mathbb{E}_Q\left(\sum_{j=1}^{d} \int_s^t c_{ij}(u,\mathbf{S}(u)) \mathbf{S}(u) \mathrm{d}W^Q_j(u) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \sum_{i,j=1}^d w_i \mathbb{E}_Q\left(\int_s^t c_{ij}(u,\mathbf{S}(u)) \mathbf{S}(u) \mathrm{d}W^Q_j(u) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \sum_{i,j=1}^d w_i 0 = 0,
    \end{align*}
    since $\mathbf{S} \in \mathcal{M}^2$ so the integrands $c_{ij}(u,\mathbf{S}(u)) S_i(\mathbf{u})$ are as well ($c_{ij}$ is bounded) and the integrals are martingales. Let $M_{ij}(t)$ be such a martingale for $i,j=1,\ldots,d$, we have
    \begin{align*}
      \mathbb{E}_Q\left( \int_s^t c_{ij}(u,\mathbf{S}(u)) \mathbf{S}(u) \mathrm{d}W^Q_j(u) \mid \mathcal{F}^\mathbf{W}_s \right)
      &= \mathbb{E}_Q\left( M_{ij}(t) - M_{ij}(s) \mid \mathcal{F}^\mathbf{W}_s \right)\\
      &= \mathbb{E}_Q\left( M_{ij}(t) \mid \mathcal{F}^\mathbf{W}_s \right) - M_{ij}(s)\\
      &= M_{ij}(s) - M_{ij}(s) = 0.
    \end{align*}

    But then,
    \begin{align*}
      \mathbb{E}_Q(\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s))
      = \mathbb{E}_Q&\left(\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s) \mid \mathcal{F}^\mathbf{W}_s\right),
    \end{align*}
    so $\widetilde{B}_\mathbf{w}(t) - \widetilde{B}_\mathbf{w}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$.
  \end{proof}
\end{lemma}

\begin{definition}
  Let $X$ be an $\mathcal{F}^\mathbf{W}_t$-adapted stochastic process. We denote by $X_\mathbf{S}$ the stochastic process given by
  \begin{align*}
    X_\mathbf{S}(t) = \mathbb{E}_Q(X(t) \mid \mathcal{F}^\mathbf{S}_t).
  \end{align*}
\end{definition}

\begin{lemma}\label{lem:local-proj-martingale}
  Let $X$ be an $\mathcal{F}^\mathbf{W}_t$-adapted stochastic process.
  If $X$ is a martingale with respect to the filtration $\mathcal{F}^\mathbf{W}_t$, then $X_\mathbf{S}$ is a martingale with respect to the filtration $\mathcal{F}^\mathbf{S}_t$.

  \begin{proof}
    Let $0 \le s < t \le T$. Since $\mathbf{S}$ is adapted to $\mathcal{F}^\mathbf{W}_t$, the filtration it generates must be coarser, i.e. $\mathcal{F}^\mathbf{S}_t \subseteq \mathcal{F}^\mathbf{W}_t$ for all $t$. Therefore,
    \begin{align*}
      \mathbb{E}_Q(X_\mathbf{S}(t) \mid \mathcal{F}^\mathbf{S}_s)
      &= \mathbb{E}_Q(\mathbb{E}_Q(X(t) \mid \mathcal{F}^\mathbf{S}_t) \mid \mathcal{F}^\mathbf{S}_s)\\
      &= \mathbb{E}_Q(X(t) \mid \mathcal{F}^\mathbf{S}_s) \tag{tower property}\\
      &= \mathbb{E}_Q(\mathbb{E}_Q(X(t) \mid \mathcal{F}^\mathbf{W}_s) \mid \mathcal{F}^\mathbf{S}_s) \tag{tower property since $\mathcal{F}^\mathbf{S}_s \subseteq \mathcal{F}^\mathbf{W}_s$}\\
      &= \mathbb{E}_Q(X(s) \mid \mathcal{F}^\mathbf{S}_s) \tag{martingale}\\
      &= X_\mathbf{S}(s)
    \end{align*}
    as required.
  \end{proof}
\end{lemma}

In the following, we work in the filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{S}_t, Q)$. We will simply denote $P_\mathbf{S}$ and $C_\mathbf{S}$ by $P$ and $C$ respectively in this setting since the ambiguity is removed.

\begin{theorem}
  The discounted price $\widetilde{P}$ of a European put basket option with weights $\mathbf{w}$ is a Markov process.

  \begin{proof}
    Fix the weights $\mathbf{w}$. We denote $B_\mathbf{w}$ as simply $B$ for convenience. Note that, since $B(t) = \mathbf{w}^T \mathbf{S}(t)$, $B$ is $\mathcal{F}^\mathbf{S}_t$-adapted. Let $f$ be a bounded Borel function and let $0 \le s < t \le T$. We have to prove that
    \begin{align*}
      \mathbb{E}_Q(f(\widetilde{P}(t)) \mid \mathcal{F}^\mathbf{S}_s)
      = \mathbb{E}_Q(f(\widetilde{P}(t)) \mid \mathcal{F}_{\mathbf{S}(s)}).
    \end{align*}

    The payoff of a European put basket option is bounded since $B(T) > 0$ and it is a Borel function of $B(T)$ so it can be expressed as
    \begin{align*}
      P(T) = \theta\left(B(T)\right)
    \end{align*}
    where $\theta : \mathbb{R} \to \mathbb{R}$ is a bounded Borel function.
    Hence, we can express $\widetilde{H}(T)$ as
    \begin{align*}
      \widetilde{P}(T) = \phi(\widetilde{B}(T))
    \end{align*}
    where $\phi(x) = e^{-\int_0^T r(u) \mathrm{d}u}\theta\left(e^{\int_0^T r(u) \mathrm{d}u} x\right)$ is thus also a bounded Borel function since the discounting factor is deterministic and bounded.

    Take $f_1(x,y) = \phi(x + y)$. It is a bounded Borel function such that
    \begin{align*}
      \widetilde{H}(T) = f_1\left(\widetilde{B}(t), \widetilde{B}(T) - \widetilde{B}(t)\right).
    \end{align*}
    The process $\widetilde{B}(t)$ is $\mathcal{F}^\mathbf{S}_t$-measurable and $\widetilde{B}(T) - \widetilde{B}(t)$ is independent from $\mathcal{F}^\mathbf{W}_t$ by Lemma \ref{lem:local-stock-increments} so it is also independent from $\mathcal{F}^\mathbf{S}_t$ since $\mathcal{F}^\mathbf{S}_t \subseteq \mathcal{F}^\mathbf{W}_t$. Hence, by Theorem \ref{thm:cond-exp-measurable-independent}, we have
    \begin{align}
      \widetilde{H}(t)
      &= \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{S}_t) \tag{by Lemma \ref{lem:local-proj-martingale}}\\
      &= \mathbb{E}_Q\left(f_1\left(\widetilde{B}(t), \widetilde{B}(T) - \widetilde{B}(t)\right) \mid \mathcal{F}^\mathbf{S}_t \right)
      = g_{f_1}\left(\widetilde{B}(t)\right),\label{eq:local-put-markov-f1}
    \end{align}
    where $g_{f_1}$ is a bounded Borel function.

    Next, take $f_2(x,y) = f(g_{f_1}(x+y))$. It is bounded since $f$ is bounded and Borel since both $f$ and $g_{f_1}$ are Borel. Similarly to the previous step, using Theorem \ref{thm:cond-exp-measurable-independent}, we obtain
    \begin{align*}
      \mathbb{E}_Q\left(f\left(\widetilde{H}(t)\right) \mid \mathcal{F}^\mathbf{S}_s\right)
      &= \mathbb{E}_Q\left(f\left(g_{f_1}\left(\widetilde{B}(t)\right)\right) \mid \mathcal{F}^\mathbf{S}_s\right) \tag{by \eqref{eq:local-put-markov-f1}}\\
      &= \mathbb{E}_Q\left(f_2\left(\widetilde{B}(s), \widetilde{B}(t) - \widetilde{B}(s)\right) \mid \mathcal{F}^\mathbf{S}_s\right)\\
      &= g_{f_2}\left(\widetilde{B}(s)\right).
    \end{align*}

    But then, $\mathbb{E}_Q(f(\widetilde{H}(t)) \mid \mathcal{F}^\mathbf{S}_s)$ is $\mathcal{F}_{\widetilde{B}(s)}$-measurable. Furthermore, since $\widetilde{B}(s)$ is a function of $\mathbf{S}(s)$, then $\mathcal{F}_{\widetilde{B}(s)} \subseteq \mathcal{F}_{\mathbf{S}(s)}$ and $\mathbb{E}_Q(f(\widetilde{H}(t)) \mid \mathcal{F}^\mathbf{S}_s)$ is thus $\mathcal{F}_{\mathbf{S}(s)}$-measurable. That is,
    \begin{align*}
      \mathbb{E}_Q(f(\widetilde{H}(t)) \mid \mathcal{F}^\mathbf{S}_s)
      = \mathbb{E}_Q(f(\widetilde{H}(t)) \mid \mathcal{F}_{\mathbf{S}(s)})
    \end{align*}
    as required.
  \end{proof}
\end{theorem}

% FIXME: Provide counterexample to European basket options being Markov even in a local volatility model under the filtration generated by S too.

\begin{theorem}
  The discounted price $\widetilde{C}$ of a European call basket option with weights $\mathbf{w}$ is a Markov process.

  \begin{proof}
    FIXME
  \end{proof}
\end{theorem}

FIXME: Option prices are It\^o diffusions => they satisfy Fokker-Planck equation?

\section{Dupire's equation}

% Give a detailed derivation of the Dupire’s equation (equation starting with ∂C = on page 171 in [2]). ∂T
% Use section 2 from [2] and the section ‘The continuous time theory’ from [3] as a source for the proof.

FIXME

\section{Generalisation to multiple assets}

% Provide the setup and give a detailed proof of Theorem 1 from [2]. This should be based on section 3 from [2]

FIXME

\section{Alternative proof}

% Present the alternative proof of Theorem 1, anded on Appendix A from [2].

FIXME

\section{Numerical example}

% Give a numerical example of how Theorem 1 can be applied to recover aij. You can restrict to the simplest setting of a two dimensional Black-Scholes model.

FIXME

\appendix
\section{Appendix}

\begin{theorem}\label{thm:app-deriv-int-swap}
  If $f : \mathbb{R}^2 \to \mathbb{R}$ is twice-continuously differentiable, then for all $a < b$, we have
  \begin{align*}
    \frac{\partial}{\partial t} \int_a^b f(t,x) \mathrm{d}x
    &= \int_a^b \frac{\partial}{\partial t} f(t,x) \mathrm{d}x.
  \end{align*}

  \begin{proof}
    Let $F$ be an antiderivative of $f$ with respect to $x$, i.e. $F_x(t,x) = f(t,x)$. Since $f \in C^2$, then by Scharz's Theorem and the Fundamental Theorem of Calculus, we have
    \begin{align*}
      \frac{\partial}{\partial t} \int_a^b f(t,x) \mathrm{d}x
      &= \frac{\partial}{\partial t} (F(t,b) - F(t,a))
      = \frac{\partial}{\partial t}F(t,b) - \frac{\partial}{\partial t}F(t,a)\\
      &= \int_a^b \frac{\partial^2}{\partial t \partial x}F(t,x) \mathrm{d}x\\
      &= \int_a^b \frac{\partial^2}{\partial x \partial t}F(t,x) \mathrm{d}x \tag{Schwarz}\\
      &= \int_a^b \frac{\partial}{\partial t}f(t,x) \mathrm{d}x
    \end{align*}
    as required.
  \end{proof}
\end{theorem}

\pagebreak
\printbibliography

\end{document}
