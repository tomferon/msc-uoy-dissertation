%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage{color}
\definecolor{note_fontcolor}{rgb}{0.800781, 0.800781, 0.800781}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter

\newcommand{\comment}[1]{\color{blue}#1\color{black}}
\newcommand{\tomcomment}[1]{\color{orange}#1\color{black}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\DeclareRobustCommand*{\lyxarrow}{%
\@ifstar
{\leavevmode\,$\triangleleft$\,\allowbreak}
{\leavevmode\,$\triangleright$\,\allowbreak}}
%% The greyedout annotation environment
\newenvironment{lyxgreyedout}
  {\textcolor{note_fontcolor}\bgroup\ignorespaces}
  {\ignorespacesafterend\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\newtheoremstyle{bolddesc}{}{}{\itshape}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{bolddesc}
\newtheorem{theorem}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[theorem]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{assumption}[theorem]{\protect\assumptionname}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\usepackage{amsfonts}
\newcommand{\commentMJC}[1]{{\color{red}#1}}

\makeatother

\providecommand{\assumptionname}{Assumption}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Title}
\author{Thomas Feron}
\date{~}

\maketitle
\vspace{2.5in}

\noindent \begin{center}
Dissertation submitted for the MSc in Mathematical Finance
\par\end{center}

\begin{center}
\bigskip{}
\par\end{center}

\begin{center}
Department of Mathematics

University of York\bigskip{}
\par\end{center}

\begin{center}
\today
\par\end{center}

\vspace{1in}

\begin{center}
Supervisor: Maciej J. Capi\'nski
\par\end{center}

\newpage{}

\tableofcontents{}\newpage{}

\pagebreak
%%%%%%%%%%%%%
\comment{I have moved your latex file to the UoY MSc latex template.}
%%%%%%%%%%%%%

%%%%%%%%%%%%%
\comment{Best not to use the section* environments. All sections should be numbered.}
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\comment{It would be best to send me sections which are finished or at least close to finished. What is here seems to be far from it, I am skipping it.}
%%%%%%%%%%%%%


FIXME: Quick intro to the topic and explanation of what is coming.
%%%%%%%%%%%%%
\comment{Above was without preparation or context. Was this supposed to be some kind of introduction? Seems rather heavy for an introduction. I am not sure how a ``Quick intro to the topic and explanation of what is coming" can incorporate the above.}
%%%%%%%%%%%%%

\pagebreak
\section{Preliminaries}

% Develop a preliminaries section, in which the tools of stochastic Ito calculus are introduced. Assume that your reader has background in the standard undergraduate modules in mathematics (including analysis, measure theory, probability theory and statistics), but introduce all the notions concerning stochastic processes from scratch. This section should not contain any proofs. Simply set up and state the needed results, providing citations to sources. While developing this section, restrict to the setup needed for the model from [2].

This section establishes the preliminaries of stochastic calculus required by further sections without proofs. See \textcite{capinski_stochastic_2012} and \textcite{capinski_blackscholes_2012} for further details.

In the following, we implicitly assume that we work in a probability space $(\Omega, \mathcal{F}, P)$ unless stated otherwise. We also restrict ourselves to processes defined on a time interval $[0,T]$ for some $T$ as it is sufficient in this context. When relevant, we further assume that we work with a filtration $(\mathcal{F}_t)_{t \in [0,T]}$.

We will denote the Borel subsets of $A$ by $\mathcal{B}(A)$, e.g. $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subets of $\mathbb{R}^d$.

\begin{definition}
  A \textbf{stochastic process} is a measurable function $X : [0,T] \times \Omega \to \mathbb{R}^d$ with respect to the $\sigma$-field $\mathcal{B}([0,T]) \times \mathcal{F}$.
%%%%%%%%%%%%%
\comment{We should explain the notation $\mathcal{B}([0,T])$}
%%%%%%%%%%%%%
\end{definition}

In the following, when we write $X(t)$ for $t \in [0,T]$, it denotes the random variable $\omega \mapsto X(t, \omega)$.
%%%%%%%%%%%%%
\comment{Formally $X(t)$ is not defined, since $X : [0,T] \times \Omega \to \mathbb{R}^d$. The choice of the phrase ``Note that ...'' might not be the best.}
%%%%%%%%%%%%%

\begin{definition}
  A \textbf{filtration} is a family $(\mathcal{F}_t)_{t \in [0,T]}$ of sub-$\sigma$-fields of $\mathcal{F}$ such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $0 \le s < t \le T$.
\end{definition}

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\}
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$ for any $\mathcal{F}$-measurable random variable $Y$.

%%%%%%%%%%%%%
\comment{small but {\bf important} comment: We must be aware how we structure paragraphs. Compare your three paragraphs to `my' single paragraph below:

...

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\}
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$ for any $\mathcal{F}$-measurable random variable $Y$.

...

The important issue is that each empty line in \LaTeX\, creates a new paragraph. We must be aware of this and use this on purpose.
}
%%%%%%%%%%%%%

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be \textbf{adapted} to a filtration $(\mathcal{F}_t)_{t \in [0,T]}$ if for all $t \in [0,T]$, $X(t)$ is $\mathcal{F}_t$-measurable.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be a \textbf{martingale} for a filtration $\mathcal{F}_t$ if $X(t)$ is integrable for each $t \in [0,T]$ and
  \comment{We have redundant line brakes, which spoil formatting. Please correct throughout the draft.}
  \begin{align*}
    \mathbb{E}(X(t) \mid \mathcal{F}_s) = X(s)
  \end{align*}
  for all $0 \le s < t \le T$.
\end{definition}

A very important stochastic process is the Wiener process. Here, we give an axiomatic definition. For a construction of such a process and thus a proof of existence, see \textcite{capinski_stochastic_2012}.

\begin{definition}
  A \textbf{Wiener process}, also called Brownian motion, is a stochastic process $(W(t))_{t \in [0,T]}$ that satisfies
  \begin{itemize}
    \item $W(0) = 0$ almost surely,
    \item for all $0 \le s < t \le T$, the increment $W(t) - W(s)$ follows a normal distribution with mean 0 and variance $t - s$,
    \item for all $0 \le t_1 < t_2 < \cdots < t_m$, the increments $W(t_k) - W(t_{k-1}), k=2,\ldots,m$ are independent,
    \item almost all paths are continuous, i.e. $t \mapsto W(t,\omega)$ are continuous functions for almost all $\omega \in \Omega$.
  \end{itemize}
\end{definition}

\begin{definition}
  A \textbf{$d$-dimensional Wiener process} is a stochastic process $\mathbf{W}(t) = (W_1(t), W_2(t), \ldots, W_d(t))$ where $W_j(t), j=1,\ldots,n$ are independent Wiener processes.
\end{definition}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{at this point we have not written out what $\mathcal{S}^2$ and $ \mathcal{M}^2$ are, so the reader cannot see that $\mathcal{S}^2 \subset \mathcal{M}^2$}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{Each equation should be part of a sentence, below we have equations which are detached from the text.}
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\begin{definition}
  We say that a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ is a \textbf{simple process}, and denote it by $X \in \mathcal{S}^2$, if
  \begin{align*}
    X(t,\omega) = \xi_0 \mathbf{1}_{\{0\}}(t) + \sum_{k=0}^{n-1} \xi_k(\omega) \mathbf{1}_{(t_k,t_{k+1}]}(t)
  \end{align*}
  for some $n > 0$, $0 = t_0 < t_1 < \cdots < t_n = T$ and $\mathcal{F}^W_{t_k}$-measurable random variables $\xi_k$ such that $\mathbb{E}(\xi_k^2) < \infty$ for $k = 0,1,\ldots,n-1$.
\end{definition}
\comment{Above we should have $\mathbf{1}_{\{0\}}(t)$ instead of $\mathbf{1}_0$. (I know that this comes from the book, but it seems that there is a typo in the book.)}

%%%%%%%%%%%%%
\begin{definition}
  The \textbf{stochastic integral}, also called It\^o integral, of a process $X \in \mathcal{S}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \sum_{k=0}^{n-1} \xi_k (W(t_{k+1}) - W(t_k)).
  \end{align*}
\end{definition}

\begin{definition}
  The set $\mathcal{M}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t \right) < \infty.
  \end{align*}
\end{definition}
\comment{Which filtration $\mathcal{F}_t$ is used here? Arbitrary filtration?}

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.4)\label{prop:s2-m2-conv}
%%%%%%%%%%%%%
\comment{citation missing. Please add a citation to a bibliography item in every theorem, lemma and proposition in the draft.}
%%%%%%%%%%%%%
  For all $X \in \mathcal{M}^2$, there exists a sequence $(X_n)_{n \ge 1}$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \lim_{n \to \infty} \int_0^T X_n(t) \mathrm{d}W(t).
  \end{align*}
  for a sequence $(X_n)$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{definition}

The limit in this definition exists and does not in fact depend on which specific sequence $(X_n)$ convergent to $X$ we choose. This together with proposition \ref{prop:s2-m2-conv}
%%%%%%%%%%%%%
\comment{Proposition \ref{prop:s2-m2-conv} (here you are referencing a proposition not an equation)}
%%%%%%%%%%%%%
ensures that the stochastic integral is well-defined on $\mathcal{M}^2$. See \cite{oksendal_stochastic_2003}, Definition 3.1.6.

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.15)\label{prop:stochastic-integral-martingale}
  For all $X \in \mathcal{M}^2$, there exists a martingale $M : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that
  \begin{align*}
    M(t) = \int_0^T \mathbf{1}_{[0,t]}(s) X(s) \mathrm{d}W(s)
  \end{align*}
  almost surely for all $t \in [0,T]$.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ is defined as the process
  \begin{align}\label{eq:stochastic-integral-process}
    \int_0^t X(s) \mathrm{d}W(s) = M(t)
  \end{align}
  for $t \in [0,T]$ where $M$ is the martingale given by proposition \ref{prop:stochastic-integral-martingale}.
\end{definition}

\begin{definition}
  The set $\mathcal{P}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \int_0^T X(t)^2 \mathrm{d}t < \infty
  \end{align*}
  almost surely.
\end{definition}

\begin{proposition}(\cite{capinski_stochastic_2012}, Proposition 4.14 and Theorem 4.16)\label{prop:p2-localising-sequence}
  Let $X \in \mathcal{P}^2$ and let $(X_n)_{n \ge 1}$ be the sequence of stochastic processes given by
  \begin{align*}
    X_n(t) = \mathbf{1}_{[0,\tau_n]}(t) X(t)
  \end{align*}
  with
  \begin{align*}
    \tau_n = \inf \left\{ t \in [0,T] : \int_0^t X(s)^2 \mathrm{d}s \ge n \right\}
  \end{align*}
  where we take $\inf \emptyset = T$.
  Then $X_n \in \mathcal{M}^2$ for all $n$ and the sequence of continuous martingales
  \begin{align*}
    M_n(t) = \int_0^t X_n(s) \mathrm{d}W(s)
  \end{align*}
  as in \eqref{eq:stochastic-integral-process} converges almost surely to a stochastic process $Y$ with continuous paths.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{P}^2$ is the process
  \begin{align*}
    \int_0^t X(s) \mathrm{d}W(s) = \lim_{n \to \infty} M_n(t) = Y(t)
  \end{align*}
  with $M_n$ and $Y$ as in proposition \ref{prop:p2-localising-sequence}.
\end{definition}

%%%%%%%%%%%%%
\comment{Better to formulate this as a theorem and give a citation:}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{this sentence was not clear. (Side issue, why are we discussing this; is this important; will we use this fact later?)}
%%%%%%%%%%%%%

%%%%%%%%%%%%%
\comment{I am not quite sure if I understand the below FIXMEs. }
%%%%%%%%%%%%%

\begin{definition}
  An \textbf{It\^o process} is a stochastic process\\ $\mathbf{X}(t) = (X_1(t), X_2(t), \ldots, X_d(t))$ of the form
  \begin{align}\label{eq:ito-process}
    X_i(t) = X_i(0) + \int_0^t a_i(s) \mathrm{d}s + \sum_{j=1}^n \int_0^t b_{ij}(s) \mathrm{d}W_j(s), \text{ for } i=1,\ldots,d
  \end{align}
  where $W_j(t), j=1,\ldots,n$ are $n$ Wiener processes with $\mathbf{W}(t) = (W_1(t),\ldots,W_n(t))$, $a_i(t), i=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes such that $\int_0^T |a_i(t)| \mathrm{d}t < \infty$ and $b_{ij} \in \mathcal{P}^2$ for $i=1,\ldots,d$ and $j=1,\ldots,n$.
\end{definition}

  %%%%%%%%%%%%%
\comment{The definition is incomplete. We must specify the conditions required by $a,b$.}
%%%%%%%%%%%%%

Taking $\mathbf{a}(t) = (a_1(t),\ldots,a_d(t))$, $\mathbf{B}(t) = [b_{ij}(t)]_{i=1,\ldots,d;j=1,\ldots,n}$ and
\begin{align*}
  \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s) = \left[
    \sum_{j=1}^{n} \int_0^t b_{ij}(s) \mathrm{d}W_j(s)
  \right]_{i=1,\ldots,d},
\end{align*}
we can write \eqref{eq:ito-process} as
%%%%%%%%%%%%%
\comment{broken label }
%%%%%%%%%%%%%
\begin{align*}
  \mathbf{X}(t) = \mathbf{X}(0) + \int_0^t \mathbf{a}(s) \mathrm{d}s + \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s)
\end{align*}
or to be even terser, in its so-called \textbf{stochastic differential} notation:
\begin{align*}
  \mathrm{d}\mathbf{X}(t) = \mathbf{a}(t) \mathrm{d}t + \mathbf{B}(t) \mathrm{d}\mathbf{W}(t).
\end{align*}

When $d=n=1$, we simply write
\begin{align*}
  \mathrm{d}X(t) = a(t) \mathrm{d}t + b(t) \mathrm{d}W(t)
\end{align*}
and we call $a, b$ the \textbf{characteristics} of the It\^o process.

\begin{definition}
  A \textbf{stochastic differential equation} (or \textbf{SDE} for short) is an equation of the form
  \begin{align}\label{eq:sde-init-value}
    \mathbf{X}(t) &= \mathbf{X}(0) + \int_0^t u(s, \mathbf{X}(s)) \mathrm{d}s + \int_0^t v(s, \mathbf{X}(s)) \mathrm{d}\mathbf{W}(s),\\
    \mathbf{X}(0) &= \mathbf{x}_0\notag
  \end{align}
  for some $u : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^d$, $v : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^{d \times n}$ and $\mathbf{x}_0 \in \mathbb{R}^d$.
\end{definition}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{why does the sentence end? }
\comment{What do we mean by ``consider''? Are we defining what is an SDE or should the reader know this. This is unclear from the context.}
%%%%%%%%%%%%%

\begin{theorem}(\cite{capinski_blackscholes_2012}, Theorem 6.9)\label{thm:sde-solution}
  Provided that both the coefficients $u(t,\mathbf{x})$ and $v(t,\mathbf{x})$ satisfy the following conditions where $\|\cdot\|$ denotes the Euclidean norm in the relevant space.
  \begin{itemize}
    \item Linear growth: there exists $C > 0$ such that
      \begin{align*}
        \|u(t,\mathbf{x})\| + \|v(t,\mathbf{x})\| \le C (1 + \|\mathbf{x}\|)
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x} \in \mathbb{R}^d$.

    \item Lipschitz continuity: there exists $K > 0$ such that
      \begin{align*}
        \|u(t,\mathbf{x}) - u(t,\mathbf{y})\| + \|v(t,\mathbf{x}) - v(t,\mathbf{y})\| \le K \|\mathbf{x}-\mathbf{y}\|
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^d$.
  \end{itemize}
  Then \eqref{eq:sde-init-value} has a unique solution.
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{``to derive new SDEs from old ones" does not sound accurate.}
%%%%%%%%%%%%%

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{citation missing}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
  %%%%%%%%%%%%%
\comment{we could explain what is $C^{1,2}$}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
  %%%%%%%%%%%%%
\comment{the statement as made here requires the Ito processes to involve Ito integrals in $\mathcal{P}^2$.}
%%%%%%%%%%%%%

\begin{theorem}[It\^o formula]
  (\cite{capinski_blackscholes_2012}, Theorem 6.10)\label{thm:ito-formula}
  Let $F : [0,T] \times \mathbb{R}^d \to \mathbb{R}$ and let $\mathbf{X}(t)$ be a $d$-dimensional It\^o process driven by $n$ independent Wiener processes. For short, we write
  \begin{align*}
    F(t,\mathbf{X}(t)) = F(t,X_1(t),X_2(t),\ldots,X_d(t)).
  \end{align*}
  If $F$ is continuously differentiable in the first argument and twice-continuously differentiable in the others, then $F(t,\mathbf{X}(t))$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}F(t,\mathbf{X}(t))
    &= F_t(t,\mathbf{X}(t)) \mathrm{d}t + \sum_{i=1}^d F_{x_i}(t,\mathbf{X}(t)) a_i(t) \mathrm{d}t\\
    &\ \ \ \ + \sum_{i=1}^d \left(F_{x_i}(t,\mathbf{X}(t)) \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t) \right)\\
    &\ \ \ \ + \frac{1}{2} \sum_{j=1}^n \sum_{i,l=1}^d F_{x_i x_l}(t,\mathbf{X}(t)) b_{ij}(t)b_{lj}(t) \mathrm{d}t.
  \end{align*}
  %%%%%%%%%%%%%
\comment{again, likely we need Ito integral on $\mathcal{P}^2$.}
%%%%%%%%%%%%%
\end{theorem}

\begin{definition}
  Given two It\^o processes $X, Y$ such that $Y$ has stochastic differential
  \begin{align*}
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  the \textbf{stochastic integral} of $X$ with respect to $Y$ is defined as
  \begin{align*}
    \int_0^t X(s) \mathrm{d}Y(s) = \int_0^t X(s) a_Y(s) \mathrm{d}s + \int_0^t X(s) b_Y(s) \mathrm{d}W(s).
  \end{align*}
  We also write
  \begin{align*}
    X(t) \mathrm{d}Y(t) = X(t) a_Y(t) \mathrm{d}t + X(t) b_Y(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

\begin{theorem}[It\^o product rule]\label{thm:ito-product-rule}
  (\cite{capinski_stochastic_2012}, Theorem 4.36)
  Given two It\^o processes $X, Y$ with stochastic differential
  \begin{align*}
    \mathrm{d}X(t) &= a_X(t) \mathrm{d}t + b_X(t) \mathrm{d}W(t),\\
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  their product $XY$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}[XY](t) = X(t) \mathrm{d}Y(t) + Y(t) \mathrm{d}X(t) + b_X(t) b_Y(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{Why ``starting''? There does not seem to be the multidimensional one later on. }
%%%%%%%%%%%%%
\tomcomment{[deleted]}

%%%%%%%%%%%%%
\comment{We need some introduction and setup to the financial statements that follow. So far we discussed stochastic processes and change of topic is somewhat sudden.}
%%%%%%%%%%%%%
\tomcomment{[deleted]}

%%%%%%%%%%%%%
\comment{I in fact think that the Girsanov theorem is a mathematical statement, which does not necessarily require a financial introduction.}
%%%%%%%%%%%%%

\begin{theorem}[Girsanov theorem]\label{thm:girsanov}
  (\cite{capinski_blackscholes_2012}, Theorem 6.15)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $\theta_j, j=1,\ldots,d$ be $\mathcal{F}^\mathbf{W}_t$-adapted processes such that
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale under $P$ and let $Q$ be the measure with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$, i.e.
  \begin{align*}
    Q(A) = \int_A M(T) \mathrm{d}P
  \end{align*}
  for all $A \in \mathcal{F}$.
  Then the process $\mathbf{W}^Q(t) = (W^Q_1(t), W^Q_2(t), \ldots, W^Q_d(t))$ with
  \begin{align*}
    W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
  \end{align*}
  is a $d$-dimensional Wiener process under $Q$.
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{Such statements, without preparation, are not helpful. I would skip this. This does not look necessary.}
%%%%%%%%%%%%%

\begin{theorem}[Novikov condition]\label{thm:novikov}
  (\cite{karatzas_brownian_1998}, Corollary 5.13)
  If $a_j(t), j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes satisfying
  \begin{align*}
    \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right) < \infty,
  \end{align*}
  then
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.

\end{theorem}

\begin{theorem}[Martingale representation theorem]
  (\cite{shreve_stochastic_2004}, Theorem 5.4.2)
  Let $\mathbf{W}(t)$ be a $d$-dimensional Wiener process and $M(t)$ be a martingale. Then there is an $\mathcal{F}^\mathbf{W}_t$-adapted
  %%%%%%%%%%%%%
\comment{adapted to which filtration? (The filtration could be specified at the beginning.)}
%%%%%%%%%%%%%
   $d$-dimensional process $\mathbf{\Gamma}(t)$ such that
  \begin{align*}
    M(t) = M(0) + \sum_{j=1}^{d} \int_0^t \Gamma_j(s) \mathrm{d}W_j(s).
  \end{align*}
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{Are you sure that above is true? To me it would seem that
\[XdW^Q = X\theta dt + X dW.\]

Take $X=W$ and $W^Q=W$ (i.e. $a=0,b=1,\theta=0$). Then my equation, which I believe to be true, gives
\[WdW^Q = W\theta dt + W dW=WdW,\]
but yours gives
\[WdW^Q =  0 dt + 1 dW = dW.\]
This looks incorrect, since then we would have $WdW=dW$. I think that what you have stated here is not true.
}

\pagebreak
\section{Multi-asset Black-Scholes model}

% Introduce the multidimensional Black-Scholes model. Use [1] as a reference.

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{sentence should not stop here.}
%%%%%%%%%%%%%
\tomcomment{[deleted]}

%%%%%%%%%%%%%
\comment{why are we discussing the one dimensional model? The section title talks of the multi-asset Black-Scholes model. }
%%%%%%%%%%%%%

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{``becomes''? How? from where does the below follow? Is this not simply assumed?}
%%%%%%%%%%%%%

In this section, we discuss a market model -- often referred to as the \textit{multi-asset Black-Scholes model with variable coefficients} -- consisting of one risk-free asset with price $A(t)$ satisfying
\begin{align*}
  \mathrm{d}A(t) &= r(t) A(t) \mathrm{d}t
\end{align*}
where $r(t)$ is the continuous risk-free rate, and many risky assets with prices $S_i(t), i=1,\ldots,d$ satisfying
\begin{align}
  \mathrm{d}S_i(t) &= \mu_i(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W_j(t), \text{ for } i = 1,\ldots,d.\label{eq:multi-bs-eq}
\end{align}
for $\mathbf{W}(t) = (W_1(t), \ldots, W_d(t))$ a Wiener process with respect to the probability $P$ in the filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$.

Some further assumptions are made by the model.

\begin{assumption}\label{ass:drift-vol-regularity}
  The processes $r$, $\mu_i$ and $c_{ij}$ for all $i,j=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted with continuous paths and are bounded by a deterministic constant.
\end{assumption}

\begin{assumption}\label{ass:vol-matrix-invertible}
  The matrix of volatily coefficients $\mathbf{C}(t)= [c_{ij}(t)]_{i,j=1,\ldots,d}$ is invertible.
\end{assumption}

% FIXME: Justificiation of $d$ Wiener processes for $d$ assets?

\tomcomment{[deleted section title]}
%%%%%%%%%%%%%
\comment{strange section title.}
%%%%%%%%%%%%%

%%%%%%%%%%%%%
\comment{this should be stated as a lemma and we should either provide a reference of give the proof.}
%%%%%%%%%%%%%

\begin{theorem}\label{thm:bs-solution}
  Equation \eqref{eq:multi-bs-eq} has the unique solution
  \begin{align*}
    S_i(t) &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Let $i = 1,\ldots,d$. By the It\^o formula (theorem \ref{thm:ito-formula}) with $F(t,x) = \ln x$ so that $F_t(t,x) = 0, F_x(t,x) = \frac{1}{x}$ and $F_{xx} = \frac{-1}{x^2}$, we have
    \begin{align*}
      \mathrm{d}F(t,S_i(t))
      &= F_t(t,S_i(t)) \mathrm{d}t + F_x(t,S_i(t)) \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + F_x(t,S_i(t)) \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d F_{xx}(t,S_i(t)) (c_{ij}(t) S_i(t))^2 \mathrm{d}t\\
      &= 0 \mathrm{d}t + \frac{1}{S_i(t)} \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + \frac{1}{S_i(t)} \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d \frac{-1}{S_i(t)^2} c_{ij}(t)^2 S_i(t)^2 \mathrm{d}t\\
      &= \mu_i(t) \mathrm{d}t
      - \frac{1}{2} \sum_{j=1}^d c_{ij}(t)^2 \mathrm{d}t
      + \sum_{j=1}^d c_{ij}(t) \mathrm{d}W_j(t).
    \end{align*}

    Expanding the stochastic differential notation into its integral form and since $\ln S_i(t) - \ln S_i(0) = \ln \frac{S_i(t)}{S_i(0)}$, we obtain
    \begin{align*}
      \ln \frac{S_i(t)}{S_i(0)}
      &= \int_0^t \mu_i(s) \mathrm{d}s
      - \frac{1}{2} \sum_{j=1}^d \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s).
    \end{align*}

    Applying the exponential function to both sides yields the required solution.

    Equation \eqref{eq:multi-bs-eq} can be expressed as
    \begin{align*}
      \mathrm{d}\mathbf{S}(t) = u(t,\mathbf{S}(t)) \mathrm{d}t + v(t,\mathbf{S}(t)) \mathrm{d}\mathbf{W}(t)
    \end{align*}
    with $u(t,\mathbf{x}) = [\mu_i(t) x_i]_{i=1,\ldots,d}$ and $v(t, \mathbf{x}) = [c_{ij}(t) x_i]_{i,j=1,\ldots,d}$.
    Since, by \ref{ass:drift-vol-regularity}, $\mu_i$ and $c_{ij}$ are bounded by a deterministic constant for all $i,j=1,\ldots,d$, both $u$ and $v$ are Lipschitz with linear growth with respect to $\mathbf{x}$. Therefore, by theorem \ref{thm:sde-solution}, the solution is unique.
  \end{proof}
\end{theorem}

\subsection{Risk-neutral probability}

We want to find a probability measure equivalent to the physical probability $P$ under which the discounted stock prices form martingales. This will in turn allow us to express derivative prices in terms of conditional expectations.

%%%%%%%%%%%%%
\comment{it is unclear where the discussion is heading. We should state the result and prove it. Moreover, the formulae spill onto the margin.}
%%%%%%%%%%%%%

\begin{definition}
  The \textbf{discounted stock prices} are given by
  \begin{align*}
    \tilde{S}_i(t) = e^{- \int_0^t r(s)\mathrm{d}s} S_i(t)
  \end{align*}
  for all $i=1,\ldots,d$.
\end{definition}

\begin{lemma}\label{lem:bs-exponential-martingale}
  If $a_j(t), j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes bounded by a deterministic constant $K > 0$, then the stochastic process
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.

  \begin{proof}
    The Novikov condition holds since
    \begin{align*}
      \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right)
      &\le \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\right)\\
      &= \exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\\
      &= \exp \left(\frac{1}{2} d K^2 T \right) < \infty.
    \end{align*}
    Hence, by theorem \ref{thm:novikov}, $M(t)$ is a martingale.
  \end{proof}
\end{lemma}

\begin{theorem}
  Let $\mathbf{\theta}(t) = [\theta_j(t)]_{j=1,\ldots,d}$ be given by
  \begin{align}\label{eq:bs-def-theta}
    \mathbf{\theta}(t) = \mathbf{C}^{-1}(t) [\mu(t) - r(t)]
  \end{align}
  and let
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right).
  \end{align*}
  Then the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$ is a risk-neutral probability, i.e. it is equivalent to $P$ and the processes $\tilde{S}_i, i=1,\ldots,d$ are martingales under $Q$.

  \begin{proof}
    For all $i=1,\ldots,d$, by theorem \ref{thm:bs-solution},
    \begin{align*}
      \tilde{S}_i(t)
      &= \exp\left(- \int_0^t r(s)\mathrm{d}s\right) S_i(t)\\
      &= S_i(0) \exp \left( \int_0^t (\mu_i(s) - r(s)) \mathrm{d}s \right.\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right).
    \end{align*}

    By \eqref{eq:bs-def-theta}, we have
    \begin{align*}
      \mathbf{\mu}(t) - r(t) = \mathbf{C}(t) \mathbf{\theta}(t)
    \end{align*}
    which in turn implies that
    \begin{align*}
      \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t)
    \end{align*}
    for all $i=1,\ldots,d$.

    Substituting this into the previous equation and by linearity of the integral, we obtain
    \begin{align}
      \tilde{S}_i(t)
      &= S_i(0) \exp \left( \sum_{j=1}^d \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\notag\\
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. + \sum_{j=1}^d \left[\int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right]\right).\label{eq:bs-risk-neutral-before-product-rule}
    \end{align}

    Equation \eqref{eq:bs-theta} together with assumption \ref{ass:drift-vol-regularity} implies that $\mathbf{\theta}(t)$ is $\mathcal{F}^\mathbf{W}_t$-adapted and bounded by a deterministic constant. This in turn implies that $M(t)$ is a martingale by lemma \ref{lem:bs-exponential-martingale}. Therefore, by the Girsanov theorem \ref{thm:girsanov}, the process $\mathbf{W}^Q(t) = (W^Q_1(t), \ldots, W^Q_d(t))$ with
    \begin{align*}
      \mathrm{d}W^Q_j(t) = \theta_j(t) \mathrm{d}t + \mathrm{d}W(t)
    \end{align*}
    is a Wiener process under Q.

    By the It\^o product rule (theorem \ref{thm:ito-product-rule}), we have that for all $j=1,\ldots,d$,
    \begin{align*}
      \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s)
      = c_{ij}(t) \mathrm{d}W^Q(t).
    \end{align*}

    Substituting in equation \eqref{eq:bs-risk-neutral-before-product-rule}, we obtain
    \begin{align*}
      \tilde{S}_i(t)
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align*}

    By lemma \ref{lem:bs-exponential-martingale} and assumption \ref{ass:drift-vol-regularity}, $\tilde{S}_i(t)$ is thus a martingale under $Q$ for all $i=1,\ldots,d$ as required.
  \end{proof}
\end{theorem}

\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{how can we notice this? Is the existence of such $\theta$ evident?}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{incorrect referencing style. This is not an equation.}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{incorrect referencing style. This is not an equation.}
%%%%%%%%%%%%%
\tomcomment{[deleted]}
%%%%%%%%%%%%%
\comment{above we have written as if this was obvious.}
%%%%%%%%%%%%%
\tomcomment{[deleted]}

%%%%%%%%%%%%%
\comment{what follows seems unfinished so I will stop here. Here are some final remarks:
\begin{itemize}
\item This is a very good first draft!
\item We must be more careful about paragraphs. The empty lines in latex code create a mess in the pdf file. If you fix this the draft will automatically become {\bf much} cleaner and obtain a professional look.
\item All theorems lemmas and propositions must come with a citation to a bibliography item.
\item There are a lot of FIXMEs. At the beginning this is fine, but I strongly encourage you not to continue like this. A draft of 20 pages which requires a lot of fixes will become an editorial nightmare. As you fix in one place, possibly some changes will be required in other places etc. Things will get out of hand. The best rule is: ``slow and steady wins the race''. It is best to finish sections, so that they are ready. They can always be expanded, but a draft should consist of work that is print ready (or as close as possible, at least). Otherwise you will have a lot of editorial problems as you proceed further.
\end{itemize}
}
%%%%%%%%%%%%%

\subsection{Strategies}

FIXME: Definition of contigent claim

FIXME: Definition of strategy

FIXME: Self-financing condition

FIXME: Definition of martingale strategy

FIXME: Definition of admissible strategy

FIXME: Explanation as to why we need admissiblity?

FIXME: Definition of replicating strategy

FIXME: Representation of contigent claims as processes assumed to be It\^o

\subsection{Completeness}

FIXME: Completeness of the model. (Is it though?)

\subsection{Pricing of derivatives}

FIXME: Pricing with risk-neutral expectations + comment about unknown joint distribution

FIXME: Black-Scholes PDE

\pagebreak
\section{The Fokker-Planck equation}

% Introduce the Fokker-Planck equation. Use [4] as a reference.

FIXME: Intro citing \textcite{pavliotis_stochastic_2014}

FIXME: We work in $(\Omega, \mathcal{F}, \mathcal{F}_t, P)$.

\begin{definition}
  A \textbf{Markov process} is a stochastic process $\mathbf{X}(t)$ that satisfies the \textit{Markov condition}

  \begin{align*}
    \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_{s})
    = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  \end{align*}

  for $0 \le s < t \le T$ and for all Borel bounded functions $f$.
\end{definition}

These processes are relevant to our discussion since It\^o processes -- and in particular Wiener processes -- are in fact Markov processes.

FIXME: It\^o processes are Markov processes

Informally, the future evolution of a Markov process only depends on its current state, independently from its past evolution.

For $s < t$ and $\Gamma \in \mathcal{B}({\mathbb{R}^d})$ -- where $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subsets of $\mathbb{R}^d$ -- the Markov condition implies that

\begin{align*}
  P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s)
  &= \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_s)
  = \mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  = \phi(\mathbf{X}(s))
\end{align*}

for some Borel function $\phi$ by the Doob-Dynkin lemma. This justifies the following definition since it ensures that it exists.

\begin{definition}
  Let $\mathbf{X}(t)$ be a $d$-dimensional Markov process. A \textbf{transition (probability) function} of $\mathbf{X}$ is a Borel function $\mu(\Gamma, t; \mathbf{x}, s)$ for $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$ and $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ such that

  \begin{align*}
    \mu(\Gamma, t; \mathbf{X}(s), s) = P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s).
  \end{align*}
\end{definition}

Note that, viewing $\mu(\Gamma, t; \mathbf{x}, s)$ as a function of $\Gamma$ with the other arguments fixed, it forms a probability measure. In the rest of this section, we focus on such transition function that have a density, leading to the following definition.

\begin{definition}
  Let $\mathbf{X}(t)$ be a Markov process with transition function $\mu(\Gamma,t;\mathbf{x},s)$ that admits a density $\rho$ with respect to the Lebesgue measure, i.e.

  \begin{align*}
    \mu(\Gamma,t;\mathbf{x},s) = \int_\Gamma \rho(\mathbf{y},t;\mathbf{x},s) \mathrm{d}\mathbf{y}
  \end{align*}

  for all $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < t \le T$.

  We say that $\rho$ is a \textbf{transition (probability) density} of the Markov process.
\end{definition}

In order to prove the main result of this section, we will make use of the following property of the transition probability density. There exist several versions of this equation including one in terms of the transition function not requiring the existence of a density. However, we only state a single version here for brevity.

\begin{theorem}\label{thm:chapman-kolmogorov}
  (\textbf{Chapman-Kolmogorov equation}) Let $\mathbf{X}(t)$ be a Markov process with transition probability density $\rho$. Then

  \begin{align*}
    \rho(\mathbf{y}, t; \mathbf{x}, s) = \int_{\mathbb{R}^d} \rho(\mathbf{y}, t; \mathbf{z}, u) \rho(\mathbf{z}, u; \mathbf{x}, s) \mathrm{d}\mathbf{z}
  \end{align*}

  for all $0 \le s < u < t \le T$ and for almost all $\mathbf{x} \in \mathrm{Im}(\mathbf{X}(s)), \mathbf{y} \in \mathbb{R}^d$.

  \begin{proof}
    Let $\Gamma \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < u < t \le T$. Then

    \begin{align*}
      \int_\Gamma \rho(\mathbf{y},t;\mathbf{X}(s),s) \mathrm{d}\mathbf{y}
      &= \mu(\Gamma,t;\mathbf{X}(s),s)\\
      &= P(X(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbf{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mathbb{E}(\mathbf{1}_\Gamma(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{X}_u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(P(\mathbf{X}(t) \in \Gamma \mid \mathcal{F}^\mathbf{X}_u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mu(\Gamma,t;\mathbf{X}(u),u) \mid \mathcal{F}^\mathbf{X}_s)\\
      &= \mathbb{E}(\mu(\Gamma,t;\mathbf{X}(u),u) \mid \mathcal{F}_{\mathbf{X}(s)})\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \mu(\mathrm{d}\mathbf{z},u;\mathbf{X}(s),s)\\
      &= \int_{\mathbb{R}^d} \mu(\Gamma,t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}\\
      &= \int_{\mathbb{R}^d} \int_\Gamma \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{z}\\
      &= \int_\Gamma \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z} \mathrm{d}\mathbf{y}.
    \end{align*}

    Since this holds for all $\Gamma$, the integrand must be equal for almost $\mathbf{y}$, i.e.

    \begin{align*}
      \rho(\mathbf{y},t;\mathbf{X}(s),s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}.
    \end{align*}

    And since this itself holds for almost all $\omega \in \Omega$, then for almost all $\mathbf{x} \in \mathrm{Im}(\mathbf{X}(s))$ such that $\mathbf{x} = \mathbf{X}(s,\omega)$, we have

    \begin{align*}
      \rho(\mathbf{y},t;\mathbf{x},s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{y},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{x},s) \mathrm{d}\mathbf{z}.
    \end{align*}
  \end{proof}
\end{theorem}

We can finally state and prove the main result of this section. Note that it is also called the \textit{forward Kolmogorov equation} in other resources.

\begin{theorem}
  (\textbf{Fokker-Planck equation}) Let $\mathbf{X}(t)$ be an It\^o process satisfying

  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t) \mathrm{d}t + \sum_{j=1}^d b_{ij}(t) \mathrm{d}W_j(t)
  \end{align*}

  for $i=1,\ldots,d$ and with transition probability density $\rho$. Then

  \begin{align*}
    \frac{\partial \rho}{\partial t} = - \frac{\partial}{\partial y} (a(t,y) \rho) + \frac{1}{2} \frac{\partial^2}{\partial y^2} (b(t,y) \rho).
  \end{align*}

  (FIXME)
\end{theorem}

\pagebreak
\section{Dupire's equation}

% Give a detailed derivation of the Dupire’s equation (equation starting with ∂C = on page 171 in [2]). ∂T
% Use section 2 from [2] and the section ‘The continuous time theory’ from [3] as a source for the proof.

FIXME

\section{Generalisation to multiple assets}

% Provide the setup and give a detailed proof of Theorem 1 from [2]. This should be based on section 3 from [2]

FIXME

\section{Alternative proof}

% Present the alternative proof of Theorem 1, anded on Appendix A from [2].

FIXME

\section{Numerical example}

% Give a numerical example of how Theorem 1 can be applied to recover aij. You can restrict to the simplest setting of a two dimensional Black-Scholes model.

FIXME

\pagebreak
\printbibliography

\end{document}
