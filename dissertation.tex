\documentclass[english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage{color}
\definecolor{note_fontcolor}{rgb}{0.800781, 0.800781, 0.800781}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{mathtools}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter

\newcommand{\comment}[1]{\color{blue}#1\color{black}}
\newcommand{\tomcomment}[1]{\color{orange}#1\color{black}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\DeclareRobustCommand*{\lyxarrow}{%
\@ifstar
{\leavevmode\,$\triangleleft$\,\allowbreak}
{\leavevmode\,$\triangleright$\,\allowbreak}}
%% The greyedout annotation environment
\newenvironment{lyxgreyedout}
  {\textcolor{note_fontcolor}\bgroup\ignorespaces}
  {\ignorespacesafterend\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\newtheoremstyle{bolddescit}{}{}{\itshape}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\newtheoremstyle{bolddesc}{}{}{}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{bolddescit}
\newtheorem{theorem}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[theorem]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
\theoremstyle{bolddesc}
\newtheorem{assumption}[theorem]{\protect\assumptionname}
\theoremstyle{plain}
\newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\usepackage{amsfonts}
\newcommand{\commentMJC}[1]{{\color{red}#1}}

\makeatother

\providecommand{\assumptionname}{Assumption}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Pricing Basket Options with a Generalisation of Dupire's Equation}
\author{Thomas Feron}
\date{~}

\maketitle
\vspace{2.5in}

\noindent \begin{center}
Dissertation submitted for the MSc in Mathematical Finance
\par\end{center}

\begin{center}
\bigskip{}
\par\end{center}

\begin{center}
Department of Mathematics

University of York\bigskip{}
\par\end{center}

\begin{center}
\today
\par\end{center}

\vspace{1in}

\begin{center}
Supervisor: Maciej J. Capi\'nski
\par\end{center}

\newpage{}

\tableofcontents{}\newpage{}

\pagebreak

FIXME: Quick intro to the topic and explanation of what is coming.

\section{Preliminaries}

% Develop a preliminaries section, in which the tools of stochastic Ito calculus are introduced. Assume that your reader has background in the standard undergraduate modules in mathematics (including analysis, measure theory, probability theory and statistics), but introduce all the notions concerning stochastic processes from scratch. This section should not contain any proofs. Simply set up and state the needed results, providing citations to sources. While developing this section, restrict to the setup needed for the model from [2].

This section establishes the preliminaries of stochastic calculus required by further sections without proofs. See \textcite{capinski_stochastic_2012} and \textcite{capinski_blackscholes_2012} for further details.

In the following, we implicitly assume that we work in a probability space $(\Omega, \mathcal{F}, P)$ unless stated otherwise. We also restrict ourselves to
%%%%%%%%%%%%%
%\comment{maybe mentioning processes here is not needed. You restrict to a time interval.}
%%%%%%%%%%%%%
the time interval $[0,T]$ for some $T$ as it is sufficient in this context.

%\tomcomment{Moved assumption about filtered probability space after definition of filtration.}
 %%%%%%%%%%%%%
%\comment{we could add a definition of a filtration.}
%%%%%%%%%%%%%

We will denote the Borel subsets of $A$ by $\mathcal{B}(A)$, e.g. $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subets of $\mathbb{R}^d$.

\begin{definition}
  A \textbf{stochastic process} is a measurable function $X : [0,T] \times \Omega \to \mathbb{R}^d$ with respect to the $\sigma$-field $\mathcal{B}([0,T]) \times \mathcal{F}$.
%%%%%%%%%%%%%
%\comment{We should explain the notation $\mathcal{B}([0,T])$}
%%%%%%%%%%%%%
\end{definition}

In the following, when we write $X(t)$ for $t \in [0,T]$, it denotes the random variable $\omega \mapsto X(t, \omega)$.
%%%%%%%%%%%%%
%\comment{Formally $X(t)$ is not defined, since $X : [0,T] \times \Omega \to \mathbb{R}^d$. The choice of the phrase ``Note that ...'' might not be the best.}
%%%%%%%%%%%%%

\begin{definition}
  For any fixed $\omega \in \Omega$, we say that the function $t \mapsto X(t,\omega)$ is a \textbf{path} of the stochastic process $X$.
\end{definition}

\begin{definition}
  A \textbf{filtration} is a family $(\mathcal{F}_t)_{t \in [0,T]}$ of sub-$\sigma$-fields of $\mathcal{F}$ such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $0 \le s < t \le T$.
\end{definition}

From here onwards, we further assume when relevant that we work with a filtration $(\mathcal{F}_t)_{t \in [0,T]}$.

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. We denote the \textbf{filtration generated by $X$} by $(\mathcal{F}^X_t)_{t \in [0,T]}$ with
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\},
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$, for any $\mathcal{F}$-measurable random variable $Y$.

\begin{definition}
  We say that a filtration $\mathcal{F}_t$ is \textbf{coarser} than a filtration $\mathcal{G}_t$ if $\mathcal{F}_t \subseteq \mathcal{G}_t$ for all $t \in [0,T]$. Equivalently, we say that $\mathcal{G}_t$ is \textbf{finer} than $\mathcal{F}_t$.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be \textbf{adapted} to a filtration $(\mathcal{F}_t)_{t \in [0,T]}$ if, for all $t \in [0,T]$, $X(t)$ is $\mathcal{F}_t$-measurable.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be a \textbf{martingale} for a filtration $\mathcal{F}_t$ if $X(t)$ is integrable for each $t \in [0,T]$ and
  \begin{align*}
    \mathbb{E}(X(t) \mid \mathcal{F}_s) = X(s),
  \end{align*}
  for all $0 \le s < t \le T$.
\end{definition}

\begin{theorem}\label{thm:bijection-filtration}
%%%%%%%%%%%%%
\comment{citation to some source?}
%%%%%%%%%%%%%
  Let $X, Y$ be stochastic processes. If there exists a Borel function $f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d$ such that $\mathbf{x} \mapsto f(t,\mathbf{x})$ is a bijection and $Y(t) = f(t, X(t))$ for all $t \in [0,T]$, then $X$ and $Y$ generate the same filtration.

  \begin{proof}
    It is enough to show that $\mathcal{F}_{X(t)} = \mathcal{F}_{Y(t)}$ for all $t \in [0,T]$ since the filtration is defined in terms of those $\sigma$-fields.

    Let $t \in [0,T]$ and let $B \in \mathcal{B}(\mathbb{R}^d)$. Then $A = X^{-1}(t,B) \in \mathcal{F}_{X(t)}$. Take $C = f(t, B)$ so that $C \in \mathcal{B}(\mathbb{R}^d)$ since $f$ is Borel. Hence, $A = X^{-1}(t,f^{-1}(t,C)) = Y^{-1}(t,C) \in \mathcal{F}_{Y(t)}$. Therefore, $\mathcal{F}_{Y(t)} \subseteq \mathcal{F}_{X(t)}$.

    Since $f$ is a bijection, we get $\mathcal{F}_{Y(t)} \supseteq \mathcal{F}_{X(t)}$ by symmetry. Hence, the $\sigma$-fields are equal as required and so are the filtrations.
  \end{proof}
\end{theorem}

The Wiener process is a central stochastic process to stochastic calculus. Here, we give an axiomatic definition. For a construction of such a process and thus a proof of existence, see \textcite{capinski_stochastic_2012}.

\begin{definition}
  A \textbf{Wiener process}, also called Brownian motion, is a stochastic process $(W(t))_{t \in [0,T]}$ that satisfies
  \begin{itemize}
    \item $W(0) = 0$ almost surely;
    \item for all $0 \le s < t \le T$, the increment $W(t) - W(s)$ follows a normal distribution with mean 0 and variance $t - s$;
    \item for all $0 \le t_1 < t_2 < \cdots < t_m$, the increments $W(t_k) - W(t_{k-1}), k=2,\ldots,m$ are independent; and
    \item almost all paths are continuous, i.e. the functions $t \mapsto W(t,\omega)$ are continuous for almost all $\omega \in \Omega$.
  \end{itemize}
\end{definition}

\begin{definition}
  A \textbf{$d$-dimensional Wiener process} is a stochastic process $\mathbf{W}(t) = (W_1(t), W_2(t), \ldots, W_d(t))$ where $W_j(t), j=1,\ldots,n$ are independent Wiener processes.
\end{definition}

\begin{definition}
  We say that a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ is a \textbf{simple process}, and denote it by $X \in \mathcal{S}^2$, if
  \begin{align*}
    X(t,\omega) = \xi_0 \mathbf{1}_{\{0\}}(t) + \sum_{k=0}^{n-1} \xi_k(\omega) \mathbf{1}_{(t_k,t_{k+1}]}(t)
  \end{align*}
  for some $n > 0$, $0 = t_0 < t_1 < \cdots < t_n = T$ and $\mathcal{F}^W_{t_k}$-measurable random variables $\xi_k$ such that $\mathbb{E}(\xi_k^2) < \infty$ for $k = 0,1,\ldots,n-1$.
\end{definition}

\begin{definition}
  The \textbf{stochastic integral}, also called It\^o integral, of a process $X \in \mathcal{S}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \sum_{k=0}^{n-1} \xi_k (W(t_{k+1}) - W(t_k)).
  \end{align*}
\end{definition}

\begin{definition}
  The set $\mathcal{M}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t \right) < \infty.
  \end{align*}
\end{definition}

\begin{theorem}
%%%%%%%%%%%%%
%\comment{maybe this could be a theorem, not proposition.}
%%%%%%%%%%%%%
(\cite{capinski_stochastic_2012}, Theorem 3.4)\label{thm:s2-m2-conv}
  For all $X \in \mathcal{M}^2$, there exists a sequence $(X_n)_{n \ge 1}$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{theorem}

\begin{theorem}
  (\cite{oksendal_stochastic_2003}, Definition 3.1.6)
  Let $X \in \mathcal{M}^2$. For all sequences $(X_n)$ in $\mathcal{S}^2$ that converge to $X$ in $L^2$, the sequence
  \begin{align*}
    \int_0^T X_n(t) \mathrm{d}W(t)
  \end{align*}
  converges in $L^2$ and the limit does not depend on the choice of $(X_n)$.
\end{theorem}

The previous two theorems ensure that the following definition of stochastic integrals in $\mathcal{M}^2$ is in fact well-defined by giving existence and uniqueness of the limit.

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \lim_{n \to \infty} \int_0^T X_n(t) \mathrm{d}W(t)
  \end{align*}
  for a sequence $(X_n)$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{definition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[a,b]$, for $0 \le a < b \le T$, is defined as
  \begin{align*}
    \int_a^b X(t) \mathrm{d}W(t)
    = \int_0^T \mathbf{1}_{[a,b]} X(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.15)\label{prop:stochastic-integral-martingale}
  For all $X \in \mathcal{M}^2$, there exists a martingale $M : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that
  \begin{align*}
    M(t) = \int_0^T \mathbf{1}_{[0,t]}(s) X(s) \mathrm{d}W(s),
  \end{align*}
  almost surely, for all $t \in [0,T]$.
\end{proposition}

\begin{definition}\label{def:stochastic-integral-as-process}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ is defined as the process
  \begin{align}\label{eq:stochastic-integral-process}
    \int_0^t X(s) \mathrm{d}W(s) = M(t)
  \end{align}
  where $M$ is the martingale given by Proposition \ref{prop:stochastic-integral-martingale}, for $t \in [0,T]$.
\end{definition}

\begin{theorem}\label{thm:stochastic-integral-expectation-m2}
  (\cite{capinski_stochastic_2012}, Theorem 3.14)
  If $X \in \mathcal{M}^2$, then
  \begin{align*}
    \mathbb{E}\left(\int_0^t X(s) \mathrm{d}s\right) = 0,
  \end{align*}
  for all $t \in [0,T]$.
\end{theorem}

\begin{definition}
  The set $\mathcal{P}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \int_0^T X(t)^2 \mathrm{d}t < \infty
  \end{align*}
  almost surely.
\end{definition}

\begin{proposition}(\cite{capinski_stochastic_2012}, Proposition 4.14 and Theorem 4.16)\label{prop:p2-localising-sequence}
  Let $X \in \mathcal{P}^2$ and let $(X_n)_{n \ge 1}$ be the sequence of stochastic processes given by
  \begin{align*}
    X_n(t) = \mathbf{1}_{[0,\tau_n]}(t) X(t)
  \end{align*}
  with
  \begin{align*}
    \tau_n = \inf \left\{ t \in [0,T] : \int_0^t X(s)^2 \mathrm{d}s \ge n \right\},
  \end{align*}
  where we take $\inf \emptyset = T$.
  Then $X_n \in \mathcal{M}^2$ for all $n$ and the sequence of continuous martingales
  \begin{align*}
    M_n(t) = \int_0^t X_n(s) \mathrm{d}W(s),
  \end{align*}
  as in \eqref{eq:stochastic-integral-process}, converges almost surely to a stochastic process $Y$ with continuous paths.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{P}^2$ is the process
  \begin{align*}
    \int_0^t X(s) \mathrm{d}W(s) = \lim_{n \to \infty} M_n(t) = Y(t)
  \end{align*}
  with $M_n$ and $Y$ as in Proposition \ref{prop:p2-localising-sequence}.
\end{definition}

%%%%%%%%%%%%%
%\comment{it would seem more natural to give first a definition of a one dimensional Ito process. Possibly setting up one dimensional versions first, and then multidimensional ones could be a choice. I understand though why you have decided to go for the general case first to make things shorter. So, this is just a choice that can be considered. }
%%%%%%%%%%%%%

For the rest of this section, we turn our attention to multidimensional stochastic processes.

\begin{definition}
  The \textbf{stochastic integral} of the $n \times d$ matrix $\mathbf{X} = [X_{ij}]$ of processes in $\mathcal{P}^2$ with respect to a $d$-dimensional Wiener process $\mathbf{W}$ is defined as
  \begin{align*}
    \int_0^t \mathbf{B}(s,\mathbf{X}(s)) \mathrm{d}\mathbf{W}(s) = \left[
      \sum_{j=1}^{n} \int_0^t b_{ij}(s) \mathrm{d}W_j(s)
    \right]_{n=1,\ldots,d}.
  \end{align*}
\end{definition}

If $n=1$, we understand the integral to be the entry as defined above rather than a 1-component vector.

Moreover, we can denote some sums of stochastic integrals with the notation
\begin{align*}
  \int_0^t \mathbf{X}(s) \cdot \mathrm{d}\mathbf{W}(s) = \sum_{j=1}^{d} \int_0^t X_j(s) \mathrm{d}W_j(s),
\end{align*}
for a stochastic process $\mathbf{X}(t) = (X_1(t), \ldots, X_d(t))$ with components in $\mathcal{P}^2$.

\begin{definition}
  We say that a process $\mathbf{X}(t) = (X_1(t), X_2(t), \ldots, X_d(t))$ is an \textbf{It\^o process} if it has the form
  %%%%%%%%%%%%%
%\comment{we should force line brakes. Pleas remove all `double slashes'. If something does not fit, possibly it should be centred.}
%%%%%%%%%%%%%
  \begin{align}\label{eq:ito-process}
    X_i(t) = X_i(0) + \int_0^t a_i(s) \mathrm{d}s + \sum_{j=1}^n \int_0^t b_{ij}(s) \mathrm{d}W_j(s),
  \end{align}
  for $i=1,\ldots,d$, where $W_j(t), j=1,\ldots,n$ are $n$ Wiener processes with $\mathbf{W}(t) = (W_1(t),\ldots,W_n(t))$; $a_i(t), i=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes such that $\int_0^T |a_i(t)| \mathrm{d}t < \infty$; and $b_{ij} \in \mathcal{P}^2$, for $i=1,\ldots,d$ and $j=1,\ldots,n$.
\end{definition}

Taking $\mathbf{a}(t) = (a_1(t),\ldots,a_d(t))$, $\mathbf{B}(t) = [b_{ij}(t)]_{i=1,\ldots,d;j=1,\ldots,n}$, we can write \eqref{eq:ito-process} as
\begin{align*}
  \mathbf{X}(t) = \mathbf{X}(0) + \int_0^t \mathbf{a}(s) \mathrm{d}s + \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s)
\end{align*}
or, to be even terser, in its so-called \textbf{stochastic differential} notation:
\begin{align*}
  \mathrm{d}\mathbf{X}(t) = \mathbf{a}(t) \mathrm{d}t + \mathbf{B}(t) \mathrm{d}\mathbf{W}(t).
\end{align*}

We call $\mathbf{a}$ the \textbf{drift} and $\mathbf{B}$ the \textbf{volatility}. Together, they are referred to as the \textbf{characteristics} of the It\^o process.

\begin{definition}
  A \textbf{stochastic differential equation} (or \textbf{SDE} for short) is an equation of the form
  \begin{align}\label{eq:sde-init-value}
    \mathbf{X}(t) &= \mathbf{X}(0) + \int_0^t a(s, \mathbf{X}(s)) \mathrm{d}s + \int_0^t b(s, \mathbf{X}(s)) \mathrm{d}\mathbf{W}(s),\\
    \mathbf{X}(0) &= \mathbf{x}_0,\notag
  \end{align}
  for some $a : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^d$, $b : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^{d \times n}$ and $\mathbf{x}_0 \in \mathbb{R}^d$.
\end{definition}

\begin{theorem}\label{thm:sde-solution}
  (\cite{oksendal_stochastic_2003}, Theorem 5.2.1)
  Provided that both the coefficients $a(t,\mathbf{x})$ and $b(t,\mathbf{x})$ satisfy the following conditions where $\|\cdot\|$ denotes the Euclidean norm on $\mathbb{R}^d$ and the $L_{2,1}$-norm on $\mathbb{R}^{d \times n}$.
  \begin{itemize}
    \item Linear growth: there exists $C > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x})\| + \|b(t,\mathbf{x})\| \le C (1 + \|\mathbf{x}\|),
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x} \in \mathbb{R}^d$.

    \item Lipschitz continuity: there exists $K > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x}) - a(t,\mathbf{y})\| + \|b(t,\mathbf{x}) - b(t,\mathbf{y})\| \le K \|\mathbf{x}-\mathbf{y}\|,
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^d$.
  \end{itemize}
  Then \eqref{eq:sde-init-value} has a unique solution with continuous paths such that $X_i \in \mathcal{M}^2$, for all $i=1,\ldots,d$.
\end{theorem}

\begin{theorem}
%%%%%%%%%%%%%
\comment{why is this a theorem? This sounds more like a definition.}
%%%%%%%%%%%%%
  An It\^o process of the form \eqref{eq:sde-init-value} whose characteristics satisfy the conditions of Theorem \ref{thm:sde-solution}, is called an \textbf{It\^o diffusion}.
\end{theorem}

\begin{lemma}\label{lem:ito-diffusion-characteristics-m2}
%%%%%%%%%%%%%
\comment{citation to a source?}
%%%%%%%%%%%%%
  The characteristics of an It\^o diffusion $\mathbf{X}$ are in $\mathcal{M}^2$, i.e.
  \begin{align*}
    a_i(t,\mathbf{X}(t)) \in \mathcal{M}^2, &&
    b_{ij}(t,\mathbf{X}(t)) \in \mathcal{M}^2,
  \end{align*}
  for $i=1,\ldots,d$ and $j=1,\ldots,n$.

  \begin{proof}
    Denote the It\^o diffusion by $\mathbf{X}$ and the characteristics $a$ and $b$ as in the definition. Let $i=1,\ldots,n$ and $j=1,\ldots,n$. By linear growth, we have
    \begin{align*}
      b_{ij}(t,\mathbf{X}(t))
      &\le \left( \sum_{k=1}^d \sum_{l=1}^n b_{kl}(t,\mathbf{X}(t))^2 \right)^{\frac{1}{2}}
      = \|b(t,\mathbf{X}(t))\|\\
      &\le \|a(t,\mathbf{X}(t))\| + \|b(t,\mathbf{X}(t))\|
      \le C (1 + \|\mathbf{X}(t)\|)
      \le \|\mathbf{X}(t)\|,
    \end{align*}
    assuming, without loss of generality, that $C > 1$.

    But then,
    \begin{align*}
      \mathbb{E} \left( \int_0^T b_{ij}(t,\mathbf{X}(t))^2 \mathrm{d}t \right)
      &\le \mathbb{E} \left( \int_0^T \|\mathbf{X}(t)\|^2 \mathrm{d}t \right)
      = \mathbb{E} \left( \int_0^T \sum_{i=1}^d X_i(t)^2 \mathrm{d}t \right)\\
      &= \sum_{i=1}^d \mathbb{E} \left( \int_0^T X_i(t)^2 \mathrm{d}t \right) < \infty \tag{linearity}
    \end{align*}
    since $X_i \in \mathcal{M}^2$ by Theorem \ref{thm:sde-solution}.

    Hence, $b_{ij}(t,\mathbf{X}(t)) \in \mathcal{M}^2$. The same proof can be used to show that $a_i(t,\mathbf{X}(t)) \in \mathcal{M}^2$. That is, all characteristics are in $\mathcal{M}^2$ as required.
  \end{proof}
\end{lemma}

\begin{theorem}[It\^o Formula]
  (\cite{capinski_blackscholes_2012}, Theorem 6.10)\label{thm:ito-formula}
  Let $F : [0,T] \times \mathbb{R}^d \to \mathbb{R}$ and let $\mathbf{X}(t)$ be a $d$-dimensional It\^o process driven by $n$ independent Wiener processes. For short, we write
  \begin{align*}
    F(t,\mathbf{X}(t)) = F(t,X_1(t),X_2(t),\ldots,X_d(t)).
  \end{align*}
  If $F$ is continuously differentiable in the first argument and twice-continuously differentiable in the others, then $F(t,\mathbf{X}(t))$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}F(t,\mathbf{X}(t))
    &= F_t(t,\mathbf{X}(t)) \mathrm{d}t + \sum_{i=1}^d F_{x_i}(t,\mathbf{X}(t)) a_i(t) \mathrm{d}t\\
    &\ \ \ \ + \sum_{i=1}^d \left(F_{x_i}(t,\mathbf{X}(t)) \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t) \right)\\
    &\ \ \ \ + \frac{1}{2} \sum_{j=1}^n \sum_{i,l=1}^d F_{x_i x_l}(t,\mathbf{X}(t)) b_{ij}(t)b_{lj}(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

\begin{definition}
  Given two It\^o processes $X, Y$ such that $Y$ has stochastic differential
  \begin{align*}
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  the \textbf{stochastic integral} of $X$ with respect to $Y$ is defined as
  \begin{align*}
    \int_0^t X(s) \mathrm{d}Y(s) = \int_0^t X(s) a_Y(s) \mathrm{d}s + \int_0^t X(s) b_Y(s) \mathrm{d}W(s).
  \end{align*}
  We also write
  \begin{align*}
    X(t) \mathrm{d}Y(t) = X(t) a_Y(t) \mathrm{d}t + X(t) b_Y(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

\begin{theorem}[It\^o Product Rule]\label{thm:ito-product-rule}
  (\cite{capinski_stochastic_2012}, Theorem 4.36)
  Given two It\^o processes $X, Y$ with stochastic differential
  \begin{align*}
    \mathrm{d}X(t) &= a_X(t) \mathrm{d}t + b_X(t) \mathrm{d}W(t),\\
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  their product $XY$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}[XY](t) = X(t) \mathrm{d}Y(t) + Y(t) \mathrm{d}X(t) + b_X(t) b_Y(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

\begin{theorem}[Girsanov Theorem]\label{thm:girsanov}
  (\cite{capinski_blackscholes_2012}, Theorem 6.15)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $\theta_j, j=1,\ldots,d$ be $\mathcal{F}^\mathbf{W}_t$-adapted processes such that
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale under $P$ and let $Q$ be the measure with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$, i.e.
  \begin{align*}
    Q(A) = \int_A M(T) \mathrm{d}P,
  \end{align*}
  for all $A \in \mathcal{F}$.
  Then the process $\mathbf{W}^Q(t) = (W^Q_1(t), W^Q_2(t), \ldots, W^Q_d(t))$ with
  \begin{align*}
    W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
  \end{align*}
  is a $d$-dimensional Wiener process under $Q$.
\end{theorem}

\begin{theorem}[Novikov Condition]\label{thm:novikov}
  (\cite{karatzas_brownian_1998}, Corollary 5.13)
  If $a_j(t)$, $j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes satisfying
  \begin{align*}
    \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right) < \infty,
  \end{align*}
  then
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.
\end{theorem}

\begin{theorem}[Martingale Representation Theorem]\label{thm:martingale-representation}
  (\cite{oksendal_stochastic_2003}, Theorem 4.3.4)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $M$ be a martingale such that $M(t)$ is square-integrable for all $t \in [0,T]$. Then there exists a unique $\mathcal{F}^\mathbf{W}_t$-adapted process $\mathbf{\Gamma}(t)$ such that
  \begin{align*}
    M(t) = M(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}(s)
  \end{align*}
  and such that $\Gamma_i \in \mathcal{M}^2$ for all $i=1,\ldots,d$.
\end{theorem}

\begin{theorem}\label{thm:cond-exp-measurable-independent}
  (\cite{kopp_probability_2013}, Theorem 4.27)
  Let $0 \le s < t \le T$ and $X, Y$ be two random variables such that
  \begin{itemize}
    \item $X$ is $\mathcal{F}^W_s$-measurable and
    \item $Y$ is $\mathcal{F}^W_t$-measurable and independent from $\mathcal{F}^W_s$.
  \end{itemize}
  For any bounded Borel function $f$, we have
  \begin{align*}
    \mathbb{E}(f(X,Y) \mid \mathcal{F}^W_s) = g_f(X),
  \end{align*}
  almost surely, where $g_f$ is a bounded Borel function given by
  \begin{align*}
    g_f(x) = \mathbb{E}(f(x,Y)).
  \end{align*}
\end{theorem}

\begin{definition}
  The \textbf{support} of a random variable $X$ with density $f$ is defined as
  \begin{align*}
    \mathrm{supp}(X) = \{x : f(x) > 0\}.
  \end{align*}
\end{definition}

%%%%%%%%%%%%%
%\comment{I do not think that we should manually brake pages. It is best to allow latex to do the formatting.}
%%%%%%%%%%%%%
\section{Multi-Asset Black-Scholes Model}

In this section, we discuss a market model -- often referred to as the \textit{multi-asset Black-Scholes model with variable coefficients} -- consisting of one risk-free asset with price $A(t)$ satisfying
\begin{align}\label{eq:multi-bs-eq-risk-free}
  \mathrm{d}A(t) = r(t) A(t) \mathrm{d}t, && A(0) = 1,
\end{align}
where $r(t)$ is a deterministic function representing the continuous risk-free rate, and many risky assets with prices $\mathbf{S}(t) = (S_1(t), \ldots, S_d(t))$ satisfying
\begin{align}
  \mathrm{d}S_i(t) &= \mu_i(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W_j(t), \text{ for } i = 1,\ldots,d,\label{eq:multi-bs-eq}
\end{align}
for $\mathbf{W}(t) = (W_1(t), \ldots, W_d(t))$ a Wiener process with respect to the probability $P$ in the filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$; some processes $\mu_i, i=1,\ldots,d$ and $c_{ij}, i,j=1,\ldots,d$; and $\mathbf{S}(0)$ such that $S_i(0) > 0$, for $i=1,\ldots,d$.

Some further assumptions are made by the model.

\begin{assumption}\label{ass:drift-vol-regularity}
  The processes $\mu_i$ and $c_{ij}$, for $i,j=1,\ldots,d$, are $\mathcal{F}^\mathbf{W}_t$-adapted with continuous paths and are bounded by a deterministic constant.
\end{assumption}

\begin{assumption}\label{ass:vol-matrix-invertible}
  The matrix of volatily coefficients $\mathbf{C}(t)= [c_{ij}(t)]_{i,j=1,\ldots,d}$ is invertible.
\end{assumption}

\begin{theorem}\label{thm:bs-solution}
  Equation \eqref{eq:multi-bs-eq} has solution
  \begin{align*}
    S_i(t) &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right),
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Let $i = 1,\ldots,d$. By the It\^o Formula (Theorem \ref{thm:ito-formula}) with $F(t,x) = \ln x$ so that $F_t(t,x) = 0, F_x(t,x) = \frac{1}{x}$ and $F_{xx} = \frac{-1}{x^2}$, we have
    \begin{align*}
      \mathrm{d}F(t,S_i(t))
      &= F_t(t,S_i(t)) \mathrm{d}t + F_x(t,S_i(t)) \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + F_x(t,S_i(t)) \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d F_{xx}(t,S_i(t)) (c_{ij}(t) S_i(t))^2 \mathrm{d}t\\
      &= 0 \mathrm{d}t + \frac{1}{S_i(t)} \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + \frac{1}{S_i(t)} \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t)\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{j=1}^d \frac{-1}{S_i(t)^2} c_{ij}(t)^2 S_i(t)^2 \mathrm{d}t\\
      &= \mu_i(t) \mathrm{d}t
      - \frac{1}{2} \sum_{j=1}^d c_{ij}(t)^2 \mathrm{d}t
      + \sum_{j=1}^d c_{ij}(t) \mathrm{d}W_j(t).
    \end{align*}

    Expanding the stochastic differential notation into its integral form and since $\ln S_i(t) - \ln S_i(0) = \ln \frac{S_i(t)}{S_i(0)}$, we obtain
    \begin{align*}
      \ln \frac{S_i(t)}{S_i(0)}
      &= \int_0^t \mu_i(s) \mathrm{d}s
      - \frac{1}{2} \sum_{j=1}^d \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s).
    \end{align*}

    Applying the exponential function to both sides yields the required solution.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-solution-risk-free}
  Equation \eqref{eq:multi-bs-eq-risk-free} has the unique solution
  \begin{align*}
    A(t) &= \exp \left( \int_0^t r(s) \mathrm{d}s \right).
  \end{align*}

  \begin{proof}
  %%%%%%%%%%%%%
\comment{why not simply
\[\frac{d}{dt} \exp \left( \int_0^t r(s) \mathrm{d}s \right) = \exp \left( \int_0^t r(s) \mathrm{d}s \right) r(t)  \]
so
\begin{align*}
A(t)-A(0) & =  \int_0^t \frac{d}{ds} \exp \left( \int_0^s r(u) \mathrm{d}u \right) ds \\
& =  \int_0^t r(s)\exp \left( \int_0^s r(u) \mathrm{d}u \right) ds \\
& = \int_0^t r(s)A(s) ds,
\end{align*}
as required.
}
%%%%%%%%%%%%%

    We can see \eqref{eq:multi-bs-eq-risk-free} as
    \begin{align*}
      \mathrm{d}A(t) = \mu_0(t) A(t) \mathrm{d}t + \sum_{j=1}^{d} c_{0j}(t) A(t) \mathrm{d}W_j(t)
    \end{align*}
    with $\mu_0(t) = r(t)$ and $c_{0j}(t) = 0$ for all $t \in [0,T]$ so, by the previous theorem,
    \begin{align*}
      A(t) &= A(0) \exp \left( \int_0^t \mu_0(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{0j}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{0j}(s) \mathrm{d}W_j(s) \right)\\
      &= \exp \left( \int_0^t r(s) \mathrm{d}s\right),
    \end{align*}
    substituting $\mu_0$ and $c_{0j}$ and since $A(0) = 1$.
  \end{proof}
\end{theorem}

\subsection{Risk-neutral Probability}

\begin{definition}
  A \textbf{risk-neutral probability} is a probability measure $Q$ that is equivalent to $P$ and under which all assets are martingales.
  %%%%%%%%%%%%%
\comment{The assets are not martingales. We need to discount.}
%%%%%%%%%%%%%
\end{definition}

\begin{definition}
  The \textbf{discounted process} of a process $X$ is defined as
  \begin{align*}
    \widetilde{X}(t) = \frac{X(t)}{A(t)} = e^{-\int_0^t r(s) \mathrm{d}s} X(t).
  \end{align*}
  The factor $\frac{1}{A(t)}$ is called the \textbf{discounting factor}.
\end{definition}

Note that since $e^{-\int_0^0 r(s) \mathrm{d}s} = 1$, a process agrees with its dicounted process at time 0, i.e. $X(0) = \widetilde{X}(0)$.

\begin{lemma}\label{lem:bs-exponential-martingale}
  If $a_j(t), j=1,\ldots,n$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes bounded by a deterministic constant $K > 0$, then the stochastic process
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.

  \begin{proof}
    The Novikov condition holds since
    \begin{align*}
      \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right)
      &\le \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\right)\\
      &= \exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\\
      &= \exp \left(\frac{1}{2} d K^2 T \right) < \infty.
    \end{align*}
    Hence, by Theorem \ref{thm:novikov}, $M(t)$ is a martingale.
  \end{proof}
\end{lemma}

\begin{theorem}
  Let $\mathbf{\theta}(t) = [\theta_j(t)]_{j=1,\ldots,d}$ be given by
  \begin{align}\label{eq:bs-def-theta}
    \mathbf{\theta}(t) = \mathbf{C}^{-1}(t) [\mu(t) - r(t)]
  \end{align}
  and let
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right).
  \end{align*}
  If $M(t)$ is a martingale under $P$, then the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$ is a risk-neutral probability, i.e. it is equivalent to $P$ and the processes $\widetilde{S}_i, i=1,\ldots,d$ are martingales under $Q$.
  %%%%%%%%%%%%%
%\comment{we should not be making definitions inside of a statement of a theorem.}
%%%%%%%%%%%%%

  \begin{proof}
    For all $i=1,\ldots,d$, by Theorem \ref{thm:bs-solution},
    \begin{align*}
      \widetilde{S}_i(t)
      &= \exp\left(- \int_0^t r(s)\mathrm{d}s\right) S_i(t)\\
      &= S_i(0) \exp \left( \int_0^t (\mu_i(s) - r(s)) \mathrm{d}s \right.\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right).
    \end{align*}

    By \eqref{eq:bs-def-theta}, we have
    \begin{align*}
      \mathbf{\mu}(t) - r(t) = \mathbf{C}(t) \mathbf{\theta}(t),
    \end{align*}
    which in turn implies that
    \begin{align*}
      \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t),
    \end{align*}
    for all $i=1,\ldots,d$.

    Substituting this into the previous equation and by linearity of the integral, we obtain
    \begin{align}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( \sum_{j=1}^d \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\notag\\
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. + \sum_{j=1}^d \left[\int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right]\right).\label{eq:bs-risk-neutral-before-product-rule}
    \end{align}

    Since we assume that $M(t)$ is a martingale, then the Girsanov Theorem \ref{thm:girsanov} implies that the process $\mathbf{W}^Q(t) = (W^Q_1(t), \ldots, W^Q_d(t))$ with
    \begin{align*}
      \mathrm{d}W^Q_j(t) = \theta_j(t) \mathrm{d}t + \mathrm{d}W(t)
    \end{align*}
    is a Wiener process under Q.

    By the It\^o Product Rule (Theorem \ref{thm:ito-product-rule}), we have that, for all $j=1,\ldots,d$,
    \begin{align*}
      \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s)
      = c_{ij}(t) \mathrm{d}W^Q(t).
    \end{align*}

    Substituting in equation \eqref{eq:bs-risk-neutral-before-product-rule}, we obtain
    \begin{align}\label{eq:bs-discounted-stock-price}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align}

    By Lemma \ref{lem:bs-exponential-martingale} with $a_j(t) \coloneq - c_{ij}(t)$ and Assumption \ref{ass:drift-vol-regularity}, $\widetilde{S}_i(t)$ is thus a martingale under $Q$ for all $i=1,\ldots,d$ as required.

    Moreover, since
    \begin{align*}
      Q(A) = \int_A M(T) \mathrm{d}P,
    \end{align*}
    for all $A \in \mathcal{F}$, then $P(A) = 0$ implies that $Q(A) = 0$. But since $M(T) > 0$, then $Q(A) = 0$ also implies that $P(A) = 0$. That is, the measures $P$ and $Q$ are equivalent.

    In conclusion, we have constructed a measure $Q$ that is indeed a risk-neutral probability for the model.
    %%%%%%%%%%%%%
%\comment{the conclusion does not seem justified since we have not defined what is a risk-neutral probability}
%%%%%%%%%%%%%
  \end{proof}
\end{theorem}

\begin{theorem}
  $\mathbf{W}$ and $\mathbf{W}^Q$ generate the same filtration, i.e. $\mathcal{F}^\mathbf{W}_t = \mathcal{F}^{\mathbf{W}^Q}_t$ for all $t \in [0,T]$.

  \begin{proof}
    Since $f(t, \mathbf{x}) = \int_0^t \mathbf{\theta}(s) \mathrm{d}s + \mathbf{x}$ is an invertible Borel function with respect to its second variable such that $\mathbf{W}^Q = f(t,\mathbf{W}(t))$, then the result follows from Theorem \ref{thm:bijection-filtration}.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-risk-neutral-dynamics}
  The risky assets satisfy
  \begin{align*}
    \mathrm{d}S_i(t) = r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t),
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Taking $r(t)$ as the drift and $\mathrm{W}^Q$ as the Wiener process in Theorem \ref{thm:bs-solution}, we obtain the unique solution for this stochastic differential
    \begin{align}\label{eq:bs-risk-neutral-dynamics}
      S_i(t)
      = S_i(0) \exp & \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s \right. \notag\\
      &\ \ \ \ \ + \left.\sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align}

    Multiplying both sides by the discounted factor $e^{-\int_0^t r(s) \mathrm{d}s}$, we can see that this is equivalent to \eqref{eq:bs-discounted-stock-price} which has been established earlier so the stochastic differential is indeed satisfied by $S_i$, for all $i=1,\ldots,d$.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-risk-neutral-dynamics-discounted}
  The risky assets satisfy
  \begin{align}\label{eq:bs-risk-neutral-dynamics-discounted}
    \mathrm{d}\widetilde{S}_i(t) = \sum_{j=1}^{d} c_{ij}(t) \widetilde{S}_i(t) \mathrm{d}W^Q_j(t),
  \end{align}
  for all $i=1,\ldots,d$.

  \begin{proof}
    From \eqref{eq:bs-risk-neutral-dynamics}, we have
    \begin{align*}
      \widetilde{S}_i(t)
      = \widetilde{S}_i(0) \exp \left( \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right),
    \end{align*}
    so, by a similar argument than for Theorem \ref{thm:bs-risk-neutral-dynamics}, it indeed satisfies \eqref{eq:bs-risk-neutral-dynamics-discounted}.
  \end{proof}
\end{theorem}

\begin{assumption}\label{ass:bs-stock-price-square-integrability}
  The risky assets $S_i$ are assumed to be square-integrable under $Q$ at time $T$, i.e.
  \begin{align*}
    \mathbb{E}_Q(S_i(T)^2) < \infty,
  \end{align*}
  for $i=1,\ldots,d$.
\end{assumption}

\subsection{Strategies}

\begin{definition}
  A (European) \textbf{derivative security} with expiry time $T$ is a security whose value is a stochastic process $H : [0,T] \times \Omega \to \mathbb{R}$ called its \textbf{price process} and such that $H(T) = H_\text{payoff}$ for some $\mathcal{F}^\mathbf{W}_T$-measurable random variable $H_\text{payoff}$ called the \textbf{payoff} of the derivative.
  %%%%%%%%%%%%%
%\comment{you could acknowledge that there is some abuse of notation here.}
%%%%%%%%%%%%%
\end{definition}

\begin{definition}
  The \textbf{extended market} for a derivative $H$ consists of the assets $A$ and $S_i, i=1,\ldots,d$ as described above (the basic market) as well as the derivative $H$.
\end{definition}

\begin{definition}
  A \textbf{strategy} is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form\\ $(x_1(t),\ldots,x_d(t),y(t))$ for $t \in [0,T]$ or simply $(x_1,\ldots,x_d,y)$ for short.
\end{definition}

\begin{definition}
  The \textbf{value (process)} of a strategy $(x_1,\ldots,x_d,y)$ is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y)}(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t).
  \end{align*}
  When there is no ambiguity, we simply denote it by $V(t)$.
\end{definition}

\begin{definition}
  An \textbf{extended strategy}, or a strategy in the extended market, is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form $(x_1,\ldots,x_d,y,z)$. Its value is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y,z)}(t)
    = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t) + z(t) H(t).
  \end{align*}
\end{definition}

\begin{definition}
  We say that a strategy is \textbf{self-financing} if it is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
  \end{align*}
\end{definition}

\begin{definition}
  We say that an extended strategy is \textbf{self-financing} if it is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t) + z(t) \mathrm{d}H(t).
  \end{align*}
\end{definition}

The following definitions apply to strategies as well as extended strategies.

\begin{definition}
  We say that a strategy is \textbf{admissible} if there exists a constant $L > 0$ such that its value process satisfies $V(t) \ge -L$ for all $t \in [0,T]$, almost surely.
\end{definition}

\begin{definition}
  An \textbf{arbitrage opportunity} is an admissible self-financing strategy such that
  \begin{itemize}
    \item $V(0) = 0$,
    \item $V(T) \ge 0$ almost surely and
    \item $V(T) > 0$ with positive probability.
  \end{itemize}
\end{definition}

\begin{remark}
  In these definitions, we do not have to specify under which probability statements hold \textit{almost surely} or \textit{with positive probability}. This is because $P$ and $Q$ are equivalent so saying that a proposition holds almost surely / with positive probability under $P$ is equivalent to saying that it holds under $Q$.
\end{remark}

\begin{assumption}[No-Arbitrage Principle]\label{ass:no-arbitrage-principle}
  There exist no arbitrage opportunities in the extended market.
\end{assumption}

\begin{assumption}\label{ass:bs-derivative-regularity}
  The price process of a derivative security with payoff $H_\text{payoff}$ is an It\^o process $H : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that $H(T) = H_\text{payoff}$ and such that there are no arbitrage opportunities in the extended market.
\end{assumption}

\subsection{Pricing of Derivatives}

%%%%%%%%%%%%%
%\comment{OK. I will stop commenting but just browse through the content.}
%%%%%%%%%%%%%

\begin{definition}
  A \textbf{replicating strategy} for a derivative with payoff $H_\text{payoff}$ is an admissible self-financing strategy (in the basic market) with value process $V$ such that
  \begin{align*}
    V(T) = H_\text{payoff}
  \end{align*}
  and such that its discounted value process $\widetilde{V}$ is a martingale under $Q$.
\end{definition}

If there exists a replicating strategy for a derivative, we say that the strategy \textbf{replicates} the derivative and that the derivative is \textbf{replicable}.

\begin{lemma}\label{lem:bs-non-negative-strat}%%%%%%%%%%%%%
\comment{citation missing}
%%%%%%%%%%%%%
  Let $(x_1, \ldots, x_d, y)$ be a self-financing strategy with value process $V$ that has continuous paths. If $V(T) \ge 0$, then $V(t) \ge 0$, almost surely, for all $t \in [0,T]$.

  \begin{proof}
    Assume to the contrary that there is a $t_0 \in [0,T)$ such that $V(t_0) < 0$ with positive probability. We prove this lemma by contradiction by constructing an arbitrage opportunity $(x_1^*, \ldots, x_d^*, y^*)$.

    On the region $A = \{V(t_0) \ge 0\}$, we simply take the strategy $(0,0,\ldots,0)$ and we now focus on the region $A^C = \{V(t_0) < 0\}$.

    For $t < t_0$, we take $x_1^*(t) = \cdots = x_d^*(t) = y^*(t) = 0$. For $t \ge t_0$, we take

    \begin{align*}
      x_i^*(t) &= \left\{ \begin{array}{ll}
        x_i(t) & \text{if } -A(t) < V(t) < 0,\\
        0 & \text{otherwise},
      \end{array}\right. \text{for } i=1,\ldots,d,\\
      y^*(t) &= \left\{ \begin{array}{ll}
        - \frac{V(t_0)}{A(t_0)} - 1 & \text{if } V(t) \le - A(t),\\
        - \frac{V(t_0)}{A(t_0)} + y(t) & \text{if } -A(t) < V(t) < 0,\\
        - \frac{V(t_0)}{A(t_0)} & \text{if } V(t) \ge 0.
      \end{array}\right.
    \end{align*}

    \textbf{Admissibility}: For $t < t_0$, we have $V^*(t) = 0$ and for $t \ge t_0$, we have
      \begin{align*}
        V^*(t)
        &= \sum_{i=1}^d x_i^*(s) S_i(s) + y^*(s) A(s)\\
        &= -\frac{V(t_0)}{A(t_0)} A(t) + \left\{
          \begin{array}{ll}
            -A(t) & \text{if } V(t) \le -A(t),\\
            V(t) & \text{if } -A(t) < V(t) < 0,\\
            0 & \text{if } V(t) \ge 0,
          \end{array}
        \right.\\
        &\ge -\frac{V(t_0)}{A(t_0)} A(t) - A(t)
        \ge - A(t) \tag{$-\frac{V(t_0)}{A(t_0)}A(t)$ > 0}\\
        &\ge -  A(T)
      \end{align*}
      which is a deterministic constant as required. Hence, the strategy $(x^*_1,\ldots,x^*_d,y^*)$ is admissible.

    \textbf{Self-financing condition}:
      For $t < t_0$, the self-financing condition holds since
      \begin{align*}
      V^*(0) &+ \sum_{i=1}^d \int_0^t x_i^*(s)\mathrm{d}S_i(s) + \int_0^t y^*(s) \mathrm{d}A(s)\\
      &= 0 + \sum_{i=1}^d \int_0^t 0 \mathrm{d}S_i(s) + \int_0^t 0 \mathrm{d}A(s)
      = 0 = V^*(t).
      \end{align*}

      At $t = t_0$, we have $V^*(t) = V(t_0) - \frac{V(t_0)}{A(t_0)} A(t_0) = 0$ so the condition also holds at this time.

      For a fixed $\omega \in \Omega$, we can divide the graph of $V(t)$ into three regions where $V(t) < -A(t)$, $-A(t) < V(t) < 0$ or $V(t) \ge 0$ respectively. While the graph of $V(t)$ remains in any of these regions, the strategy only changes as a result of $(x_1,\ldots,x_d,y)$ changing and so the self-financing condition holds since $(x_1,\ldots,x_d,y)$ is itself self-financing. Since both $V(t)$ and $A(t)$ have continuous paths, the graph of $V(t)$ can only cross from one region to another at a point where $V(t) = -A(t)$ or $V(t) = 0$. It is therefore sufficient to verify that the self-financing condition holds at these points. When $V(t) = -A(t)$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = -A(t) - \frac{V(t_0)}{A(t_0)} A(t)\\
        &= \left(- \frac{V(t_0)}{A(t_0)} - 1\right) A(t)
      \end{align*}
      and, when $V(t) = 0$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = - \frac{V(t_0)}{A(t_0)} A(t)
      \end{align*}
      so this is indeed the case.

    \textbf{Arbitrage conditions}: As shown above, we have $V^*(0)$. On $A$, $V^*(T) = 0 \ge 0$ and on $A^C$ which has positive probability, we have
    \begin{align*}
      V^*(T) = -\frac{V(t_0)}{A(t_0)} A(T) > 0
    \end{align*}
    since $V(T) \ge 0$. Hence $V^*(T)$ is non-negative almost surely and strictly positive with positive probability as required.

    That is, we have constructed an arbitrage opportunity but this leads to a contradiction by the No-Arbitrage Principle (Assumption \ref{ass:no-arbitrage-principle}), thus completing the proof.
  \end{proof}
\end{lemma}

Note that by inspecting the proof, we can see that this lemma equally applies to strategies in the extended market.

\begin{theorem}\label{thm:bs-repl-strat-derivative-prices}
  If a derivate with payoff $H_\text{payoff}$ is replicable by a strategy with value process $V$ that has continuous paths, then $V(t) = H(t)$, for all $t \in [0,T]$.

  \begin{proof}
    Let $(x_1,\ldots,x_d,y)$ be the replicating strategy with value process $V$. Consider the two strategies in the extended market $(x_1^+,\ldots,x_d^+,y^+,z^+)$ and $(x_1^-,\ldots,x_d^-,y^-,z^-)$ with value process $V^\pm$ and with $x_i^\pm(t) = \pm x_i(t)$ for $i=1,\ldots,d$, $y^\pm(t) = \pm y(t)$ and $z^\pm(t) = (-1) (\pm z(t))$ for all $t \in [0,T]$.  Then their values satisfy $V^+(t) = - V^-(t) = V(t) - H(t)$ for all $t \in [0,T]$.

    Note that since both $V(t)$ and $H(t)$ have continuous paths (by assumption of this theorem and by Assumption \ref{ass:bs-derivative-regularity} respectively), then so do $V^\pm$. Moreover, $V^\pm(T) = V(T) - H(T) = 0 \ge 0$. Hence, by Lemma \ref{lem:bs-non-negative-strat} twice, we have
    \begin{align*}
      V^+(t) \ge 0, && V^-(t) \ge 0,
    \end{align*}
    almost surely, for all $t \in [0,T]$. But then $V^+(t) = - V^+(t) \le 0$ so $V^+(t) = 0$.

    In other words, $V(t) - H(t) = 0$ or, equivalently, $V(t) = H(t)$ almost surely for all $t \in [0,T]$ as required.
  \end{proof}
\end{theorem}

  %%%%%%%%%%%%%
%\comment{It seems to me that this can be proven. In your email you have written that a proof would be technical. I am wondering if this indeed is the case. We could discuss this.}
%%%%%%%%%%%%%

\begin{theorem}\label{thm:bs-replicability}
  Let $H$ be a derivative with square-integrable payoff. If there exists a deterministic constant $L > 0$ such that $H(T) \ge -L$ almost surely, then the derivative is replicable by a strategy whose value process has continuous paths.

  \begin{proof}
    We construct a replicating strategy $(x_1,\ldots,x_d,y)$ with value process $V$ for the derivative. Since it replicates the derivative, we must have that $V(T) = H(T)$ and its discounted value must be a $Q$-martingale. Hence, for all $t \in [0,T]$,
    \begin{align*}
      \widetilde{V}(t)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} V(T) \mid \mathcal{F}^\mathbf{W}_t\right)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right).
    \end{align*}

    In particular,
    \begin{align*}
      \widetilde{V}(T)
      &= \mathbb{E}_Q\left(e^{-\int_0^T r(s)\mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_T\right)
      = e^{-\int_0^T r(s)\mathrm{d}s} H(T)
    \end{align*}
    since $H(T)$ is $\mathcal{F}^\mathbf{W}_T$-measurable. Hence, since $H(T)$ is square-integrable, then so is $\widetilde{V}(T) = e^{-\int_0^T r(s)\mathrm{d}s} H(T)$. But then,
    \begin{align*}
      \int_0^t \widetilde{V}(s)^2 \mathrm{d}s
      &\le \int_0^t \widetilde{V}(s)^2 \mathrm{d}s + \int_t^T \widetilde{V}(s)^2 \mathrm{d}s
      = \int_0^T \widetilde{V}(s)^2 \mathrm{d}s < \infty,
    \end{align*}
    so $\widetilde{V}(t)$ is square-integrable for all $t \in [0,T]$. Therefore, by the Martingale Representation Theorem \ref{thm:martingale-representation}, there exists $\mathbf{\Gamma}$ such that
    \begin{align}\label{eq:bs-repl-deriv-gamma}
      \widetilde{V}(t)
      &= \widetilde{V}(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}^Q(s).
    \end{align}

    Since $\Gamma_i \in \mathcal{M}^2$, the stochastic integrals have continuous paths by Definition \ref{def:stochastic-integral-as-process} and, since $A(t)$ is also continuous, then $V(t) = A(t) \widetilde{V}(t)$ has continuous paths.

    Note that this process $\widetilde{V}$ exists whether there exists a replicating strategy or not, being the conditional expectation under $Q$ of the discounted payoff, so we are not in fact using the existence of a replicating strategy in order to construct one, however it may seem at first sight.

    In order for the strategy to be self-financing, it needs to satisfy
    \begin{align}\label{eq:bs-repl-deriv-self-financing}
      \mathrm{d}V(t) &= \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
    \end{align}
    By the It\^o Formula (Theorem \ref{thm:ito-formula}) with $F(t,x) = \frac{1}{x}$,
    \begin{align}\label{eq:bs-repl-deriv-risk-free-inv}
      \mathrm{d}A(t)^{-1} = -A(t)^{-2} r(t) A(t) \mathrm{d}t = - r(t) A(t)^{-1} \mathrm{d}t,
    \end{align}
    so, by the It\^o Product Rule (Theorem \ref{thm:ito-product-rule}), we obtain
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= A(t)^{-1} \mathrm{d}V(t) + V(t) \mathrm{d}A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) + V(t) \mathrm{d}A(t)^{-1}\tag{by \eqref{eq:bs-repl-deriv-self-financing}}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) - V(t) r(t) A(t)^{-1} \mathrm{d}t \tag{by \eqref{eq:bs-repl-deriv-risk-free-inv}}.
    \end{align*}
    Substituting $V(t)$ and by \eqref{eq:multi-bs-eq-risk-free},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) r(t) A(t) \mathrm{d}t - y(t) A(t) r(t) A(t)^{-1} \mathrm{d}t\\
        &\ \ \ \ - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t.
    \end{align*}
    Then, by Theorem \ref{thm:bs-solution},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d [ A(t)^{-1} x_i(t) r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^d A(t)^{-1} x_i(t) c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t) \\
        &\ \ \ \ \ \ \ \ \ - x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t ]\\
      &= \sum_{i,j=1}^d x_i(t) c_{ij}(t) \widetilde{S}_i(t) \mathrm{d}W^Q_j(t)
      = \mathbf{C}(t) \mathbf{\theta}(t) \cdot \mathrm{d}\mathbf{W}^Q(t),
    \end{align*}
    where $\theta_i(t) = x_i(t) \widetilde{S}_i(t)$, for $i=1,\ldots,d$.

    Both this last equation and \eqref{eq:bs-repl-deriv-gamma} can be satisfied if $\mathbf{C} \mathbf{\theta} = \mathbf{\Gamma}$. This implies that $\mathbf{\theta} = \mathbf{C}^{-1} \mathbf{\Gamma}$. But then,
    \begin{align*}
      x_i = \frac{(\mathbf{C}^{-1} \mathbf{\Gamma})_i}{\widetilde{S}_i},
    \end{align*}
    for all $i=1,\ldots,d$. Moreover, since $V(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t)$, we also have
    \begin{align*}
      y(t) = \frac{1}{A(t)} \left(\sum_{i=1}^d x_i(t) S_i(t) - V(t)\right).
    \end{align*}

    Hence, the strategy constructed above is self-financing with a discounted value process that forms a $Q$-martingale and such that $V(T) = H(T)$. We are only left to show admissibility for that strategy to replicate $H$. But, for all $t \in [0,T]$, we have that, almost surely,
    \begin{align*}
      V(t) &= \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right)\\
      &\ge \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} L \mid \mathcal{F}^\mathbf{W}_t\right)
      = e^{-\int_t^T r(s) \mathrm{d}s} L,
    \end{align*}
    which is a deterministic constant.

    Therefore, we have constructed a replicating strategy, thus proving that the derivative is indeed replicable. Moreover, we have also shown that its value process has continuous paths.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-derivative-pricing}
  If a derivative $H$ is replicable by a strategy whose value process $V$ has continuous paths, then
  \begin{align}\label{eq:bs-derivative-pricing}
    \widetilde{H}(t) = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t),
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    By Theorem \ref{thm:bs-repl-strat-derivative-prices}, we have that $H(t) = V(t)$, for all $t \in [0,T]$. Multiplying both sides by the discounting factor $\exp\left(-\int_0^t r(s) \mathrm{d}s\right)$, we also have that $\widetilde{H}(t) = \widetilde{V}(t)$, for all $t \in [0,T]$.

    Since $(x_1, \ldots, x_d, y)$ is a replicating strategy, its discounted value $\widetilde{V}$ is a Q-martingale. Therefore,
    \begin{align*}
      \widetilde{H}(t) = \widetilde{V}(t)
      = \mathbb{E}_Q(\widetilde{V}(T) \mid \mathcal{F}^\mathbf{W}_t)
      = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t).
    \end{align*}
  \end{proof}
\end{theorem}

Alternatively, we can write equation \eqref{eq:bs-derivative-pricing} as
\begin{align*}
  H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right),
\end{align*}
for all $t \in [0,T]$. In particular at time 0, since $\mathcal{F}^\mathbf{W}_0$ is the trivial $\sigma$-field, we have
\begin{align*}
  H(0) = \mathbb{E}_Q\left(\widetilde{H}(T)\right)
  = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T)\right).
\end{align*}

\subsection{European Options}

\begin{definition}
  The \textbf{positive part function} $(\cdot)^+ : \mathbb{R} \to \mathbb{R}$ is given by
  \begin{align*}
    x^+ = \left\{\begin{array}{ll}
      0 & \text{if } x < 0,\\
      x & \text{if } x \ge 0.
    \end{array}\right.
  \end{align*}
\end{definition}

\begin{definition}
  European \textbf{call and put options} with \textit{expiry time} $T$, \textit{strike price} $K > 0$, and \textit{underlying} $U$, a replicable $\mathcal{F}^\mathbf{W}_t$-adapted process with continuous paths, are derivative securities with payoffs
  \begin{align*}
    H_\text{call} = (U(T) - K)^+, && H_\text{put} = (K - U(T))^+.
  \end{align*}
\end{definition}

\begin{theorem}[Put-call parity]
  Let $C(t)$ and $P(t)$ be the price processes of European call and put options respectively with expiry time $T$, strike prike $K$, and underlying $U$. Then
  \begin{align}\label{eq:put-call-parity}
    C(t) - P(t) = U(t) - e^{-\int_t^T r(s) \mathrm{d}s} K,
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    Taking $(x_1^U,\ldots,x_d^U,y^U)$ the strategy with value $V^U$ replicating the underlying $U$.

    A portfolio consisting of one long such call option and one short such put option is equivalent to a derivative with payoff
    \begin{align*}
      (U(T) - K)^+ - (K - U(T))^+
      &= U(T) - K.
    \end{align*}

    We thus can replicate it by the strategy $(x_1,\ldots,x_d,y)$ where
    \begin{align*}
      x_i(t) &= x_i^U(t), \text{ for } i=1,\ldots,d,\\
      y(t) &= y^U(t) - \frac{K}{A(T)}.
    \end{align*}
    Note that $y(t)$ is $\mathcal{F}^\mathbf{W}_t$-adapted since $y^U(t)$ is adapted and $r$ is a deterministic function. The value of the strategy at time $T$ is
    \begin{align*}
      V(T) = \sum_{i=1}^d x_i^U(T) S_i(T) + y^U(T) A(T) - \frac{K}{A(T)} A(T)
      = U(T) - K.
    \end{align*}

    Hence, Theorem \ref{thm:bs-repl-strat-derivative-prices} implies that equation \eqref{eq:put-call-parity} indeed holds.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-option-pricing}
  A European option with price process $H$ and payoff $H_\text{payoff}$ on an underlying $U$ such that $U(T)$ is square-integrable, is replicable by a strategy whose value process has continuous paths. Moreover,
  \begin{align}\label{eq:bs-option-pricing}
    H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H_\text{payoff} \mid \mathcal{F}^\mathbf{W}_t\right).
  \end{align}

  \begin{proof}
    First note that
    \begin{align*}
      H_\text{call}^2 = ((U(T) - K)^+)^2 &\le (U(T) - K)^2,\\
      H_\text{put}^2 = ((K - U(T))^+)^2 &\le (U(T) - K)^2,
    \end{align*}
    so, in both cases, we have
    \begin{align*}
      \mathbb{E}_Q(H_\text{payoff}^2)
      \le \mathbb{E}_Q((U(T) - K)^2)
      = \mathbb{E}_Q(U(T)^2) - 2K\mathbb{E}_Q(U(T))) + K^2
      < \infty
    \end{align*}
    since $U(T) \in L^2(\Omega, Q)$ and hence $U(T) \in L^1(\Omega, Q)$ as well.

    In addition, since the payoff of an option is non-negative, it is bounded below by the deterministic constant $0$. Thus, by Theorem \ref{thm:bs-replicability}, there exists a replicating strategy whose value process has continuous paths.

    By Theorem \ref{thm:bs-derivative-pricing}, the existence of such a strategy in turn implies that \eqref{eq:bs-option-pricing} holds.
  \end{proof}
\end{theorem}

\begin{definition}
  A European \textbf{basket option} is a European option on an underlying of the form
  \begin{align*}
    B_\mathbf{w}(t) = \sum_{i=1}^{d} w_i S_i(t) = \mathbf{w}^T \mathbf{S}(t)
  \end{align*}
  with $\mathbf{w} = (w_1, \ldots, w_d)$, called the \textbf{weights}, such that $\sum_{i=1}^{d} w_i = 1$.
\end{definition}

Note that this is well-defined since $U$ is clearly replicable by the constant strategy $(w_1, \ldots, w_d, 0)$.

We denote the price of a European basket call option with weights $\mathbf{w}$ by $C_\mathbf{w}(t)$ and its put option couterpart by $P_\mathbf{w}(t)$.

\begin{theorem}
  Consider a European basket option with underlying $U$ and weights $\mathbf{w}$. Then $U(T)$ is square-integrable.

  \begin{proof}
    By Assumption \ref{ass:bs-stock-price-square-integrability}, $S_i \in L^2(\Omega, Q)$ for all $i=1,\ldots,d$. Since $L^2(\Omega, Q)$ is a vector space over $\mathbb{R}$, the linear combination $U(T) = \sum_{i=1}^{d} w_i S_i(T)$ is also in $L^2(\Omega, Q)$, thus completing the proof.
  \end{proof}
\end{theorem}

\begin{corollary}
  The price process of a European basket option satisfies \eqref{eq:bs-option-pricing}.

  \begin{proof}
    The result immediately follows from the previous theorem together with Theorem \ref{thm:bs-option-pricing}.
  \end{proof}
\end{corollary}

\section{The Fokker-Planck Equation}

% Introduce the Fokker-Planck equation. Use [4] as a reference.

This section is based on \textcite{pavliotis_stochastic_2014}. We work in a filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$ without stating it explicitly as to not clutter the argument unecessarily.

\begin{definition}
  A \textbf{Markov process} is a stochastic process $\mathbf{X}(t)$ that satisfies the \textit{Markov condition}:
  \begin{align*}
    \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
    = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  \end{align*}
  for all bounded Borel functions $f$ and for $0 \le s < t \le T$.
\end{definition}

We also say that a Markov process is Markov, using the name as an adjective. Informally, the future evolution of a Markov process only depends on its current state, independently from its past evolution.

For $0 \le s < t \le T$ and $\mathbf{\Gamma} \in \mathcal{B}({\mathbb{R}^d})$, the Markov condition implies that
\begin{align*}
  P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s)
  &= \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
  = \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  = \phi(\mathbf{X}(s)),
\end{align*}
for some Borel function $\phi$ by the Doob-Dynkin Lemma (\cite{oksendal_stochastic_2003}, Lemma 2.1.2)
%%%%%%%%%%%%%
\comment{The dissertation needs to be self contained. We should not call upon theorems without including them in preliminaries.}
%%%%%%%%%%%%%
. This justifies the following definition since it ensures that it exists.

\begin{definition}
  Let $\mathbf{X}(t)$ be a $d$-dimensional Markov process. A \textbf{transition (probability) function} of $\mathbf{X}$ is a Borel function $\nu(\mathbf{\Gamma}, t; \mathbf{x}, s)$, for $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$, and $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$, such that
  \begin{align*}
    \nu(\mathbf{\Gamma}, t; \mathbf{X}(s), s) = P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s).
  \end{align*}
\end{definition}

Note that, the function $\mathbf{\Gamma} \mapsto \nu(\mathbf{\Gamma}, t; \mathbf{x}, s)$ is a probability measure
%%%%%%%%%%%%%
\comment{why? I think that $P(Y|F)$ is a random variable and not a number.}
%%%%%%%%%%%%%
. In the rest of this section, we focus on such transition functions that have a density, leading to the following definition.

\begin{definition}
  Let $\mathbf{X}(t)$ be a Markov process such that it has a transition function $\nu(\mathbf{\Gamma},t;\mathbf{x},s)$ that admits a density $\rho$ with respect to the Lebesgue measure, i.e.
  \begin{align*}
    \nu(\mathbf{\Gamma},t;\mathbf{y},s) = \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x},
  \end{align*}
  for all $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < t \le T$.
  We say that $\rho$ is a \textbf{transition (probability) density} of the Markov process.
\end{definition}

In order to prove the main result of this section, we will make use of the following property of the transition probability density. There exist several versions of this equation including one in terms of the transition function that does not require the existence of a density. However, we only state a single version here for brevity.

\begin{theorem}[Chapman-Kolmogorov Equation]\label{thm:fp-chapman-kolmogorov}
  Let $\mathbf{X}(t)$ be a Markov process with transition probability density $\rho$. Then
  \begin{align*}
    \rho(\mathbf{x}, t; \mathbf{y}, s) = \int_{\mathbb{R}^d} \rho(\mathbf{x}, t; \mathbf{z}, u) \rho(\mathbf{z}, u; \mathbf{y}, s) \mathrm{d}\mathbf{z},
  \end{align*}
  for $0 \le s < u < t \le T$, for $\mathbf{y} \in \mathrm{Im}(\mathbf{X}(s))$, and for almost all $\mathbf{x} \in \mathbb{R}^d$.

  \begin{proof}
    Let $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < u < t \le T$. Then
    \begin{align*}
      \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{X}(s),s) \mathrm{d}\mathbf{x}
      &= \nu(\mathbf{\Gamma},t;\mathbf{X}(s),s)
      = P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_u) \mid \mathcal{F}^\mathbf{W}_s) \tag{tower property}\\
      &= \mathbb{E}(\nu(\mathbf{\Gamma},t;\mathbf{X}(u),u) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \int_{\mathbb{R}^d} \nu(\mathbf{\Gamma},t;\mathbf{z},u) \nu(\mathrm{d}\mathbf{z},u;\mathbf{X}(s),s)\\
      &= \int_{\mathbb{R}^d} \nu(\mathbf{\Gamma},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}\\
      &= \int_{\mathbb{R}^d} \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{z}\\
      &= \int_\mathbf{\Gamma} \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z} \mathrm{d}\mathbf{x},
    \end{align*}
    by Fubini's Theorem (\cite{kopp_probability_2013}, Theorem 3.18)
    %%%%%%%%%%%%%
\comment{again, we should not use sources from the outside.}
%%%%%%%%%%%%%
     since the integral of a density is finite.

    Since this holds for all $\mathbf{\Gamma}$, the integrand must be equal for almost all $\mathbf{x}$, i.e.
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{X}(s),s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}.
    \end{align*}

    And since this itself holds for all $\omega \in \Omega$, then for all $\mathbf{y} \in \mathrm{Im}(\mathbf{X}(s))$ such that $\mathbf{y} = \mathbf{X}(s,\omega)$ for some $\omega \in \Omega$, we have
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{y},s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{y},s) \mathrm{d}\mathbf{z}.
    \end{align*}
  \end{proof}
\end{theorem}

\begin{lemma}\label{lem:fp-ito-diffusion-indep-increments}
  Let $\mathbf{X}$ be an It\^o diffusion satisfying
  \begin{align}\label{eq:fp-ito-diffusion-deterministic-drift}
    \mathrm{d}\mathbf{X}(t) = a(t) \mathrm{d}t + b(t,\mathbf{X}(t)) \mathrm{d}\mathbf{W}(t),
  \end{align}
  where $a$ is a deterministic function and $b(t,\mathbf{S}(t)) \in \mathcal{M}^2$. Then the increments $\mathbf{X}(t) - \mathbf{X}(s)$ are independent from $\mathcal{F}^\mathbf{W}_s$, for all $0 \le s < t \le T$.

  \begin{proof}
    Let $0 \le s < t \le T$. Firstly,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u \mid \mathcal{F}^\mathbf{W}_s\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right) \tag{$a$ deterministic}\\
      &= \int_s^t a(u) \mathrm{d}u + \int_s^s b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u)
      = \int_s^t a(u) \mathrm{d}u
    \end{align*}
    since the assumption that $b(t,\mathbf{S}(t)) \in \mathcal{M}^2$ implies that the stochastic integral is a martingale.

    Secondly,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s))\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u)\right)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\right) \tag{tower property}\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(0\right) \tag{as before}\\
      &= \int_s^t a(u) \mathrm{d}u.
    \end{align*}

    But then,
    \begin{align*}
      \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)
      = \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s)),
    \end{align*}
    so the required independence follows.
  \end{proof}
\end{lemma}

\begin{theorem}\label{thm:fp-ito-diffusion-markov}
  If $\mathbf{X}$ is an It\^o diffusion such that the increments $\mathbf{X}(t) - \mathbf{X}(s)$ are independent from $\mathcal{F}^\mathbf{W}_s$ for all $0 \le s < t \le T$, then $\mathbf{X}$ is Markov.

  \begin{proof}
    Let $f$ be a bounded Borel function. Take $f^*(x,y) = f(x+y)$. It is a bounded Borel function since $f$ is as well. Since $\mathbf{X}(s)$ is $\mathcal{F}^\mathbf{W}_s$-measurable and $\mathbf{X}(t) - \mathbf{X}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$, then, by Theorem \ref{thm:cond-exp-measurable-independent},
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
      &= \mathbb{E}(f^*(\mathbf{X}(s), \mathbf{X}(t) - \mathbf{X}(s)) \mid \mathcal{F}^\mathbf{W}_s)
      = g_{f^*}(\mathbf{X}(s))
    \end{align*}
    where $g_{f^*}$ is a Borel function.

    In other words, $\mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)$ is $\mathcal{F}_{\mathbf{X}(s)}$-measurable and thus
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
      = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}).
    \end{align*}
    That is, $\mathbf{X}$ is indeed a Markov process.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:fp-markov-composing}
  Let $g$ be an invertible Borel function and let $\mathbf{X}$ be a Markov process. If $\mathbf{Y}$ is a stochastic process given by $\mathbf{Y}(t) = g(\mathbf{X}(t))$, then $\mathbf{Y}$ is Markov.

  \begin{proof}
    Let $f$ be a bounded Borel function and let $0 \le s < t \le T$. Since $g$ is an invertible Borel function, then $\mathcal{F}_{\mathbf{X}(s)} = \mathcal{F}_{\mathbf{Y}(s)}$ by Theorem \ref{thm:bijection-filtration}. Morover, since $g$ is Borel, then $f^* = f \circ g$ is a bounded Borel function. Hence,
    \begin{align*}
      \mathbb{E}(f(\mathbf{Y}(t)) \mid \mathcal{F}^\mathbf{W}_s)
      &= \mathbb{E}(f^*(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(f^*(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}) \tag{$\mathbf{X}$ is Markov}\\
      &= \mathbb{E}(f(\mathbf{Y}(t)) \mid \mathcal{F}_{\mathbf{Y}(s)}),
    \end{align*}
    so $\mathbf{Y}$ is indeed Markov.
  \end{proof}
\end{theorem}

In the following, the notation $C^2_0(\mathbb{R}^d)$ denotes the set of twice-continuously differentiable functions on $\mathbb{R}^d$ such that they vanish at $\pm \infty$, i.e. for $f \in C^2_0(\mathbb{R}^d)$, we have
\begin{align*}
  \lim_{x \to \pm \infty} f(x) = 0.
\end{align*}
This implies that the first and second derivatives also vanish at $\pm \infty$.

We can finally state and prove the main result of this section. Note that it is also called the \textit{Forward Kolmogorov Equation} in other resources.

\begin{theorem}[Fokker-Planck Equation]\label{thm:fokker-planck}
  Let $\mathbf{X}(t)$ be an It\^o diffusion that is Markov, with transition probability density $\rho(\mathbf{x},t;\mathbf{y},s)$ that is in $C^2$ as a function of $\mathbf{x}$ and $t$, satisfying equations of the form
  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t, \mathbf{X}(t)) \mathrm{d}t + \sum_{j=1}^d b_{ij}(t, \mathbf{X}(t)) \mathrm{d}W_j(t),
  \end{align*}
  for $i=1,\ldots,d$. Then, for all $0 \le s < t \le T$ and $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$, we have
  \begin{align*}
    \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
    &= - \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)]\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)].
  \end{align*}

  \begin{proof}
    Let $f \in C^2_0(\mathbb{R}^d)$, let $0 \le s < t \le T$ and let $h$ be such that $0 \le t < t + h \le T$. Then, by the It\^o Formula (Theorem \ref{thm:ito-formula}),
    \begin{align*}
      f(\mathbf{X}&(t+h)) - f(\mathbf{X}(t))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\\
        &\ \ \ \ \ + \sum_{i,j=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) \mathrm{d}W_j(s)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s.
    \end{align*}

    Since $f$ and its partial derivatives in the previous equation are in $C_0$, then they are bounded. Indeed, for any $g \in C_0(\mathbb{R}^d)$ and any $\epsilon > 0$, there exists a closed rectangle $R \subseteq \mathbb{R}$ such that $g(\mathbf{x}) < \epsilon$ for $\mathbf{x} \in \mathbb{R}^d \setminus R$. Moreover, $R$ is compact and $g$ is continuous so $g$ is bounded on $R$ by the Boundedness Theorem. Hence, it is bounded on the whole $\mathbb{R}^d$.

    By Lemma \ref{lem:ito-diffusion-characteristics-m2}, $b_{ij}(s,\mathbf{X}(s)) \in \mathcal{M}^2$, so, together with the boundedness of $f_{x_i}(\mathbf{X}(s))$, we have that the integrands $f_{x_i}(\mathbf{X}(s))b_{ij}(s,\mathbf{X}(s)) \in \mathcal{M}^2$ in the previous equation are in $\mathcal{M}^2$. Therefore, by Theorem \ref{thm:stochastic-integral-expectation-m2} and by linearity of the expectation,
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\right)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s\right).
    \end{align*}
    By the Fubini Theorem, we have
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s.
    \end{align*}

    Therefore, we have
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \lim_{h \to 0} \frac{1}{h} (\mathbb{E}(f(\mathbf{X}(t+h))) - \mathbb{E}(f(\mathbf{X}(t))))\\
      &= \lim_{h \to 0} \frac{1}{h} \mathbb{E}(f(\mathbf{X}(t+h)) - f(\mathbf{X}(t))) \tag{linearity}\\
      &= \sum_{i=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s\\
      &= \sum_{i=1}^{d} \mathbb{E}(f_{x_i}(\mathbf{X}(t)) a_i(t,\mathbf{X}(t)))\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(t)) b_{ij}(t,\mathbf{X}(t)) b_{lj}(t,\mathbf{X}(t))\right)\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}.
    \end{align*}

    Using the Chapman-Kolmogorov Equation (Theorem \ref{thm:fp-chapman-kolmogorov}), we get
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x})\rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}
    \end{align*}
    and, by the Fubini Theorem,
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d}  \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.
    \end{align*}

    By integration by parts and since $f$ and its partial derivatives vanish at $\pm \infty$, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}
      &= - \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x},
    \end{align*}
    for all $i=1,\ldots,d$. Similarly using integration by parts twice, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) &b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}\\
      &= - \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) \frac{\partial}{\partial x_l}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}\\
      &= \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}.
    \end{align*}
    Hence,
    \begin{align}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\notag\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \left( - \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \right) \mathrm{d}\mathbf{y}\notag\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \left.\int_{\mathbb{R}^d} \right(\rho(\mathbf{y},s;\mathbf{X}(0),0) \notag\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.\int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \right)\mathrm{d}\mathbf{y}\notag\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \left(- \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)] \right.\notag\\
        &\ \ \ \ \ + \left.\frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \right) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.\label{eq:fp-proof-1}
    \end{align}

    But
    \begin{align}
      \frac{\partial}{\partial t}\mathbb{E}(f(\mathbf{X}(t)))
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{y}\notag\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\tag{Chapman-Kolmogorov}\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\tag{Fubini}\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\partial}{\partial t}[f(\mathbf{y}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0)] \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \tag{by the Leibniz Integral Rule since $f\rho \in C^2$}\\
      &= \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\label{eq:fp-proof-2}
    \end{align}
    since $f$ does not depend on $t$.

    But then, since \eqref{eq:fp-proof-1} and \eqref{eq:fp-proof-2} are equal for any choice of $f \in C^2_0(\mathbb{R}^d)$, we have
    \begin{align*}
      \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
      &= - \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}))\rho(\mathbf{x},t;\mathbf{y},s)]\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)],
    \end{align*}
    for all $\mathbf{y} \in \mathbb{R}^d$ such that $\rho(\mathbf{y},s;\mathbf{X}(0),0) \neq 0$, i.e. for all $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$, thus completing the proof.
  \end{proof}
\end{theorem}

\section{Local Volatility Model}

We specialise the multi-asset Black-Scholes model developed before by restricting the risky assets to be It\^o diffusions, i.e.
\begin{align*}
  \mathrm{d}S_i(t) = a_i(t,\mathbf{S}(t)) \mathrm{d}t + \sum_{j=1}^{d} b_{ij}(t,\mathbf{S}(t)) \mathrm{d}W_j(t),
\end{align*}
for all $i=1,\ldots,d$ for some $a,b$ as in Theorem \ref{thm:sde-solution}. By Theorem \ref{thm:bs-solution}, the prices of the risky assets are strictly positive. We can thus take
\begin{align*}
  \mu_i(t,\mathbf{S}(t)) = \frac{a_i(t, \mathbf{S}(t))}{S_i(t)},
  && c_{ij}(t,\mathbf{S}(t)) = \frac{b_{ij}(t, \mathbf{S}(t))}{S_i(t)},
\end{align*}
for $i,j=1,\ldots,d$, so that
\begin{align*}
  \mathrm{d}S_i(t) = \mu_i(t,\mathbf{S}(t)) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t,\mathbf{S}(t)) S_i(t) \mathrm{d}W_j(t),
\end{align*}
for all $i=1,\ldots,d$, where $\mu_i(t,\mathbf{S}(t)), c_{ij}(t,\mathbf{S}(t)), i,j=1,\ldots,d$ satisfy Assumptions \ref{ass:drift-vol-regularity} and \ref{ass:vol-matrix-invertible}. Such a model is called a \textit{local volatility model}.

\begin{theorem}\label{thm:local-stock-prices-markov}
  The prices and discounted prices of the risky assets are Markov under the risk-neutral probability $Q$.

  \begin{proof}
    By Theorem \ref{thm:bs-risk-neutral-dynamics-discounted}, the discounted prices $\widetilde{S}_i$, for $i=1,\ldots,d$, have zero drift. Moreover, by Assumption \ref{ass:drift-vol-regularity}, $\mathbf{C}(t,\mathbf{S}(t))$ is adapted and bounded, so it is in $\mathcal{M}^2$. Hence, by Lemma \ref{lem:fp-ito-diffusion-indep-increments}, $\widetilde{\mathbf{S}}$ has independent increments and, by Theorem \ref{thm:fp-ito-diffusion-markov}, $\widetilde{\mathbf{S}}$ is a Markov process.

    Moreover, since $g(\mathbf{x}) = e^{\int_0^t r(s) \mathrm{d}s} \mathbf{x}$ is an invertible Borel function such that $\mathbf{S}(t) = g(\widetilde{\mathbf{S}}(t))$, then $\mathbf{S}$ is also Markov by Theorem \ref{thm:fp-markov-composing}.
  \end{proof}
\end{theorem}

\begin{assumption}\label{ass:local-density}
  The price process of risky assets $\mathbf{S}$ admits a transition probability density $\rho(\mathbf{x},t;\mathbf{y},s)$ such that $\rho \in C^2$. We denote the transition function of $\mathbf{S}$ as $\nu$.
\end{assumption}

\begin{theorem}\label{thm:local-derivative-pricing}
  Let $H_\text{payoff} = f(\mathbf{S}(T))$ be the payoff of some derivative for some Borel function $f : \mathbb{R}^d \to \mathbb{R}$.
  If the derivative is replicable by a strategy whose value process $V$ has continuous paths, then
  \begin{align*}
    \widetilde{H}(t) = e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
  \end{align*}
  for all $t \in [0,T]$.

  \begin{proof}
    Let $t \in [0,T]$. By Theorem \ref{thm:bs-derivative-pricing},
    \begin{align*}
      \widetilde{H}(t)
      &= \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t)\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \mathbb{E}_Q(H(T) \mid \mathcal{F}^\mathbf{W}_t) \tag{$r$ is deterministic}\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \mathbb{E}_Q(f(\mathbf{S}(T)) \mid \mathcal{F}^\mathbf{W}_t)\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \nu(\mathrm{d}\mathbf{x},T;\mathbf{S}(t),t)
    \end{align*}
    since $\nu(\mathbf{\Gamma},T;\mathbf{S}(t),t) = P(\mathbf{S}(T) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_t)$ by definition of Markov process.

    But then, by Assumption \ref{ass:local-density},
    \begin{align*}
      \widetilde{H}(t)
      &= e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
    \end{align*}
    as required.
  \end{proof}
\end{theorem}

Hence,
%%%%%%%%%%%%%
\comment{unclear why "hence".}
%%%%%%%%%%%%%
 for all $t \in [0,T]$,
\begin{align*}
  H(t) = e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
\end{align*}
and, in particular,
\begin{align*}
  H(0) = e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(0),t) \mathrm{d}\mathbf{x}.
\end{align*}

\begin{corollary}\label{cor:local-option-pricing}
  The prices of European basket call and put options with weights $\mathbf{w}$ and strike price $K$ are
  \begin{align*}
    C_\mathbf{w}(t)
    &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} \left(\mathbf{w}^T\mathbf{x} - K\right)^+ \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},\\
    P_\mathbf{w}(t)
    &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} \left(K - \mathbf{w}^T\mathbf{x}\right)^+ \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
  \end{align*}
  for all $t \in [0,T]$.

  \begin{proof}
    The payoff of the call option is $(\mathbf{w}^T \mathbf{S}(T) - K)^+$ so it can be expressed as $g(\mathbf{S}(T))$ where $g(\mathbf{x}) = (\mathbf{w}^T \mathbf{x} - K)^+$ is a Borel function so the result follows for $C_\mathbf{w}$ from Theorem \ref{thm:local-derivative-pricing}.

    The same argument with $g(\mathbf{x}) = (K - \mathbf{w}^T \mathbf{x})$ gives us the result for $P_\mathbf{w}$.
  \end{proof}
\end{corollary}

%%%%%%%%%%%%%
\comment{The work is looking good. I suggest to add citations (cross-references to bibliography items) for all the theorems and lemmas given in the thesis.}
%%%%%%%%%%%%%

For the rest of this dissertation, we restrict ourselves to local volatility models.

\section{Dupire's Equation}

\tomcomment{Still work in progress from here} %%%%%%%%%%%%%
\comment{ok.}
%%%%%%%%%%%%%

% Give a detailed derivation of the Dupire’s equation (equation starting with ∂C = on page 171 in [2]). ∂T
% Use section 2 from [2] and the section ‘The continuous time theory’ from [3] as a source for the proof.

In the case where $d=1$, i.e. there is one risky asset driven by a unique Wiener process, an interesting result was derived by \textcite{dupire_pricing_1994} for local volatility models making use of the fact that, in such models, the price of the risky asset is a Markov process.

Since we work in a single-asset specialisation of the local volatility model developed in the previous section, we simply write $S$ for the risky asset, with dynamics
\begin{align*}
  \mathrm{d}S(t) = r(t) S(t) \mathrm{d}t + c(t,S(t)) S(t) \mathrm{d}W^Q(t).
\end{align*}

We can view an option on $S$ as a basket option with a single weight of 1 so the results from previous sections still hold in this degenerate case.

In this section, we fix $t \ge 0$ and let $C(T,K)$ denote the price at time $t$ of a European call option on the underlying $S$ with expiry time $T$ and strike price $K$.

\begin{theorem}\label{thm:dupire-asymptotic}
  The function $K \mapsto C(T,K)$ vanishes almost surely at infinity, i.e.
  \begin{align*}
    \lim_{K \to \infty} C(T,K) = 0 \text{ a.s.}
  \end{align*}

  \begin{proof}
    Let $(f_K)_{K > 0}$ be the sequence $f_K(x) = (x-K)^+ \rho(x,T;S(t),t)$. Note that
    \begin{align*}
      |f_K(x)|
      &\le |x \rho(x,T;S(t),t)|,
    \end{align*}
    for $x > 0$, and
    \begin{align*}
      \int_0^\infty |x\rho(x,T;S(t),t)| \mathrm{d}x
      &= \int_0^\infty x\rho(x,T;S(t),t) \mathrm{d}x \tag{positive integrand}\\
      &= \mathbb{E}_Q(S(T) \mid \mathcal{F}^W_t)
      = S(t) < \infty,
    \end{align*}
    since $S$ is a $Q$-martingale, so $x \rho(x,T;S(t),t)$ is integrable. Hence, by Lebesgue's Dominated Convergence Theorem, we have
    \begin{align*}
      \lim_{K \to \infty} C(T,K)
      &= \lim_{K \to \infty} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_\mathbb{R} (x-K)^+ \rho(x,T;S(t),t) \mathrm{d}x\right) \\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \lim_{K \to \infty} \int_0^\infty f_K(x) \mathrm{d}x\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_0^\infty \lim_{K \to \infty} f_K(x) \mathrm{d}x\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_0^\infty 0 \mathrm{d}x = 0,
    \end{align*}
    since $(f_K)$ converges pointwise to $0$.
  \end{proof}
\end{theorem}

\begin{lemma}\label{lem:dupire-asymptotic-dK}
  The following expressions also vanish as $K \to \infty$:
  \begin{align*}
    K \frac{\partial}{\partial K} C(T,K), && K^2 \frac{\partial^2}{\partial K^2} C(T,K).
  \end{align*}

  \begin{proof}
    Since
    \begin{align*}
      \frac{\partial}{\partial K} (K C(T,K)) = K \frac{\partial}{\partial K} C(T,K) + C(T,K),
    \end{align*}
    then
    \begin{align*}
      K \frac{\partial}{\partial K} C(T,K)
      &= \frac{\partial}{\partial K} (K C(T,K)) - C(T,K)\\
      &= \frac{\partial}{\partial K} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty K(x-K) \rho(x,T;S(t),t)\mathrm{d}x \right) - C(T,K)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \frac{\partial}{\partial K} K(x-K) \rho(x,T;S(t),t)\mathrm{d}x\\
        &\ \ \ \ - K(K-K)\rho(K,T;S(t),t) - C(T,K) \tag{Leibniz integral rule}\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (x-2K) \rho(x,T;S(t),t)\mathrm{d}x - C(T,K)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_\mathbb{R} (x-2K)^+ \rho(x,T;S(t),t)\mathrm{d}x - C(T,K)\\
      &= C(T,2K) - C(T,K),
    \end{align*}
    so
    \begin{align}
      \lim_{K \to \infty} \left(K \frac{\partial}{\partial K} C(T,K)\right)
      &= \lim_{K \to \infty} C(T,2K) - \lim_{K \to \infty} C(T,K)
      = 0,\label{eq:dupire-asymptotic-dK}
    \end{align}
    by Theorem \ref{thm:dupire-asymptotic}.

    Similarly, since
    \begin{align*}
      \frac{\partial^2}{\partial K^2} (K^2 C(T,K))
      = 2C(T,K) + 4K \frac{\partial}{\partial K} C(T,K) + K^2 \frac{\partial^2}{\partial K^2} C(T,K)
    \end{align*}
    and
    \begin{align*}
      \frac{\partial^2}{\partial K^2} (K^2 C(T,K))
      &= e^{-\int_t^T r(s)\mathrm{d}s} \frac{\partial^2}{\partial K^2} \int_K^\infty K^2 (x-K) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= e^{-\int_t^T r(s)\mathrm{d}s} \int_K^\infty (2x-6K) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= 2 e^{-\int_t^T r(s)\mathrm{d}s} \int_\mathbb{R} (x-3K)^+ \rho(x,T;S(t),t) \mathrm{d}x\\
      &= 2 C(T,3K),
    \end{align*}
    then
    \begin{align*}
      \lim_{K \to \infty} &\left(K^2 \frac{\partial^2}{\partial K^2} C(T,K)\right)\\
      &= - 2\lim_{K \to \infty} C(T,K)
        - 4 \lim_{K \to \infty} \left(K \frac{\partial}{\partial K} C(T,K)\right)
        + 2 \lim_{K \to \infty} C(T,3K)
      = 0,
    \end{align*}
    by Theorem \ref{thm:dupire-asymptotic} and \eqref{eq:dupire-asymptotic-dK}
  \end{proof}
\end{lemma}

Considering Theorem \ref{thm:dupire-asymptotic}, it seems reasonable to make the following assumption.

\begin{assumption}\label{ass:dupire-asymptotic-dT}
  The partial derivative $\frac{\partial}{\partial T} C(T,K)$ also vanishes at infinity, i.e.
  \begin{align*}
    \lim_{K \to \infty} \frac{\partial}{\partial T} C(T,K) = 0.
  \end{align*}
\end{assumption}

\begin{theorem}[Dupire Equation]
  The function $C(T,K)$ satisfies the partial differential equation
  \begin{align*}
    \frac{\partial}{\partial T}C(T,K) = \frac{1}{2} c(T,K)^2 \frac{\partial^2}{\partial K^2} C(T,K) - r(T) K \frac{\partial}{\partial K}C(T,K).
  \end{align*}

  \begin{proof}
    By Corollary \ref{cor:local-option-pricing}, we have
    \begin{align*}
      C(T,K) &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}} \left(x - K\right)^+ \rho(x,T;S(t),t) \mathrm{d}x.
    \end{align*}

    Dividing by the discounting factor and differentiating with respect to $K$, we obtain
    \begin{align*}
      \frac{\partial}{\partial K}&\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right)\\
      &= \frac{\partial}{\partial K} \int_{\mathbb{R}} \left(x - K\right)^+ \rho(x,T;S(t),t) \mathrm{d}x\\
      &= \frac{\partial}{\partial K} \int_K^\infty \left(x - K\right) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= \int_K^\infty \frac{\partial}{\partial K} \left(x - K\right) \rho(x,T;S(t),t) \mathrm{d}x - (K-K) \rho(K,T;S(t),s) \frac{\partial}{\partial K} K \tag{Leibniz Integral Rule}\\
      &= - \int_K^\infty \rho(x,T;S(t),t) \mathrm{d}x.
    \end{align*}
    Differentiating with respect to $K$ once again, we have
    \begin{align}
      \frac{\partial^2}{\partial K^2}\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right)
      &= - \frac{\partial}{\partial K}\int_K^\infty \rho(x,T;S(t),t) \mathrm{d}x\notag\\
      &= \rho(K,T;S(t),t),\label{eq:dupire-rho}
    \end{align}
    by the Fundamental Theorem of Calculus.

    By the Fokker-Plank Equation (Theorem \ref{thm:fokker-planck}) with $a(t,x) \coloneq r(t) x$ and $b(t,x) := c(t,x) x$ at the point $(K,T)$, we have
    \begin{align*}
      \frac{\partial}{\partial T}&\rho(K,T;S(t),t)\\
      &= - \frac{\partial}{\partial K}[r(T) K \rho(K,T;S(t),t)]
        + \frac{1}{2} \frac{\partial^2}{\partial K^2}[c(T,K)^2 K^2 \rho(K,T;S(t),t)].
    \end{align*}
    Substituting $\rho(K,T;S(t),t)$ using \eqref{eq:dupire-rho} in the term on the LHS, we obtain
    \begin{align*}
      \frac{\partial}{\partial T}&\rho(K,T;S(t),t)\\
      &= \frac{\partial^2}{\partial K^2} \frac{\partial}{\partial T}\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right) \tag{by Schwarz's Theorem since $\rho \in C^2$}\\
      &= \frac{\partial^2}{\partial K^2} \left(e^{\int_t^T r(s) \mathrm{d}s} \left(r(T) C(T,K) + \frac{\partial}{\partial T}C(T,K)\right)\right).
    \end{align*}
    Next, integrating the term in the partial derivative with respect to $K$, we have
    \begin{align*}
      \int r(T) K &\rho(K,T;S(t),t) \mathrm{d}K\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \int K \frac{\partial^2}{\partial K^2} C(T,K) \mathrm{d}K \tag{by \eqref{eq:dupire-rho}}\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - \int \frac{\partial}{\partial K} C(T,K) \mathrm{d}K \right)\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) + \gamma(T)\right),
    \end{align*}
    by integration by parts, where $\gamma(T)$ is the constant of integration, so
    \begin{align*}
      \frac{\partial}{\partial K} &[r(T) K \rho(K,T;S(t),t)]\\
      &= \frac{\partial^2}{\partial K^2} \int r(T) K \rho(K,T;S(t),t) \mathrm{d}K\\
      &= \frac{\partial^2}{\partial K^2} \left[r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) + \gamma\right)\right].
    \end{align*}
    Hence, substituting in the Fokker-Planck equation and integrating twice with respect to $K$, we get
    \begin{align*}
      e^{\int_t^T r(s) \mathrm{d}s} &\left(r(T) C(T,K) + \frac{\partial}{\partial T}C(T,K)\right)\\
      &= - r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) \right)\\
        &\ \ \ \ \ + \frac{1}{2} c(T,K)^2 K^2 e^{\int_t^T r(s) \mathrm{d}s}\frac{\partial^2}{\partial K^2} C(T,K) + \alpha(T) K + \beta(T),
    \end{align*}
    where $\alpha(T), \beta(T)$ are the constants of integration of each integration ($\gamma$ is incorporated into $\beta$ in the process). Dividing by the discounting factor and noticing that the terms in $r(T)C(T,K)$ cancel out, we have
    \begin{align*}
      \frac{\partial}{\partial T}C(T,K)
      &= - r(T) K \frac{\partial}{\partial K} C(T,K)
        + \frac{1}{2} c(T,K)^2 K^2 \frac{\partial^2}{\partial K^2} C(T,K)\\
        &\ \ \ \ \ + \alpha(T) K + \beta(T).
    \end{align*}
    Finally, when $K \to \infty$, all terms besides the constants of integration vanish by Lemma \ref{lem:dupire-asymptotic-dK} and Assumption \ref{ass:dupire-asymptotic-dT} so the constants must be zero themselves. That is,
    \begin{align*}
      \frac{\partial}{\partial T}C(T,K)
      &= - r(T) K \frac{\partial}{\partial K} C(T,K)
        + \frac{1}{2} c(T,K)^2 K^2 \frac{\partial^2}{\partial K^2} C(T,K)
    \end{align*}
    as expected.
  \end{proof}
\end{theorem}

\section{Generalisation to Multiple Assets}

% Provide the setup and give a detailed proof of Theorem 1 from [2]. This should be based on section 3 from [2]

FIXME: Show that a similar approach than in the previous section doesn't lead anywhere.

Since, contrary to Dupire's Equation in the single-asset case, the partial differential equation that is satisfies by the prices of European basket call options will turn out to involve partial derivatives with respect to the expiry time $T$ and to the weights $w_i, i=1,\ldots,d$, we fix $t$ and $K$ and denote by $C(T,\mathbf{w})$ the price at time $t$ of a European basket call option with expiry time $T$, strike price $K$ and weights $\mathbf{w}$.

Similary to the proof of Dupire's equation, we will want to be able to express the integral without the positive part to make it differentiable. However, we then have
\begin{align}\label{eq:generalisation-int-over-V}
  C(T,\mathbf{w}) = e^{-\int_t^T r(s) \mathrm{d}s} \int_V (\mathbf{w}^T\mathbf{x} - K) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
\end{align}
where $V = \{ \mathbf{x} \in \mathbb{R}^d : \mathbf{w}^T \mathbf{x} \ge K \}$ and the space $V$ depends on the weights, thus constituting a difficulty when calculating partial derivatives such as $\frac{\partial}{\partial w_i}C(T,\mathbf{w})$. Instead, we can circumvent this difficulty with a change of variable.

\begin{lemma}\label{lem:generalisation-var-change}
  Let $B = \mathbf{w}^T \mathbf{x}$ and $\mathbf{Q} = (Q_1,\ldots,Q_{d-1})$ with
  \begin{align}\label{eq:generalisation-var-change-Q}
    Q_i = \frac{w_i x_i}{B} = \frac{w_i x_i}{\mathbf{w}^T \mathbf{x}}.
  \end{align}
  Then
  \begin{align*}
    C(T,\mathbf{w}) = e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
  \end{align*}
  where $A = \{\mathbf{Q} : Q_i \ge 0, i=1,\ldots,d-1, \sum_{i=1}^{d-1} Q_i \le 1\}$ and
  \begin{align*}
    \mathbf{x}(\mathbf{Q},B) = \left(\frac{Q_1 B}{w_1},\dots, \frac{Q_{d-1} B}{w_{d-1}}, \frac{(1 - \sum_{i=1}^{d-1} Q_i) B}{w_{d}}\right).
  \end{align*}

  \begin{proof}
    We have
    \begin{align*}
      \frac{\partial}{\partial Q_j} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_j} \frac{Q_i B}{w_i} = 0, \text{ for } i \neq j,\\
      \frac{\partial}{\partial Q_i} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_i} \frac{Q_i B}{w_i} = \frac{B}{w_i}, \text{ for } i = 1,\ldots,d-1,\\
      \frac{\partial}{\partial Q_i} x_d(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_i} \frac{(1-\sum_{k=1}^{d-1} Q_k) B}{w_d} = \frac{-B}{w_d}, \text{ for } i = 1,\ldots,d-1,\\
      \frac{\partial}{\partial B} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial B} \frac{Q_i B}{w_i} = \frac{Q_i}{w_i}, \text{ for } i=1,\ldots,d-1,\\
      \frac{\partial}{\partial B} x_d(\mathbf{Q},B)
      &= \frac{\partial}{\partial B} \frac{(1-\sum_{i=1}^{d-1} Q_i) B}{w_d} = \frac{1-\sum_{i=1}^{d-1} Q_i}{w_d}.
    \end{align*}
    so the Jacobian of the change of variable is
    \begin{align*}
      J = \frac{\partial \mathbf{x}}{\partial (\mathbf{Q}, B)} = \begin{bmatrix}
        \frac{B}{w_1} & 0 & \cdots & 0 & \frac{-B}{w_d}\\
        0 & \frac{B}{w_2} & \cdots & 0 & \frac{-B}{w_d}\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & \frac{B}{w_{d-1}} & \frac{-B}{w_d}\\
        \frac{Q_1}{w_1} & \cdots & \cdots & \frac{Q_{d-1}}{w_{d-1}} & \frac{1-\sum_{i=1}^{d-1} Q_i}{w_d}
      \end{bmatrix}.
    \end{align*}
    We apply a sequence elementary operations to calculate the determinant of the Jacobian. Dividing each column by $w_1, \ldots, w_d$ respectively, we obtain
    \begin{align*}
      A_1 &= \begin{bmatrix}
        B & 0 & \cdots & 0 & -B\\
        0 & B & \cdots & 0 & -B\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & B & -B\\
        Q_1 & \cdots & \cdots & Q_{d-1} & 1-\sum_{i=1}^{d-1} Q_i
      \end{bmatrix}
    \end{align*}
    and $\det J = \frac{\det A_1}{w_1 \cdots w_d}$. Next, we add each of the first $d-1$ columns to the last one, thus obtaining
    \begin{align*}
      A_2 &= \begin{bmatrix}
        B & 0 & \cdots & 0 & 0\\
        0 & B & \cdots & 0 & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & B & 0\\
        Q_1 & \cdots & \cdots & Q_{d-1} & 1
      \end{bmatrix}
    \end{align*}
    and $\det A_2 = \det A_1$. But $\det A_2 = B^{d-1}$ so
    \begin{align*}
      \det J = \frac{\det A_1}{w_1 \cdots w_d} = \frac{\det A_2}{w_1 \cdots w_d} = \frac{B^{d-1}}{w_1 \cdots w_d}.
    \end{align*}

    Hence, with $V$ as in \eqref{eq:generalisation-int-over-V},
    \begin{align*}
      C(T,\mathbf{w})
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_V \left(\mathbf{w}^T\mathbf{x} - K\right) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x}\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) |\det J| \mathrm{d}\mathbf{Q} \mathrm{d}B\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B.
    \end{align*}
    as required.
  \end{proof}
\end{lemma}

\begin{theorem}
  The function $C(T,\mathbf{w})$ satisfies the partial differential equation
  \begin{align*}
    FIXME
  \end{align*}

  \begin{proof}
    We begin by calculating the partial derivatives of $C(T,\mathbf{w})$ with respect to $w_i$ for some $i=1,\ldots,d$ and with respect to $T$ using the result from Lemma \ref{lem:generalisation-var-change}. For $i=1,\ldots,d$, we have
    \begin{align*}
      &\frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
      &= \frac{\partial}{\partial w_i} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B\right)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s}\int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial w_i} \left(\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}\right) \mathrm{d}\mathbf{Q} \mathrm{d}B,
    \end{align*}
    by repeated use of the Leibniz integral rule since
    \begin{align*}
      \int_K^\infty \int_A & f(\mathbf{Q},B) \mathrm{d}\mathbf{Q} \mathrm{d}B \\
      &= \int_K^\infty \int_0^1 \int_{Q_1}^1 \cdots \int_{Q_1 + \cdots + Q_{d-2}}^1 f(\mathbf{Q},B) \mathrm{d}Q_{d-1} \cdots \mathrm{d}Q_2 \mathrm{d}Q_1 \mathrm{d}B,
    \end{align*}
    for all integrable $f$. In this context, $\mathbf{Q}$ is quantified over by the integral rather than being defined as in \eqref{eq:generalisation-var-change-Q} so it does not depend on $\mathbf{w}$ and
    \begin{align*}
      \frac{\partial}{\partial w_i} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial w_i} \frac{Q_i B}{w_i}
      = \frac{- Q_i B}{w_i^2}, \text{ for } i=1,\ldots,d-1,\\
      \frac{\partial}{\partial w_d} x_d(\mathbf{Q},B)
      &= \frac{\partial}{\partial w_d} \frac{(1 - \sum_{i=1}^{d-1} Q_i) B}{w_d}
      = \frac{\partial}{\partial w_d} \frac{Q_d B}{w_d}
      = \frac{- Q_d B}{w_d^2},
    \end{align*}
    where we take $Q_d = 1 - \sum_{i=1}^{d-1} Q_i$. Thus,
    \begin{align*}
      &\frac{\partial}{\partial w_i} \left(\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}\right)\\
      &= \frac{\partial}{\partial w_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}
        + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &= \frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} x_i(\mathbf{Q},B) \frac{B^{d-1}}{w_1 \cdots w_d}\\
        &\ \ \ \ \ + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} \frac{B^{d-1}}{w_1 \cdots w_d} \\
      &= \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{-Q_i B}{w_i^2} \frac{B^{d-1}}{w_1 \cdots w_d}
        + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{-1}{w_i} \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &= -\frac{1}{w_i} \left(\frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{Q_i B}{w_i}
        + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right) \frac{B^{d-1}}{w_1 \cdots w_d}.
    \end{align*}
    Hence,
    \begin{align}
      &\frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
      &= - \frac{1}{w_i}e^{-\int_t^T r(s) \mathrm{d}s}\int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\notag\\
        &\ \ \ \ \ \left(x_i(\mathbf{Q},B)\frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right) \mathrm{d}\mathbf{Q} \mathrm{d}B.\notag\\
      &= - \frac{1}{w_i}e^{-\int_t^T r(s) \mathrm{d}s}\notag\\
      &\ \ \ \ \ \ \ \ \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}
         \frac{\partial}{\partial x_i}\left[x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{Q} \mathrm{d}B.\label{eq:generalisation-dC-dw}
    \end{align}

    As in the proof of Dupire's Equation, we make use of the Fokker-Planck Equation (Theorem \ref{thm:fokker-planck}). For all $\mathbf{x} \in \mathrm{Im}(\mathbf{S}(T))$, we have
    \begin{align}
      \sum_{i=1}^d &\frac{\partial}{\partial x_i}[r(T) x_i \rho(\mathbf{x},T;\mathbf{S}(t),t)]\notag\\
        &= \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[c_{ij}(T,\mathbf{x}) x_i(T) c_{lj}(T,\mathbf{x}) x_l(T) \rho(\mathbf{x},T;\mathbf{S}(t),t)]\notag\\
        &\ \ \ \ -\frac{\partial}{\partial T}\rho(\mathbf{x},T;\mathbf{S}(t),t).\label{eq:generalisation-fokker-planck}
    \end{align}
    Note that this equation holds almost surely since it holds if we substitute $\mathbf{S}(t)$ for any $\mathbf{y} \in \mathrm{supp}(\mathbf{S}(t))$.

    But then, by \eqref{eq:generalisation-dC-dw},
    \begin{align*}
      \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
      &= - \sum_{i=1}^{d} \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\\
         &\ \ \ \ \ \ \ \ \ \ \ \ \  r(T)\frac{\partial}{\partial x_i}\left[x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{Q} \\
      &= - \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\\
         &\ \ \ \ \ \ \ \ \ \ \ \ \ \sum_{i=1}^{d} \left( r(T)\frac{\partial}{\partial x_i}\left[x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \right) \mathrm{d}\mathbf{Q} \mathrm{d}B,
    \end{align*}
    and therefore, by \eqref{eq:generalisation-fokker-planck},
    \begin{align}
      \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
      &= \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}
        \left( \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right.\notag\\
        &\ \ \ \ \ \ \ - \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T,\mathbf{x}(\mathbf{Q},B)) x_i(\mathbf{Q},B) \notag\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ c_{lj}(T,\mathbf{x}(\mathbf{Q},B)) x_l(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B. \label{eq:generalisation-one-deriv}
    \end{align}

    Next, we calculate the partial derivative of call prices with respect to expiry time. Using Lemma \ref{lem:generalisation-var-change} once again, we have
    \begin{align*}
      \frac{\partial}{\partial T} &C(T,\mathbf{w})\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T}\int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
    \end{align*}
    by repeated use of the Leibniz integral rule. But then
    \begin{align*}
      e^{\int_t^T r(s) \mathrm{d}s} &\frac{\partial}{\partial T} C(T,\mathbf{w})\\
      &= \int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
    \end{align*}
    so, substituting in \eqref{eq:generalisation-one-deriv}, we obtain
    \begin{align*}
      \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
      &= e^{\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T} C(T,\mathbf{w}) - \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\notag\\
        &\ \ \ \ \ \ \ \Bigg( \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T,\mathbf{x}(\mathbf{Q},B)) x_i(\mathbf{Q},B) \notag\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ c_{lj}(T,\mathbf{x}(\mathbf{Q},B)) x_l(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B.
    \end{align*}
    Applying Lemma \ref{lem:generalisation-var-change} the other way around, we obtain
    \begin{align}
      \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
      &= e^{\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T} C(T,\mathbf{w}) - \frac{1}{2} \sum_{j=1}^{d} \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) \notag\\
        &\ \ \ \ \ \ \ \Bigg( \sum_{i,l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{x}. \label{eq:generalisation-two-derivs}
    \end{align}

    For any $j = 1,\ldots,d$, we are now concerned with the integral
    \begin{align*}
      I_j \coloneq \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) \left( \sum_{i,l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \right) \mathrm{d}\mathbf{x}.
    \end{align*}

    Let $\mathbf{F} = (F_1,\ldots,F_d)$ be the vector field such that
    \begin{align*}
      F_l(\mathbf{x}) = \frac{\partial}{\partial x_i} \big[c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big],
    \end{align*}
    for $l=1,\ldots,d$, whose divergence is
    \begin{align*}
      \nabla \cdot \mathbf{F} = \sum_{l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big],
    \end{align*}
    so that
    \begin{align*}
      I_j = \sum_{i=1}^{d} \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) (\nabla \cdot \mathbf{F})(\mathbf{x}) \mathrm{d}\mathbf{x}.
    \end{align*}
    Since
    \begin{align*}
      \nabla \cdot ((\mathbf{w}^T \mathbf{x} - K) \mathbf{F}
      &= \nabla (\mathbf{w}^T \mathbf{x} - K) \cdot \mathbf{F} + (\mathbf{w}^T \mathbf{x} - K) (\nabla \cdot \mathbf{F})\\
      &= \mathbf{w} \cdot \mathbf{F} + (\mathbf{w}^T \mathbf{x} - K) (\nabla \cdot \mathbf{F}),
    \end{align*}
    then
    \begin{align*}
      I_j = \sum_{i=1}^{d} \left[\int_V \nabla \cdot \left(\left(\mathbf{w}^T \mathbf{x} - K\right) \mathbf{F}(\mathbf{x})\right) \mathrm{d}\mathbf{x} - \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x} \right].
    \end{align*}
    Let $S$ be the boundary of $V$. Hence,
    \begin{align*}
      S = \{\mathbf{x} : \mathbf{w}^T \mathbf{x} = K\},
    \end{align*}
    so, by the Divergence Theorem, we have
    \begin{align*}
      I_j &= \sum_{i=1}^{d} \left[\int_S \left(\mathbf{w}^T \mathbf{x} - K\right) \mathbf{F}(\mathbf{x}) \cdot \mathrm{d}\mathbf{x} - \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x} \right]\\
      &= - \sum_{i=1}^{d} \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x}\\
      &= - \sum_{l=1}^{d} w_l \int_V \sum_{i=1}^d \frac{\partial}{\partial x_i} \big[c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \mathrm{d}\mathbf{x},
    \end{align*}
    since $(\mathbf{w}^T \mathbf{x} - K) = 0$ on $S$.

    Similarly, let $\mathbf{G} = (G_1,\ldots,G_d)$ be the vector field such that
    \begin{align*}
      G_i(\mathbf{x}) = c_{ij}(T,\mathbf{x}) x_i c_{lj}(T,\mathbf{x}) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t),
    \end{align*}
    for $i=1,\ldots,d$, so that
    \begin{align*}
      I_j
      &= - \sum_{l=1}^{d} w_l \int_V \nabla \cdot \mathbf{G}(\mathbf{x}) \mathrm{d}\mathbf{x}
      = - \sum_{l=1}^{d} w_l \int_S \mathbf{G}(\mathbf{x}) \cdot \mathrm{d}\mathbf{x}.
    \end{align*}
    Since $S$ is a level set of $\mathbf{x} \mapsto \mathbf{w}^T \mathbf{x}$, the gradient $\nabla (\mathbf{w}^T \mathbf{x}) = \mathbf{w}$ is perpendicular to the tangent plane of $S$ at any $\mathbf{x}$ and points in the direction of increasing values of $\mathbf{w}^T \mathbf{x}$, i.e. it points towards $V$. In other words, $- \frac{\mathbf{w}}{\|\mathbf{w}\|}$ is a normal vector of $S$ pointing outwards. Therefore,
    \begin{align*}
      I_j
      = - \sum_{l=1}^{d} w_l \int_S \mathbf{G}(\mathbf{x}) \cdot \frac{-\mathbf{w}}{\|\mathbf{w}\|} \mathrm{d}\mathbf{x}.
    \end{align*}

    FIXME
  \end{proof}
\end{theorem}

\subsection{Numerical Example}

% Give a numerical example of how Theorem 1 can be applied to recover aij. You can restrict to the simplest setting of a two dimensional Black-Scholes model.

FIXME

%%%%%%%%%%%%%
\comment{we should fix the bibliography items so that they do not stick out onto the margin.}
%%%%%%%%%%%%%

\pagebreak
\printbibliography

\end{document}
