\documentclass[english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
%\usepackage[latin9]{inputenc}
\usepackage{color}
\definecolor{note_fontcolor}{rgb}{0.800781, 0.800781, 0.800781}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{needspace}

\definecolor{lightlightgray}{gray}{0.97}
\definecolor{commentgray}{gray}{0.7}
\usepackage{caption}
\usepackage{courier}
\usepackage{listings}
\lstset{
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{lightlightgray},
  commentstyle=\color{commentgray},
  stringstyle=\color{teal},
  xleftmargin=-2.5cm,
  xrightmargin=-2.5cm,
  framesep=5pt,
  rulesepcolor=\color{lightgray},
  frame=shadowbox,
  showstringspaces=false
}
\captionsetup[lstlisting]{font={small,tt}}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter

\newcommand{\comment}[1]{\color{blue}#1\color{black}}
\newcommand{\tomcomment}[1]{\color{orange}#1\color{black}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\DeclareRobustCommand*{\lyxarrow}{%
\@ifstar
{\leavevmode\,$\triangleleft$\,\allowbreak}
{\leavevmode\,$\triangleright$\,\allowbreak}}
%% The greyedout annotation environment
\newenvironment{lyxgreyedout}
  {\textcolor{note_fontcolor}\bgroup\ignorespaces}
  {\ignorespacesafterend\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\newtheoremstyle{bolddescit}{}{}{\itshape}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\newtheoremstyle{bolddesc}{}{}{}{}{\bfseries}{.}{ }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{bolddescit}
\newtheorem{theorem}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[theorem]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
\theoremstyle{bolddesc}
\newtheorem{assumption}[theorem]{\protect\assumptionname}
\theoremstyle{plain}
\newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\usepackage{amsfonts}
\newcommand{\commentMJC}[1]{{\color{red}#1}}

\makeatother

\providecommand{\assumptionname}{Assumption}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Pricing Basket Options with a Generalisation of Dupire's Equation}
\author{Thomas Feron}
\date{~}

\maketitle
\vspace{2.5in}

\noindent \begin{center}
Dissertation submitted for the MSc in Mathematical Finance
\par\end{center}

\begin{center}
\bigskip{}
\par\end{center}

\begin{center}
Department of Mathematics

University of York\bigskip{}
\par\end{center}

\begin{center}
\today
\par\end{center}

\vspace{1in}

\begin{center}
Supervisor: Maciej J. Capi\'nski
\par\end{center}

\newpage{}

\tableofcontents{}\newpage{}

\pagebreak

%%%%%%%%%%%%%
\comment{a section "Introduction" should start here }
%%%%%%%%%%%%%

\section{Introduction}

The classic Black-Scholes model assumes that the volatility is constant. This has the advantage that the model is simple and interesting results can be derived, most notably the Black-Scholes formula that allows us to find the price of European options given a certain volatility. By inverting this relation, we can also find the so-called \textit{implied volatility} from prices of traded European options. Unfortunately, the implied volatility is not constant and, in fact, vary as a function of expiry time or of strike price. These relationships are both referred to as the \textit{volatility smile} due to the shape of their graphs.

Several extensions of the Black-Scholes model have been proposed to solve this discrepancy. In particular, \textit{local volatility models} have the advantage of being simpler than \textit{stochastic volatility models} while still being able to fit the volatility smile. In such a model, \textcite{dupire_pricing_1993} has shown that the prices of European call options satisfy a partial differential equation, the so-called \textit{Dupire Equation}.

Later, \textcite{amster_towards_2009} attempted to generalise Dupire's Equation to multiple assets.
They showed that the prices of European \textit{basket} call options, i.e. options whose underlying is a basket of assets with fixed weights, also satisfy some partial differential equation. This PDE is related to Dupire's Equation but has a different form. Indeed, it has partial derivatives with respect to expiry time and weights while Dupire's Equation has partial derivates with respect to expiry time and strike price. Moreover, the PDE holds if the volatility is time-dependent only. Nonetheless, we will refer to it as the \textit{generalisation} of Dupire's Equation here for simplicity.

The goal of this dissertation is to present the necessary background for these results and to present them in detail, taking particular care to make each step explicit.

We begin by establishing the prerequisites about stochastic processes and stochastic calculus in Section~\ref{sec:preliminaries}. Most results in this section are stated without proofs but references are provided in each case. The \textit{multi-asset Black-Scholes model with variable coefficients} is then presented in Section~\ref{sec:black-scholes}, where we also define many concepts and show important results to \textit{price} basket options. In Section~\ref{sec:fokker-planck}, we define \textit{Markov processes} and derive the \textit{Fokker-Planck Equation} and the conditions for a Markov process to satisfy it.

Section~\ref{sec:local-vol} presents a restriction of the model developed in Section~\ref{sec:black-scholes} where the volatility coefficients are deterministic functions of the time and the prices of the risky assets, the so-called \textit{local volatility model}. We show that, in this model, the prices of the risky assets satisfy the Fokker-Planck Equation.

This key property of local volatility models is used to prove Dupire's Equation in Section~\ref{sec:dupire} in the single-asset case. Indeed, we will see that the second-order partial derivative of the call option price with respect to the strike price is equal to the transition probability density of the price of the underlying asset, which itself satisfies the Fokker-Planck Equation. This leads to a PDE satisfied by call option prices.

Finally, once again using the Fokker-Planck Equation, we show its generalisation to multiple assets in Section~\ref{sec:generalisation}. However, in the multi-asset case, the second-order partial derivative of basket call option prices with respect to the strike price is not equal to the transition probability density so we have to instead use the partial derivatives with respect to the weights of the basket. We end up deriving a PDE that the prices of basket call options satisfy.

We also provide a numerical example that uses this generalisation to price basket options using finite difference methods.  The implementation of that method in C++ is provided in the appendix as well as in accompanying source files. The prices returned using this method are compared to the ones obtained from Monte Carlo simulations as a sanity check. Some evidence is then given that the model is able to fit the volatility smile to some extent.
%%%%%%%%%%%%%
\comment{here also we should go into more details, allocating say a paragraph for the description of the content.}
%%%%%%%%%%%%%

Most results are provided with the reference on which they are based. However, some results were required for which no references were found. A proof is always provided in these cases.

\section{Contribution}

In general, the main contribution of this dissertation is to collate and harmonise the material from different sources using various notations, terminologies, or even definitions, into a coherent text, as well as expanding on the proofs from the references to present them in finer detail. More specifically, the following contributions were made.

Lemma~\ref{lem:ito-diffusion-characteristics-m2} was needed in the proof of Theorem~\ref{thm:fokker-planck} but no references to this result could be found, so a proof is provided here.

The construction of a risk-neutral probability for the multi-asset Black-Scholes model in Theorem~\ref{thm:bs-risk-neutral-prob} is based on \textcite{capinski_blackscholes_2012} where the existence of such a probability measure is only shown in the case of deterministic drift and volatility coefficients. We extend it here for stochastic coefficients that are bounded by a deterministic constant. We also provide a more detailed proof.

The multi-asset Black-Scholes model with variable coefficients is presented here based on \textcite{capinski_blackscholes_2012}. Most results from this reference are concerned with the classic Black-Scholes model (single asset, constant coefficients). Here, they are extended and the proofs are adapted to the more general model. More specifically, this is the case for Theorem~\ref{thm:bs-risk-neutral-dynamics}, Theorem~\ref{thm:bs-risk-neutral-dynamics-discounted}, Theorem~\ref{thm:bs-repl-strat-derivative-prices}, Theorem~\ref{thm:bs-replicability} and Theorem~\ref{thm:bs-derivative-pricing}.

In the reference on which the proof of Theorem~\ref{thm:bs-repl-strat-derivative-prices} is based, the theorem is proved directly. Here, besides extending the proof to the multi-asset case, we first prove a simpler lemma (Lemma~\ref{lem:bs-non-negative-strat}) which is easier to show and then makes the proof of the theorem trivial.

In \textcite{pavliotis_stochastic_2014}, the proof of the Fokker-Planck Equation is rather short (less than a page) and uses a different definition of It\^o diffusion. Here, we provide a more detailed proof (about 3.5 pages) based on the definition of It\^o diffusion as a solution to an SDE (see Definition~\ref{def:ito-diffusion}).

In \textcite{oksendal_stochastic_2003}, it is shown that all It\^o diffusions are Markov. However, it was simpler to give alternative sufficient conditions for processes to be Markov in this context. We can then use these results to show that the prices of risky assets in the local volatility model, which are incidentally It\^o diffusions, are in fact Markov. These results are Lemma~\ref{lem:fp-ito-diffusion-indep-increments}, Theorem~\ref{thm:fp-ito-diffusion-markov} and Theorem~\ref{thm:fp-markov-composing}.

Adapting the pricing results from the multi-asset Black-Scholes model with variable coefficients to the local volatility model to express them in terms of the transition probability density of the risky assets was sufficiently simple that it is not based on any reference. These results are Theorem~\ref{thm:local-derivative-pricing} and Corollary~\ref{cor:local-option-pricing}.

In \textcite{dupire_pricing_1993}, the asymptotic behaviour of the option price as the strike price goes to infinity as well as the asymptotic behaviour of its partial derivatives is merely mentioned. Here, we prove them in Theorem~\ref{thm:dupire-asymptotic} and Lemma~\ref{lem:dupire-asymptotic-dK}. We also provide a more detailed proof of Dupire's Equation in Theorem~\ref{thm:dupire} that, contrary to the paper, does not ignore interest rates.

The proof of the generalisation of Dupire's Equation to higher dimensions in \textcite{amster_towards_2009} is dense, with only about 2 pages. In the paper, the change of variables in the integral is done without much explanation. Here, we prove it in Lemma~\ref{lem:generalisation-var-change} before moving on to the proof of the main result (see Theorem~\ref{thm:generalisation}) which is presented here in much greater detail. Together, they account for 8 pages.

Finally, we present a discretisation of the PDE from Theorem~\ref{thm:generalisation} using finite difference methods. An implementation of the method in C++ is also provided. The code also implements Monte Carlo simulations to compare the results as a sanity check. A rudimentary calibration of a parametric version of the model is used to demonstrate the relative ability of the model to fit a volatility smile.

\section{Preliminaries}\label{sec:preliminaries}

This section establishes the preliminaries of stochastic calculus required by further sections without proofs. See \textcite{capinski_stochastic_2012} and \textcite{capinski_blackscholes_2012} for further details.

In the following, we implicitly assume that we work in a probability space $(\Omega, \mathcal{F}, P)$ unless stated otherwise. We also restrict ourselves to the time interval $[0,T]$ for some $T$ as it is sufficient in this context.

We will denote the Borel subsets of $A$ by $\mathcal{B}(A)$, e.g. $\mathcal{B}(\mathbb{R}^d)$ denotes the Borel subets of $\mathbb{R}^d$.

\begin{definition}
  The \textbf{support} of a random variable $X$ with density $f$ is defined as
  \begin{align*}
    \mathrm{supp}(X) = \{x : f(x) > 0\}.
  \end{align*}
\end{definition}

\begin{definition}
  A \textbf{stochastic process} is a measurable function $X : [0,T] \times \Omega \to \mathbb{R}^d$ with respect to the $\sigma$-field $\mathcal{B}([0,T]) \times \mathcal{F}$.
\end{definition}

In the following, when we write $X(t)$ for $t \in [0,T]$, it denotes the random variable $\omega \mapsto X(t, \omega)$.

\begin{definition}
  For any fixed $\omega \in \Omega$, we say that the function $t \mapsto X(t,\omega)$ is a \textbf{path} of the stochastic process $X$.
\end{definition}

Since $X(t)$ is a random variable for each $t \in [0,T]$, the question of measurability arises, leading to the concepts of \textit{filtrations} and \textit{adaptedness}.

\begin{definition}
  A \textbf{filtration} is a family $(\mathcal{F}_t)_{t \in [0,T]}$ of sub-$\sigma$-fields of $\mathcal{F}$ such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $0 \le s < t \le T$.
\end{definition}

From here onwards, we further assume when relevant that we work with a filtration $(\mathcal{F}_t)_{t \in [0,T]}$.

Consider a stochastic process $X$ in the probability space $(\Omega, \mathcal{F}, P)$. The \textbf{filtration generated by $X$} is denoted by $(\mathcal{F}^X_t)_{t \in [0,T]}$ and is given by
\begin{align*}
  \mathcal{F}^X_t = \sigma \left\{ A : A \in \mathcal{F}_{X(s)}, s \in [0,t] \right\},
\end{align*}
where $\mathcal{F}_{Y}$ denotes the sub-$\sigma$-field of $\mathcal{F}$ generated by $Y$, for any $\mathcal{F}$-measurable random variable $Y$.

\begin{definition}
  We say that a filtration $\mathcal{F}_t$ is \textbf{coarser} than a filtration $\mathcal{G}_t$ if $\mathcal{F}_t \subseteq \mathcal{G}_t$ for all $t \in [0,T]$. Equivalently, we say that $\mathcal{G}_t$ is \textbf{finer} than $\mathcal{F}_t$.
\end{definition}

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be \textbf{adapted} to a filtration $(\mathcal{F}_t)_{t \in [0,T]}$ if, for all $t \in [0,T]$, $X(t)$ is $\mathcal{F}_t$-measurable.
\end{definition}

We will repeatedly make use of the following property of some stochastic processes throughout this text.

\begin{definition}
  A stochastic process $X : [0,T] \times \Omega \to \mathbb{R}^d$ is said to be a \textbf{martingale} for a filtration $\mathcal{F}_t$ if $X(t)$ is integrable for each $t \in [0,T]$ and
  \begin{align*}
    \mathbb{E}(X(t) \mid \mathcal{F}_s) = X(s),
  \end{align*}
  for all $0 \le s < t \le T$.
\end{definition}

The Wiener process is a central stochastic process to stochastic calculus. Here, we give an axiomatic definition. For a construction of such a process, and thus a proof of existence, see \textcite{capinski_stochastic_2012}.

\begin{definition}
  A \textbf{Wiener process}, also called Brownian motion, is a stochastic process $(W(t))_{t \in [0,T]}$ that satisfies
  \begin{itemize}
    \item $W(0) = 0$ almost surely;
    \item for all $0 \le s < t \le T$, the increment $W(t) - W(s)$ follows a normal distribution with mean 0 and variance $t - s$;
    \item for all $0 \le t_1 < t_2 < \cdots < t_m$, the increments $W(t_k) - W(t_{k-1}), k=2,\ldots,m$ are independent; and
    \item almost all paths are continuous, i.e. the functions $t \mapsto W(t,\omega)$ are continuous for almost all $\omega \in \Omega$.
  \end{itemize}
\end{definition}

\begin{definition}
  A \textbf{$d$-dimensional Wiener process} is a stochastic process $\mathbf{W}(t) = (W_1(t), W_2(t), \ldots, W_d(t))$ where $W_j(t), j=1,\ldots,d$ are independent Wiener processes.
\end{definition}

The \textit{It\^o integral} allows us to integrate a stochastic process with respect to a Wiener process, giving a random variable as the result. We will first define it for \textit{simple processes} and extend the definitions in steps.

\begin{definition}
  We say that a stochastic process $X : [0,T] \times \Omega \to \mathbb{R}$ is a \textbf{simple process}, and denote it by $X \in \mathcal{S}^2$, if
  \begin{align*}
    X(t,\omega) = \xi_0 \mathbf{1}_{\{0\}}(t) + \sum_{k=0}^{n-1} \xi_k(\omega) \mathbf{1}_{(t_k,t_{k+1}]}(t)
  \end{align*}
  for some $n > 0$, $0 = t_0 < t_1 < \cdots < t_n = T$ and $\mathcal{F}^W_{t_k}$-measurable random variables $\xi_k$ such that $\mathbb{E}(\xi_k^2) < \infty$ for $k = 0,1,\ldots,n-1$.
\end{definition}

\begin{definition}
  The \textbf{stochastic integral}, also called It\^o integral, of a process $X \in \mathcal{S}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \sum_{k=0}^{n-1} \xi_k (W(t_{k+1}) - W(t_k)).
  \end{align*}
\end{definition}

We now extend the definition of stochastic integral to processes that are square-integrable in the sense of $L^2([0,T] \times \Omega)$.

\begin{definition}
  The set $\mathcal{M}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \mathbb{E}\left(\int_0^T X(t)^2 \mathrm{d}t \right) < \infty.
  \end{align*}
\end{definition}

Stochastic integrals can be extended to $\mathcal{M}^2$ using the fact that $\mathcal{S}^2$ is dense in $\mathcal{M}^2$. More specifically, we will use the following two theorems.

\begin{theorem}
  (\cite{capinski_stochastic_2012}, Theorem 3.4)\label{thm:s2-m2-conv}
  For all $X \in \mathcal{M}^2$, there exists a sequence $(X_n)_{n \ge 1}$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{theorem}

\begin{theorem}
  (\cite{oksendal_stochastic_2003}, Definition 3.1.6)
  Let $X \in \mathcal{M}^2$. For all sequences $(X_n)$ in $\mathcal{S}^2$ that converge to $X$ in $L^2$, the sequence
  \begin{align*}
    \int_0^T X_n(t) \mathrm{d}W(t)
  \end{align*}
  converges in $L^2$ and the limit does not depend on the choice of $(X_n)$.
\end{theorem}

We are now ready to extend stochastic integrals to $\mathcal{M}^2$. The previous results ensure that the following definition of stochastic integrals in $\mathcal{M}^2$ is in fact well-defined by giving existence and uniqueness of the limit.

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[0,T]$ is defined as
  \begin{align*}
    \int_0^T X(t) \mathrm{d}W(t) = \lim_{n \to \infty} \int_0^T X_n(t) \mathrm{d}W(t)
  \end{align*}
  for a sequence $(X_n)$ in $\mathcal{S}^2$ that converges to $X$ in $L^2([0,T] \times \Omega)$.
\end{definition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ over $[a,b]$, for $0 \le a < b \le T$, is defined as
  \begin{align*}
    \int_a^b X(t) \mathrm{d}W(t)
    = \int_0^T \mathbf{1}_{[a,b]}(t) X(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

For fixed endpoints of integration, the stochastic integral of a process is a random variable. However, as shown below, it can also be seen as a stochastic process if we consider the integral as a function of its upper endpoint of integration.

\begin{proposition}(\cite{capinski_stochastic_2012}, Theorem 3.15)\label{prop:stochastic-integral-martingale}
  For all $X \in \mathcal{M}^2$, there exists a martingale $M : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that
  \begin{align*}
    M(t) = \int_0^T \mathbf{1}_{[0,t]}(s) X(s) \mathrm{d}W(s),
  \end{align*}
  almost surely, for all $t \in [0,T]$.
\end{proposition}

\begin{definition}\label{def:stochastic-integral-as-process}
  The \textbf{stochastic integral} of $X \in \mathcal{M}^2$ is defined as the process
  \begin{align}\label{eq:stochastic-integral-process}
    \int_0^t X(s) \mathrm{d}W(s) = M(t),
  \end{align}
  where $M$ is the martingale given by Proposition~\ref{prop:stochastic-integral-martingale}, for $t \in [0,T]$.
\end{definition}

Before extending the stochastic integral further, we note the following property of stochastic integrals on $\mathcal{M}^2$, which does not necessarily hold for processes to which we extend the integral thereafter.

\begin{theorem}\label{thm:stochastic-integral-expectation-m2}
  (\cite{capinski_stochastic_2012}, Theorem 3.14)
  If $X \in \mathcal{M}^2$, then
  \begin{align*}
    \mathbb{E}\left(\int_0^t X(s) \mathrm{d}s\right) = 0,
  \end{align*}
  for all $t \in [0,T]$.
\end{theorem}

We further extend the stochastic integral to an even larger class of processes.

\begin{definition}
  The set $\mathcal{P}^2$ is defined as the set of $\mathcal{F}^W_t$-adapted processes $X$ such that
  \begin{align*}
    \int_0^T X(t)^2 \mathrm{d}t < \infty
  \end{align*}
  almost surely.
\end{definition}

Unsurprisingly, the extension makes use of the definition of stochastic integrals on $\mathcal{M}^2$ using the following result to ensure that it is well-defined.

\begin{proposition}(\cite{capinski_stochastic_2012}, Proposition 4.14 and Theorem 4.16)\label{prop:p2-localising-sequence}
  Let $X \in \mathcal{P}^2$ and let $(X_n)_{n \ge 1}$ be the sequence of stochastic processes given by
  \begin{align*}
    X_n(t) = \mathbf{1}_{[0,\tau_n]}(t) X(t)
  \end{align*}
  with
  \begin{align*}
    \tau_n = \inf \left\{ t \in [0,T] : \int_0^t X(s)^2 \mathrm{d}s \ge n \right\},
  \end{align*}
  where we take $\inf \emptyset = T$.
  Then $X_n \in \mathcal{M}^2$ for all $n$ and the sequence of continuous martingales
  \begin{align*}
    M_n(t) = \int_0^t X_n(s) \mathrm{d}W(s),
  \end{align*}
  as in \eqref{eq:stochastic-integral-process}, converges almost surely to a stochastic process $Y$ with continuous paths.
\end{proposition}

\begin{definition}
  The \textbf{stochastic integral} of $X \in \mathcal{P}^2$ is the process
  \begin{align*}
    \int_0^t X(s) \mathrm{d}W(s) = \lim_{n \to \infty} M_n(t) = Y(t)
  \end{align*}
  with $M_n$ and $Y$ as in Proposition~\ref{prop:p2-localising-sequence}.
\end{definition}

For the rest of this section, we turn our attention to multidimensional stochastic processes. We can extend the previous definition to multiple dimensions as follows.

\begin{definition}
  The \textbf{stochastic integral} of the $n \times d$ matrix $\mathbf{B} = [b_{ij}]$ of processes in $\mathcal{P}^2$ with respect to a $d$-dimensional Wiener process $\mathbf{W}$ is defined as the random vector given by
  \begin{align*}
    \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s) = \left[
      \sum_{j=1}^{n} \int_0^t b_{ij}(s) \mathrm{d}W_j(s)
    \right]_{i=1,\ldots,d}.
  \end{align*}
\end{definition}

If $n=1$, we understand the integral to be the entry as defined above, rather than a 1-component vector.

Moreover, we can denote some sums of stochastic integrals with the notation
\begin{align*}
  \int_0^t \mathbf{X}(s) \cdot \mathrm{d}\mathbf{W}(s) = \sum_{j=1}^{d} \int_0^t X_j(s) \mathrm{d}W_j(s),
\end{align*}
for a stochastic process $\mathbf{X}(t) = (X_1(t), \ldots, X_d(t))$ with components in $\mathcal{P}^2$.

Stochastic processes defined in terms of a Lebesgue integral and a stochastic integral occupy a central place in the theory shown in later sections. We now define classes of such processes and some of their properties.

\begin{definition}
  We say that a process $\mathbf{X}(t) = (X_1(t), X_2(t), \ldots, X_d(t))$ is an \textbf{It\^o process} if it has the form
  \begin{align}\label{eq:ito-process}
    X_i(t) = X_i(0) + \int_0^t a_i(s) \mathrm{d}s + \sum_{j=1}^n \int_0^t b_{ij}(s) \mathrm{d}W_j(s),
  \end{align}
  for $i=1,\ldots,d$, where $\mathbf{W}(t)= (W_1(t),\ldots,W_n(t))$ is an $n$-dimensional Wiener process; $a_i(t), i=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes such that
  \begin{align*}
    \int_0^T |a_i(t)| \mathrm{d}t < \infty;
  \end{align*}
  and $b_{ij} \in \mathcal{P}^2$, for $i=1,\ldots,d$ and $j=1,\ldots,n$.
\end{definition}

Taking $\mathbf{a}(t) = (a_1(t),\ldots,a_d(t))$, $\mathbf{B}(t) = [b_{ij}(t)]_{i=1,\ldots,d;j=1,\ldots,n}$, we can write \eqref{eq:ito-process} as
\begin{align*}
  \mathbf{X}(t) = \mathbf{X}(0) + \int_0^t \mathbf{a}(s) \mathrm{d}s + \int_0^t \mathbf{B}(s) \mathrm{d}\mathbf{W}(s)
\end{align*}
or, to be even terser, in its so-called \textbf{stochastic differential} notation:
\begin{align*}
  \mathrm{d}\mathbf{X}(t) = \mathbf{a}(t) \mathrm{d}t + \mathbf{B}(t) \mathrm{d}\mathbf{W}(t).
\end{align*}

We call $\mathbf{a}$ the \textbf{drift} and $\mathbf{B}$ the \textbf{volatility}. Together, they are referred to as the \textbf{characteristics} of the It\^o process.

\begin{definition}
  A \textbf{stochastic differential equation} (or \textbf{SDE} for short) is an equation of the form
  \begin{align}\label{eq:sde-init-value}
    \mathbf{X}(t) &= \mathbf{X}(0) + \int_0^t a(s, \mathbf{X}(s)) \mathrm{d}s + \int_0^t b(s, \mathbf{X}(s)) \mathrm{d}\mathbf{W}(s),\\
    \mathbf{X}(0) &= \mathbf{x}_0,\notag
  \end{align}
  for some $a : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^d$, $b : [0,T] \times \mathbb{R}^{d} \to \mathbb{R}^{d \times n}$ and $\mathbf{x}_0 \in \mathbb{R}^d$.
\end{definition}

When faced with an equation such as \eqref{eq:sde-init-value}, one might naturally wonder whether a solution exists and, if so, whether it is unique. The following theorem gives both existence and uniquenesss of a solution under certain conditions.

\begin{theorem}\label{thm:sde-solution}
  (\cite{oksendal_stochastic_2003}, Theorem 5.2.1)
  Provided that both the coefficients $a(t,\mathbf{x})$ and $b(t,\mathbf{x})$ satisfy the following conditions, where $\|\cdot\|$ denotes the Euclidean norm on $\mathbb{R}^d$ and the $L_{2,1}$-norm on $\mathbb{R}^{d \times n}$ respectively.
  \begin{itemize}
    \item Linear growth: there exists $C > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x})\| + \|b(t,\mathbf{x})\| \le C (1 + \|\mathbf{x}\|),
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x} \in \mathbb{R}^d$.
    \item Lipschitz continuity: there exists $K > 0$ such that
      \begin{align*}
        \|a(t,\mathbf{x}) - a(t,\mathbf{y})\| + \|b(t,\mathbf{x}) - b(t,\mathbf{y})\| \le K \|\mathbf{x}-\mathbf{y}\|,
      \end{align*}
      for $t \in [0,T]$ and $\mathbf{x},\mathbf{y} \in \mathbb{R}^d$.
  \end{itemize}
  Then \eqref{eq:sde-init-value} has a unique solution with continuous paths such that $X_i \in \mathcal{M}^2$, for all $i=1,\ldots,d$.
\end{theorem}

\begin{definition}\label{def:ito-diffusion}
  An It\^o process of the form \eqref{eq:sde-init-value}, whose characteristics satisfy the conditions of Theorem~\ref{thm:sde-solution}, is called an \textbf{It\^o diffusion}.
\end{definition}

We will need the following property of It\^o diffusions for some proof in the next sections. Since a reference could not be found for this lemma, a proof is provided.

\begin{lemma}\label{lem:ito-diffusion-characteristics-m2}
  The characteristics of an It\^o diffusion $\mathbf{X}$ are in $\mathcal{M}^2$, i.e.
  \begin{align*}
    a_i(t,\mathbf{X}(t)) \in \mathcal{M}^2, &&
    b_{ij}(t,\mathbf{X}(t)) \in \mathcal{M}^2,
  \end{align*}
  for $i=1,\ldots,d$ and $j=1,\ldots,n$.

  \begin{proof}
    Denote the It\^o diffusion by $\mathbf{X}$ and the characteristics $a$ and $b$ as in the definition. Let $i=1,\ldots,n$ and $j=1,\ldots,n$. By linear growth, we have
    \begin{align*}
      b_{ij}(t,\mathbf{X}(t))
      &\le \sum_{k=1}^d \left( \sum_{l=1}^n b_{kl}(t,\mathbf{X}(t))^2 \right)^{\frac{1}{2}}
      = \|b(t,\mathbf{X}(t))\|\\
      &\le \|a(t,\mathbf{X}(t))\| + \|b(t,\mathbf{X}(t))\|
      \le C (1 + \|\mathbf{X}(t)\|).
    \end{align*}

    Therefore,
    \begin{align*}
      \mathbb{E} &\left( \int_0^T b_{ij}(t,\mathbf{X}(t))^2 \mathrm{d}t \right)\\
      &\le \mathbb{E} \left( \int_0^T C^2(1+\|\mathbf{X}(t)\|)^2 \mathrm{d}t \right)\\
      &= C^2 T + 2 C^2 \mathbb{E} \left( \int_0^T \sqrt{\sum_{i=1}^d X_i(t)^2} \mathrm{d}t \right) + C^2 \mathbb{E} \left( \int_0^T \sum_{i=1}^d X_i(t)^2 \mathrm{d}t \right)\\
      &\le C^2 T + 2 C^2 \sqrt{\mathbb{E} \left( \int_0^T \sum_{i=1}^d X_i(t)^2 \mathrm{d}t \right)} + C^2 \mathbb{E} \left( \int_0^T \sum_{i=1}^d X_i(t)^2 \mathrm{d}t \right),
    \end{align*}
    by Jensen's Inequality in $L^1([0,T] \times \Omega)$ since $x \mapsto \sqrt{x}$ is concave. But,
    \begin{align*}
      \mathbb{E} \left( \int_0^T \sum_{i=1}^d X_i(t)^2 \mathrm{d}t \right)
      = \sum_{i=1}^d \mathbb{E} \left( \int_0^T X_i(t)^2 \mathrm{d}t \right) < \infty,
    \end{align*}
    since $X_i \in \mathcal{M}^2$ by Theorem~\ref{thm:sde-solution}. Hence,
    \begin{align*}
      \mathbb{E} \left( \int_0^T b_{ij}(t,\mathbf{X}(t))^2 \mathrm{d}t \right) < \infty.
    \end{align*}

    That is, $b_{ij}(t,\mathbf{X}(t)) \in \mathcal{M}^2$. The same proof can be used to show that $a_i(t,\mathbf{X}(t)) \in \mathcal{M}^2$. In other words, all characteristics are in $\mathcal{M}^2$ as required.
  \end{proof}
\end{lemma}

Provided that a stochastic process satisfies an SDE, we can find an SDE that a function of said process itself satisfies by the \textit{It\^o Formula}, also called \textit{It\^o Lemma} in other resources.

\begin{theorem}[It\^o Formula]
  (\cite{capinski_blackscholes_2012}, Theorem 6.10)\label{thm:ito-formula}
  Let $F : [0,T] \times \mathbb{R}^d \to \mathbb{R}$ and let $\mathbf{X}(t)$ be a $d$-dimensional It\^o process driven by $n$ independent Wiener processes. For short, we write
  \begin{align*}
    F(t,\mathbf{X}(t)) = F(t,X_1(t),X_2(t),\ldots,X_d(t)).
  \end{align*}
  If $F$ is continuously differentiable in the first argument and twice-continuously differentiable in the others, then $F(t,\mathbf{X}(t))$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}F(t,\mathbf{X}(t))
    &= F_t(t,\mathbf{X}(t)) \mathrm{d}t + \sum_{i=1}^d F_{x_i}(t,\mathbf{X}(t)) a_i(t) \mathrm{d}t\\
    &\ \ \ \ + \sum_{i=1}^d \left(F_{x_i}(t,\mathbf{X}(t)) \sum_{j=1}^n b_{ij}(t) \mathrm{d}W_j(t) \right)\\
    &\ \ \ \ + \frac{1}{2} \sum_{j=1}^n \sum_{i,l=1}^d F_{x_i x_l}(t,\mathbf{X}(t)) b_{ij}(t)b_{lj}(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

We extend the stochastic integral once more to integrate an It\^o process with respect to another. Notice that the Wiener process is itself an It\^o process by taking $a \coloneqq 0$ and $b \coloneqq 1$, so we essentially extend the definition of stochastic integrals to a larger class of integrators. This also gives us a convenient notation to write some equations in a terser way.

\begin{definition}
  Given two It\^o processes $X, Y$ such that $Y$ has stochastic differential
  \begin{align*}
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  the \textbf{stochastic integral} of $X$ with respect to $Y$ is defined as
  \begin{align*}
    \int_0^t X(s) \mathrm{d}Y(s) = \int_0^t X(s) a_Y(s) \mathrm{d}s + \int_0^t X(s) b_Y(s) \mathrm{d}W(s).
  \end{align*}
  We also write
  \begin{align*}
    X(t) \mathrm{d}Y(t) = X(t) a_Y(t) \mathrm{d}t + X(t) b_Y(t) \mathrm{d}W(t).
  \end{align*}
\end{definition}

The product of two It\^o processes is also an It\^o process and its stochastic differential is given by the following result.

\begin{theorem}[It\^o Product Rule]\label{thm:ito-product-rule}
  (\cite{capinski_stochastic_2012}, Theorem 4.36)
  Given two It\^o processes $X, Y$ with stochastic differential
  \begin{align*}
    \mathrm{d}X(t) &= a_X(t) \mathrm{d}t + b_X(t) \mathrm{d}W(t),\\
    \mathrm{d}Y(t) &= a_Y(t) \mathrm{d}t + b_Y(t) \mathrm{d}W(t),
  \end{align*}
  their product $XY$ is an It\^o process with stochastic differential
  \begin{align*}
    \mathrm{d}[XY](t) = X(t) \mathrm{d}Y(t) + Y(t) \mathrm{d}X(t) + b_X(t) b_Y(t) \mathrm{d}t.
  \end{align*}
\end{theorem}

If a process $\mathbf{W}$ is a Wiener process under $P$, it is not necessarily a Wiener process under a different measure $Q$. However, the Girsanov Theorem gives us a way to construct a Wiener process under this new measure $Q$, provided that it satisfies some conditions.

\begin{theorem}[Girsanov Theorem]\label{thm:girsanov}
  (\cite{capinski_blackscholes_2012}, Theorem 6.15)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $\theta_j, j=1,\ldots,d$ be $\mathcal{F}^\mathbf{W}_t$-adapted processes such that
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale under $P$ and let $Q$ be the measure with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$, i.e.
  \begin{align*}
    Q(A) = \int_A M(T) \mathrm{d}P,
  \end{align*}
  for all $A \in \mathcal{F}$.
  Then the process $\mathbf{W}^Q(t) = (W^Q_1(t), W^Q_2(t), \ldots, W^Q_d(t))$ with
  \begin{align*}
    W^Q_j(t) = \int_0^t \theta_j(s) \mathrm{d}s + W_j(t)
  \end{align*}
  is a $d$-dimensional Wiener process under $Q$.
\end{theorem}

The following result is a sufficient condition under which the martingale condition on $M$ in the Girsanov Theorem holds.

\begin{theorem}[Novikov Condition]\label{thm:novikov}
  (\cite{karatzas_brownian_1998}, Corollary 5.13)
  If $a_j(t)$, $j=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes satisfying
  \begin{align*}
    \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right) < \infty,
  \end{align*}
  then
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.
\end{theorem}

Stochastic integrals on $\mathcal{M}^2$ are martingales by Definition~\ref{def:stochastic-integral-as-process}. As it turns out, we can go the other way around and express square-integrable martingales in terms of stochastic integrals on $\mathcal{M}^2$.

\begin{theorem}[Martingale Representation Theorem]\label{thm:martingale-representation}
  (\cite{oksendal_stochastic_2003}, Theorem 4.3.4)
  Let $\mathbf{W}$ be a $d$-dimensional Wiener process and $M$ be a martingale such that $M(t)$ is square-integrable, i.e. $\mathbb{E}(M(t)^2) < \infty$, for all $t \in [0,T]$. Then there exists a unique $\mathcal{F}^\mathbf{W}_t$-adapted process $\mathbf{\Gamma}(t)$ such that
  \begin{align*}
    M(t) = M(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}(s)
  \end{align*}
  and such that $\Gamma_i \in \mathcal{M}^2$ for all $i=1,\ldots,d$.
\end{theorem}

Finally, the following result about conditional expectations will be required by some proof in Section~\ref{sec:fokker-planck}.

\begin{theorem}\label{thm:cond-exp-measurable-independent}
  (\cite{kopp_probability_2013}, Theorem 4.27)
  Let $0 \le s < t \le T$ and $X, Y$ be two random variables such that
  \begin{itemize}
    \item $X$ is $\mathcal{F}^W_s$-measurable and
    \item $Y$ is $\mathcal{F}^W_t$-measurable and independent from $\mathcal{F}^W_s$.
  \end{itemize}
  For any bounded Borel function $f$, we have
  \begin{align*}
    \mathbb{E}(f(X,Y) \mid \mathcal{F}^W_s) = g_f(X),
  \end{align*}
  almost surely, where $g_f$ is a bounded Borel function given by
  \begin{align*}
    g_f(x) = \mathbb{E}(f(x,Y)).
  \end{align*}
\end{theorem}

\section{Multi-Asset Black-Scholes Model}\label{sec:black-scholes}

In this section, we discuss a market model -- often referred to as the \textit{multi-asset Black-Scholes model with variable coefficients} -- consisting of one risk-free asset with price $A(t)$ satisfying
\begin{align}\label{eq:multi-bs-eq-risk-free}
  \mathrm{d}A(t) = r(t) A(t) \mathrm{d}t, && A(0) = 1,
\end{align}
where $r(t)$ is a deterministic function representing the continuous risk-free rate, and $d$ risky assets with prices $\mathbf{S}(t) = (S_1(t), \ldots, S_d(t))$ satisfying
\begin{align}
  \mathrm{d}S_i(t) &= \mu_i(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W_j(t), \text{ for } i = 1,\ldots,d,\label{eq:multi-bs-eq}
\end{align}
for $\mathbf{W}(t) = (W_1(t), \ldots, W_d(t))$ a Wiener process with respect to the probability $P$ in the filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$; some processes $\mu_i, i=1,\ldots,d$ (the \textit{drift coefficients}) and $c_{ij}, i,j=1,\ldots,d$ (the \textit{volatility coefficients}); and a constant $\mathbf{S}(0)$ such that $S_i(0) > 0$, for $i=1,\ldots,d$.

Some references provided in this section refer to the equivalent results in the Black-Scholes model (single-asset, constant coefficients). We extend them here to the multi-asset Black-Scholes model with variable coefficients and provide proofs of the same. These references are marked as such.

Some further assumptions are made by the model.

\begin{assumption}\label{ass:drift-vol-regularity}
  The processes $\mu_i$ and $c_{ij}$, for $i,j=1,\ldots,d$, are $\mathcal{F}^\mathbf{W}_t$-adapted with continuous paths and are bounded by a deterministic constant.
\end{assumption}

\begin{assumption}\label{ass:vol-matrix-invertible}
  The matrix of volatility coefficients $\mathbf{C}(t)= [c_{ij}(t)]_{i,j=1,\ldots,d}$ is invertible.
\end{assumption}

The prices of the risk-free asset and the risky ones are given above in terms of the equations they satisfy. We can find the unique solutions for both cases.

\begin{theorem}\label{thm:bs-solution-risk-free}
  Equation \eqref{eq:multi-bs-eq-risk-free} has the unique solution
  \begin{align*}
    A(t) &= \exp \left( \int_0^t r(s) \mathrm{d}s \right).
  \end{align*}

  \begin{proof}
    Rewriting \eqref{eq:multi-bs-eq-risk-free} in its integral form, we have
    \begin{align*}
      A(t) = A(0) + \int_0^t r(s) A(s) \mathrm{d}s.
    \end{align*}
    Differentiating both sides with respect to $t$, we get
    \begin{align*}
      \frac{\mathrm{d}}{\mathrm{d}t}A(t) = r(t) A(t).
    \end{align*}
    Putting all terms on the LHS and multiplying by $e^{-\int_0^t r(s) \mathrm{d}s}$, we obtain
    \begin{align*}
      e^{-\int_0^t r(s) \mathrm{d}s} \frac{\mathrm{d}}{\mathrm{d}t}A(t) - r(t) e^{-\int_0^t r(s) \mathrm{d}s} A(t) = 0.
    \end{align*}
    Therefore, recognising the LHS as the derivative of $e^{-\int_0^t r(s) \mathrm{d}s} A(t)$, it must be constant since its derivative is zero. That is,
    \begin{align*}
      A(t) = k \exp\left(\int_0^t r(s) \mathrm{d}s\right),
    \end{align*}
    for some constant $k$. But then, $A(0) = k e^{\int_0^0 r(s) \mathrm{d}s}$ so $k = A(0) = 1$, thus completing the proof.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-solution}
  Equation \eqref{eq:multi-bs-eq} has solution
  \begin{align*}
    S_i(t) &= S_i(0) \exp \left( \int_0^t \mu_i(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right),
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Let $i = 1,\ldots,d$. By the It\^o Formula (Theorem~\ref{thm:ito-formula}) with $F(t,x) = \ln x$ so that $F_t(t,x) = 0, F_x(t,x) = \frac{1}{x}$ and $F_{xx} = \frac{-1}{x^2}$, we have
    \begin{align*}
      &\mathrm{d}F(t,S_i(t))\\
      &= F_t(t,S_i(t)) \mathrm{d}t + F_x(t,S_i(t)) \mu_i(t) S_i(t) \mathrm{d}t\\
      &\ \ \ \ \ + F_x(t,S_i(t)) \sum_{j=1}^d c_{ij}(t) S_i(t) \mathrm{d}W_j(t) + \frac{1}{2} \sum_{j=1}^d F_{xx}(t,S_i(t)) (c_{ij}(t) S_i(t))^2 \mathrm{d}t\\
      &= \mu_i(t) \mathrm{d}t
      - \frac{1}{2} \sum_{j=1}^d c_{ij}(t)^2 \mathrm{d}t
      + \sum_{j=1}^d c_{ij}(t) \mathrm{d}W_j(t).
    \end{align*}

    Expanding the stochastic differential notation into its integral form and since $\ln S_i(t) - \ln S_i(0) = \ln \frac{S_i(t)}{S_i(0)}$, we obtain
    \begin{align*}
      \ln \frac{S_i(t)}{S_i(0)}
      &= \int_0^t \mu_i(s) \mathrm{d}s
      - \frac{1}{2} \sum_{j=1}^d \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s).
    \end{align*}

    Applying the exponential function to both sides yields the required solution.
  \end{proof}
\end{theorem}

\subsection{Risk-neutral Probability}

In this section, we construct a probability measure under which the discounted stock prices are martingales. Later, we will see that we express some prices as conditional expectations under that probability measure. First, we begin with a couple of definitions.

\begin{definition}
  The \textbf{discounted process} of a process $X$ is defined as
  \begin{align*}
    \widetilde{X}(t) = \frac{X(t)}{A(t)} = e^{-\int_0^t r(s) \mathrm{d}s} X(t).
  \end{align*}
  The factor $\frac{1}{A(t)}$ is called the \textbf{discounting factor}.
\end{definition}

Note that since $e^{-\int_0^0 r(s) \mathrm{d}s} = 1$, a process always agrees with its dicounted process at time 0, i.e. $X(0) = \widetilde{X}(0)$.

\begin{definition}
  A \textbf{risk-neutral probability} is a probability measure $Q$ that is equivalent to $P$ and under which the discounted prices of all assets are martingales.
\end{definition}

The construction of the risk-neutral probability below requires the following lemma. Note that the reference provided refers to a similar statement where $a_j, j=1,\ldots,d$ are deterministic. We extend it here to stochastic processes bounded by a deterministic constant.

\begin{lemma}\label{lem:bs-exponential-martingale}
  (\cite{capinski_blackscholes_2012}, Proposition 6.16)
  If $a_j(t), j=1,\ldots,d$ are $\mathcal{F}^\mathbf{W}_t$-adapted stochastic processes bounded by a deterministic constant $K > 0$, then the stochastic process
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t a_j(s)^2 \mathrm{d}s - \sum_{j=1}^{d} \int_0^t a_j(s) \mathrm{d}W_j(s) \right)
  \end{align*}
  is a martingale.

  \begin{proof}
    The Novikov Condition holds since
    \begin{align*}
      \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T a_j(t)^2 \mathrm{d}t\right)\right)
      &\le \mathbb{E}\left(\exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\right)\\
      &= \exp \left(\frac{1}{2} \sum_{j=1}^d \int_0^T K^2 \mathrm{d}t\right)\\
      &= \exp \left(\frac{1}{2} d K^2 T \right) < \infty.
    \end{align*}
    Hence, by Theorem~\ref{thm:novikov}, $M(t)$ is a martingale.
  \end{proof}
\end{lemma}

We can now show how to construct a risk-neutral probability for this model.

\begin{theorem}\label{thm:bs-risk-neutral-prob}
  (\cite{capinski_blackscholes_2012}, Theorem 6.17)
  Let $\mathbf{\theta}(t) = [\theta_j(t)]_{j=1,\ldots,d}$ be given by
  \begin{align}\label{eq:bs-def-theta}
    \mathbf{\theta}(t) = \mathbf{C}^{-1}(t) [\mu(t) - r(t)]
  \end{align}
  and let
  \begin{align*}
    M(t) = \exp \left( - \frac{1}{2} \sum_{j=1}^d \int_0^t \theta_j(s)^2 \mathrm{d}s - \sum_{j=1}^d \int_0^t \theta_j(s) \mathrm{d}W_j(s) \right).
  \end{align*}
  If $M(t)$ is a martingale under $P$, then the measure $Q$ with density $\frac{\mathrm{d}Q}{\mathrm{d}P} = M(T)$ is a risk-neutral probability, i.e. it is equivalent to $P$ and the processes $\widetilde{S}_i, i=1,\ldots,d$ are martingales under $Q$.

  \begin{proof}
    For all $i=1,\ldots,d$, by Theorem~\ref{thm:bs-solution},
    \begin{align*}
      \widetilde{S}_i(t)
      &= \exp\left(- \int_0^t r(s)\mathrm{d}s\right) S_i(t)\\
      &= S_i(0) \exp \Bigg( \int_0^t (\mu_i(s) - r(s)) \mathrm{d}s \\
      &\hspace*{60pt} - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \Bigg).
    \end{align*}

    By \eqref{eq:bs-def-theta}, we have
    \begin{align*}
      \mathbf{\mu}(t) - r(t) = \mathbf{C}(t) \mathbf{\theta}(t),
    \end{align*}
    which in turn implies that
    \begin{align*}
      \mu_i(t) - r(t) = \sum_{j=1}^d c_{ij}(t) \theta_j(t),
    \end{align*}
    for all $i=1,\ldots,d$.

    Substituting this into the previous equation and by linearity of the integral, we obtain
    \begin{align}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( \sum_{j=1}^d \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right)\notag\\
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s \right.\notag\\
      &\hspace*{60pt} \left. + \sum_{j=1}^d \left[\int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s) \right]\right).\label{eq:bs-risk-neutral-before-product-rule}
    \end{align}

    Since we assume that $M(t)$ is a martingale, then the Girsanov Theorem~\ref{thm:girsanov} implies that the process $\mathbf{W}^Q(t) = (W^Q_1(t), \ldots, W^Q_d(t))$ with
    \begin{align*}
      \mathrm{d}W^Q_j(t) = \theta_j(t) \mathrm{d}t + \mathrm{d}W(t)
    \end{align*}
    is a Wiener process under Q.

    But we have that, for all $j=1,\ldots,d$,
    \begin{align*}
      \int_0^t c_{ij}(s) \mathrm{d}W^Q(s)
      = \int_0^t c_{ij}(t) \theta_j(t) \mathrm{d}s + \int_0^t c_{ij}(s) \mathrm{d}W_j(s),
    \end{align*}
    so, substituting in equation \eqref{eq:bs-risk-neutral-before-product-rule}, we obtain
    \begin{align}\label{eq:bs-discounted-stock-price}
      \widetilde{S}_i(t)
      &= S_i(0) \exp \left( - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s
      + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align}

    By Lemma~\ref{lem:bs-exponential-martingale} with $a_j(t) \coloneqq - c_{ij}(t)$ and Assumption~\ref{ass:drift-vol-regularity}, $\widetilde{S}_i(t)$ is thus a martingale under $Q$ for all $i=1,\ldots,d$ as required.

    Moreover, since
    \begin{align*}
      Q(A) = \int_A M(T) \mathrm{d}P,
    \end{align*}
    for all $A \in \mathcal{F}$, then $P(A) = 0$ implies that $Q(A) = 0$. But since $M(T) > 0$, then $Q(A) = 0$ also implies that $P(A) = 0$. That is, the measures $P$ and $Q$ are equivalent.

    In conclusion, we have constructed a measure $Q$ that is indeed a risk-neutral probability for the model.
  \end{proof}
\end{theorem}

\begin{theorem}
  $\mathbf{W}$ and $\mathbf{W}^Q$ generate the same filtration, i.e. $\mathcal{F}^\mathbf{W}_t = \mathcal{F}^{\mathbf{W}^Q}_t$ for all $t \in [0,T]$.

  \begin{proof}
    Since $f(t, \mathbf{x}) = \int_0^t \mathbf{\theta}(s) \mathrm{d}s + \mathbf{x}$ is an invertible Borel function with respect to its second variable (take $g$ to be its inverse function), then $\mathcal{F}^{\mathbf{W}^Q}_t \subseteq \mathcal{F}^{\mathbf{W}}_t$ since $\mathbf{W}^Q(t) = f(t,\mathbf{W}(t))$ and $\mathcal{F}^{\mathbf{W}}_t \subseteq \mathcal{F}^{\mathbf{W}^Q}_t$ since $\mathbf{W}(t) = g(t,\mathbf{W}^Q(t))$. Therefore, the filtrations are equal as required.
  \end{proof}
\end{theorem}

Consequently, we will always refer to the filtration $\mathcal{F}^\mathbf{W}_t$ in the following, even when working with the risk-neutral probability $Q$ for brevity.

Under this risk-neutral probability, the risky assets follow a different dynamics than under $P$ as shown below. Note that, since $A(t)$ is deterministic, its dynamics remains the same.

\begin{theorem}\label{thm:bs-risk-neutral-dynamics}
  (\cite{capinski_blackscholes_2012}, Corollary 2.3 \footnote{This reference is to the equivalent statement in the single-asset, constant coefficients Black-Scholes model. The proof is extended here to the multi-asset Black-Scholes model with variable coefficients.\label{foot:basic-bs-model}})
  The risky assets satisfy
  \begin{align*}
    \mathrm{d}S_i(t) = r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t),
  \end{align*}
  for all $i=1,\ldots,d$.

  \begin{proof}
    Taking $r(t)$ as the drift and $\mathrm{W}^Q$ as the Wiener process in Theorem~\ref{thm:bs-solution}, we obtain the unique solution for this stochastic differential
    \begin{align}\label{eq:bs-risk-neutral-dynamics}
      S_i(t)
      = S_i(0) \exp  \left( \int_0^t r(s) \mathrm{d}s - \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right).
    \end{align}

    Multiplying both sides by the discounted factor $e^{-\int_0^t r(s) \mathrm{d}s}$, we can see that this is equivalent to \eqref{eq:bs-discounted-stock-price} which has been established earlier so the stochastic differential is indeed satisfied by $S_i$, for all $i=1,\ldots,d$.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:bs-risk-neutral-dynamics-discounted}
  (\cite{capinski_blackscholes_2012}, Corollary 2.3 \footref{foot:basic-bs-model})
  The risky assets satisfy
  \begin{align}\label{eq:bs-risk-neutral-dynamics-discounted}
    \mathrm{d}\widetilde{S}_i(t) = \sum_{j=1}^{d} c_{ij}(t) \widetilde{S}_i(t) \mathrm{d}W^Q_j(t),
  \end{align}
  for all $i=1,\ldots,d$.

  \begin{proof}
    From \eqref{eq:bs-risk-neutral-dynamics}, we have
    \begin{align*}
      \widetilde{S}_i(t)
      = \widetilde{S}_i(0) \exp \left( \frac{1}{2} \sum_{j=1}^{d} \int_0^t c_{ij}(s)^2 \mathrm{d}s + \sum_{j=1}^d \int_0^t c_{ij}(t) \mathrm{d}W^Q_j(s)\right),
    \end{align*}
    so, by a similar argument than for Theorem~\ref{thm:bs-risk-neutral-dynamics}, it indeed satisfies \eqref{eq:bs-risk-neutral-dynamics-discounted}.
  \end{proof}
\end{theorem}

We make a further assumption on the prices of the risky assets under $Q$.

\begin{assumption}\label{ass:bs-stock-price-square-integrability}
  The risky assets $S_i$ are assumed to be square-integrable under $Q$ at time $T$, i.e.
  \begin{align*}
    \mathbb{E}_Q(S_i(T)^2) < \infty,
  \end{align*}
  for $i=1,\ldots,d$.
\end{assumption}

\subsection{Strategies}

Our ultimate goal is to \textit{price} derivative securities, i.e. calculate their price at any given time, such that it is `consistent' with the prices of other assets. In this section, we introduce a few definitions to help make this idea precise. Pricing will be covered in the next section.

\begin{definition}
  A (European) \textbf{derivative security} with expiry time $T$ is a security whose value is a stochastic process $H : [0,T] \times \Omega \to \mathbb{R}$ called its \textbf{price process} and such that $H(T) = H_\text{payoff}$ for some $\mathcal{F}^\mathbf{W}_T$-measurable random variable $H_\text{payoff}$ called the \textbf{payoff} of the derivative.
\end{definition}

So far, our market model consisted of the risk-free assets $A$ and the risky assets $S_i, i=1,\ldots,d$. This is referred to as the \textit{basic market}. The introduction of a derivative security leads the following concept.

\begin{definition}
  The \textbf{extended market} for a derivative $H$ consists of the assets $A$ and $S_i, i=1,\ldots,d$ as described above (the basic market) as well as the derivative $H$.
\end{definition}

\begin{definition}
  A \textbf{strategy} is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form\\ $(x_1(t),\ldots,x_d(t),y(t))$, for $t \in [0,T]$, or simply $(x_1,\ldots,x_d,y)$ for short.
\end{definition}

\begin{definition}
  The \textbf{value (process)} of a strategy $(x_1,\ldots,x_d,y)$ is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y)}(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t).
  \end{align*}
\end{definition}

When there is no ambiguity, we simply denote the value of a strategy by $V(t)$. We can extend strategies to the extended market with no surprises.

\begin{definition}
  An \textbf{extended strategy}, or a strategy in the extended market, is defined as an $\mathcal{F}^\mathbf{W}_t$-adapted process of the form $(x_1,\ldots,x_d,y,z)$. Its value is
  \begin{align*}
    V_{(x_1,\ldots,x_d,y,z)}(t)
    = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t) + z(t) H(t).
  \end{align*}
\end{definition}

Not all strategies will be acceptable for our purposes and we therefore introduce some additional conditions that we can impose on strategies.

\begin{definition}
  We say that a strategy is \textbf{self-financing} if its value is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
  \end{align*}
\end{definition}

\begin{definition}
  We say that an extended strategy is \textbf{self-financing} if it is an It\^o process satisfying
  \begin{align*}
    \mathrm{d}V(t) = \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t) + z(t) \mathrm{d}H(t).
  \end{align*}
\end{definition}

Informally, the changes in the value of a self-financing strategy are completely determined by the changes to the positions in each asset, i.e. there are no inflows or outflows of value to or from the strategy.

The following definitions apply to strategies as well as extended strategies.

\begin{definition}
  We say that a strategy is \textbf{admissible} if there exists a constant $L > 0$ such that its value process satisfies $V(t) \ge -L$ for all $t \in [0,T]$, almost surely.
\end{definition}

Informally, an admissible strategy has a limited potential loss.

\begin{definition}
  An \textbf{arbitrage opportunity} is an admissible self-financing strategy such that
  \begin{itemize}
    \item $V(0) = 0$,
    \item $V(T) \ge 0$ almost surely, and
    \item $V(T) > 0$ with positive probability.
  \end{itemize}
\end{definition}

\begin{remark}
  In these definitions, we do not have to specify under which probability measure the statements hold \textit{almost surely} or \textit{with positive probability}. This is because $P$ and $Q$ are equivalent so saying that a proposition holds almost surely / with positive probability under $P$ is equivalent to saying that it holds under $Q$.
\end{remark}

We can now formalise the word `consistent' used before: the price we calculate for a derivative security should not introduce arbitrage opportunities. This is enshrined in the following key assumption.

\begin{assumption}[No-Arbitrage Principle]\label{ass:no-arbitrage-principle}
  There exist no arbitrage opportunities in the extended market.
\end{assumption}

We also assume some regularity of the prices of derivative securities.

\begin{assumption}\label{ass:bs-derivative-regularity}
  The price process of a derivative security with payoff $H_\text{payoff}$ is an It\^o process $H : [0,T] \times \Omega \to \mathbb{R}$ with continuous paths such that $H(T) = H_\text{payoff}$ and such that there are no arbitrage opportunities in the extended market.
\end{assumption}

\subsection{Pricing of Derivatives}

We first define the replication of a derivative by a strategy and show how this can be used to price the derivative under the No-Arbitrage Principle. We then show an alternative way to express the price of a derivative as a $Q$-expectation.

\begin{definition}
  A \textbf{replicating strategy} for a derivative with payoff $H_\text{payoff}$ is an admissible self-financing strategy (in the basic market) with value process $V$ such that
  \begin{align*}
    V(T) = H_\text{payoff}
  \end{align*}
  and such that its discounted value process $\widetilde{V}$ is a martingale under $Q$.
\end{definition}

If there exists a replicating strategy for a derivative, we say that the strategy \textbf{replicates} the derivative or, equivalently, that the derivative is \textbf{replicable}. As we now show, the value of a replicating strategy is equal to the price of the derivative it replicates. We can therefore price the derivative by calculating the value of its replicating strategy.

\begin{theorem}\label{thm:bs-repl-strat-derivative-prices}
  (\cite{capinski_blackscholes_2012}, Theorem 2.16 \footref{foot:basic-bs-model})
  If a derivate with payoff $H_\text{payoff}$ is replicable by a strategy with value process $V$ that has continuous paths, then $V(t) = H(t)$, for all $t \in [0,T]$.
\end{theorem}

In the reference given for the previous theorem (in the single-asset Black-Scholes model with constant coefficients), a proof is given directly. However, we find it easier to first prove a simpler statement in the following lemma. The proof of the theorem, provided thereafter, is then straightforward.

\begin{lemma}\label{lem:bs-non-negative-strat}
  Let $(x_1, \ldots, x_d, y)$ be a self-financing strategy with value process $V$ that has continuous paths. If $V(T) \ge 0$, then $V(t) \ge 0$, almost surely, for all $t \in [0,T]$.

  \begin{proof}
    Assume to the contrary that there is a $t_0 \in [0,T)$ such that $V(t_0) < 0$ with positive probability. We prove this lemma by contradiction by constructing an arbitrage opportunity $(x_1^*, \ldots, x_d^*, y^*)$.

    On the region $A = \{V(t_0) \ge 0\}$, we simply take the strategy $(0,0,\ldots,0)$ and we now focus on the region $A^C = \{V(t_0) < 0\}$.

    For $t < t_0$, we take $x_1^*(t) = \cdots = x_d^*(t) = y^*(t) = 0$. For $t \ge t_0$, we take

    \begin{align*}
      x_i^*(t) &= \left\{ \begin{array}{ll}
        x_i(t) & \text{if } -A(t) < V(t) < 0,\\
        0 & \text{otherwise},
      \end{array}\right. \text{for } i=1,\ldots,d,\\
      y^*(t) &= \left\{ \begin{array}{ll}
        - \frac{V(t_0)}{A(t_0)} - 1 & \text{if } V(t) \le - A(t),\\
        - \frac{V(t_0)}{A(t_0)} + y(t) & \text{if } -A(t) < V(t) < 0,\\
        - \frac{V(t_0)}{A(t_0)} & \text{if } V(t) \ge 0.
      \end{array}\right.
    \end{align*}

    \textbf{Admissibility}: For $t < t_0$, we have $V^*(t) = 0$ and for $t \ge t_0$, we have
      \begin{align*}
        V^*(t)
        &= \sum_{i=1}^d x_i^*(s) S_i(s) + y^*(s) A(s)\\
        &= -\frac{V(t_0)}{A(t_0)} A(t) + \left\{
          \begin{array}{ll}
            -A(t) & \text{if } V(t) \le -A(t),\\
            V(t) & \text{if } -A(t) < V(t) < 0,\\
            0 & \text{if } V(t) \ge 0,
          \end{array}
        \right.\\
        &\ge -\frac{V(t_0)}{A(t_0)} A(t) - A(t)
        \ge - A(t) \tag{$-\frac{V(t_0)}{A(t_0)}A(t)$ > 0}\\
        &\ge -  A(T)
      \end{align*}
      which is a deterministic constant as required. Hence, the strategy $(x^*_1,\ldots,x^*_d,y^*)$ is admissible.

    \textbf{Self-financing condition}:
      For $t < t_0$, the self-financing condition holds since
      \begin{align*}
      V^*(t) &+ \sum_{i=1}^d \int_0^t x_i^*(s)\mathrm{d}S_i(s) + \int_0^t y^*(s) \mathrm{d}A(s)\\
      &= 0 + \sum_{i=1}^d \int_0^t 0 \mathrm{d}S_i(s) + \int_0^t 0 \mathrm{d}A(s)
      = 0 = V^*(t).
      \end{align*}

      At $t = t_0$, we have $V^*(t) = V(t_0) - \frac{V(t_0)}{A(t_0)} A(t_0) = 0$ so the condition also holds at this time.

      For a fixed $\omega \in \Omega$, we can divide the $(t,v)$-plane into three regions where $v < -A(t)$, $-A(t) < v < 0$ or $v \ge 0$ respectively. While the graph $(t,V(t))$ of $V(t)$ remains in any of these regions, the strategy only changes as a result of $(x_1,\ldots,x_d,y)$ changing and so the self-financing condition holds since $(x_1,\ldots,x_d,y)$ is itself self-financing. Since both $V(t)$ and $A(t)$ have continuous paths, the graph of $V(t)$ can only cross from one region to another at the boundary, i.e. at a point where $V(t) = -A(t)$ or $V(t) = 0$. It is therefore sufficient to verify that the self-financing condition holds at these points. When $V(t) = -A(t)$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = -A(t) - \frac{V(t_0)}{A(t_0)} A(t)\\
        &= \left(- \frac{V(t_0)}{A(t_0)} - 1\right) A(t)
      \end{align*}
      and, when $V(t) = 0$, we have
      \begin{align*}
        \sum_{i=1}^{d} &x_i(t) S_i(t) + \left(- \frac{V(t_0)}{A(t_0)} + y(t)\right) A(t)\\
        &= V(t) - \frac{V(t_0)}{A(t_0)} A(t) = - \frac{V(t_0)}{A(t_0)} A(t).
      \end{align*}
      That is, the strategy is self-financing.

    \textbf{Arbitrage conditions}: We have $V^*(0) = 0$. On $A$, $V^*(T) = 0 \ge 0$ and on $A^C$ which has positive probability, we have
    \begin{align*}
      V^*(T) = -\frac{V(t_0)}{A(t_0)} A(T) > 0
    \end{align*}
    since $V(T) \ge 0$. Hence $V^*(T)$ is non-negative almost surely and strictly positive with positive probability as required.

    That is, we have constructed an arbitrage opportunity but this leads to a contradiction by the No-Arbitrage Principle (Assumption~\ref{ass:no-arbitrage-principle}), thus completing the proof.
  \end{proof}
\end{lemma}

Note that by inspecting the proof, we can see that this lemma equally applies to strategies in the extended market. We can now use this lemma to prove the theorem as promised.

\begin{proof}[Proof of Theorem~\ref{thm:bs-repl-strat-derivative-prices}]
  Let $(x_1,\ldots,x_d,y)$ be the replicating strategy with value process $V$. Consider the two strategies in the extended market
  \begin{align*}
    (x_1^+,\ldots,x_d^+,y^+,z^+), && (x_1^-,\ldots,x_d^-,y^-,z^-),
  \end{align*}
  with value process $V^\pm$ and with $x_i^\pm(t) = \pm x_i(t)$ for $i=1,\ldots,d$, $y^\pm(t) = \pm y(t)$ and $z^\pm(t) = (-1) (\pm z(t))$ for all $t \in [0,T]$.  Then their values satisfy $V^+(t) = - V^-(t) = V(t) - H(t)$ for all $t \in [0,T]$.

  Note that since both $V(t)$ and $H(t)$ have continuous paths (by assumption of this theorem and by Assumption~\ref{ass:bs-derivative-regularity} respectively), then so do $V^\pm$. Moreover, $V^\pm(T) = \pm (V(T) - H(T)) = 0 \ge 0$. Hence, by Lemma~\ref{lem:bs-non-negative-strat} twice, we have
  \begin{align*}
    V^+(t) \ge 0, && V^-(t) \ge 0,
  \end{align*}
  almost surely, for all $t \in [0,T]$. But then $V^+(t) = - V^+(t) \le 0$ so $V^+(t) = 0$.

  In other words, $V(t) - H(t) = 0$ or, equivalently, $V(t) = H(t)$ almost surely for all $t \in [0,T]$ as required.
\end{proof}

The following theorem gives us sufficient conditions for replicability. As we will see, the existence of a replicating strategy is enough to price the derivative.

\begin{theorem}\label{thm:bs-replicability}
  (\cite{capinski_blackscholes_2012}, Theorem 3.11 \footref{foot:basic-bs-model})
  Let $H$ be a derivative with a square-integrable payoff, i.e. $\mathbb{E}_Q(H_\text{payoff}^2) < \infty$. If there exists a deterministic constant $L > 0$ such that $H(T) \ge -L$ almost surely, then the derivative is replicable by a strategy whose value process has continuous paths.

  \begin{proof}
    We construct a replicating strategy $(x_1,\ldots,x_d,y)$ with value process $V$ for the derivative. Since it replicates the derivative, we must have that $V(T) = H(T)$ and its discounted value must be a $Q$-martingale. Hence, for all $t \in [0,T]$,
    \begin{align*}
      \widetilde{V}(t)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} V(T) \mid \mathcal{F}^\mathbf{W}_t\right)
      = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right).
    \end{align*}

    But then,
    \begin{align*}
      \mathbb{E}_Q(\widetilde{V}(t)^2)
      &= \mathbb{E}_Q\left(\mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right)^2\right)\\
      &\le \mathbb{E}_Q\left(\mathbb{E}_Q\left(e^{-2\int_0^T r(s) \mathrm{d}s} H(T)^2 \mid \mathcal{F}^\mathbf{W}_t\right)\right) \tag{Conditional Jensen's Inequality}\\
      &= e^{-2\int_0^T r(s) \mathrm{d}s} \mathbb{E}_Q\left(H(T)^2\right) < \infty,
    \end{align*}
    since $H(T) = H_\text{payoff}$ is square-integrable. Hence, $\widetilde{V}(t)$ is square-integrable for all $t \in [0,T]$. Therefore, by the Martingale Representation Theorem~\ref{thm:martingale-representation}, there exists $\mathbf{\Gamma}$ such that
    \begin{align}\label{eq:bs-repl-deriv-gamma}
      \widetilde{V}(t)
      &= \widetilde{V}(0) + \int_0^t \mathbf{\Gamma}(s) \cdot \mathrm{d}\mathbf{W}^Q(s).
    \end{align}

    Since $\Gamma_i \in \mathcal{M}^2$, the stochastic integrals have continuous paths by Definition~\ref{def:stochastic-integral-as-process} and, since $A(t)$ is also continuous, then $V(t) = A(t) \widetilde{V}(t)$ has continuous paths.

    Note that this process $\widetilde{V}$ exists whether there exists a replicating strategy or not, being the conditional expectation under $Q$ of the discounted payoff, so we are not in fact using the existence of a replicating strategy in order to construct one, however it may seem at first sight.

    In order for the strategy to be self-financing, it needs to satisfy
    \begin{align}\label{eq:bs-repl-deriv-self-financing}
      \mathrm{d}V(t) &= \sum_{i=1}^d x_i(t) \mathrm{d}S_i(t) + y(t) \mathrm{d}A(t).
    \end{align}
    By the It\^o Formula (Theorem~\ref{thm:ito-formula}) with $F(t,x) = \frac{1}{x}$,
    \begin{align}\label{eq:bs-repl-deriv-risk-free-inv}
      \mathrm{d}A(t)^{-1} = -A(t)^{-2} r(t) A(t) \mathrm{d}t = - r(t) A(t)^{-1} \mathrm{d}t,
    \end{align}
    so, by the It\^o Product Rule (Theorem~\ref{thm:ito-product-rule}), we obtain
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= A(t)^{-1} \mathrm{d}V(t) + V(t) \mathrm{d}A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) + V(t) \mathrm{d}A(t)^{-1}\tag{by \eqref{eq:bs-repl-deriv-self-financing}}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) \mathrm{d}A(t) - V(t) r(t) A(t)^{-1} \mathrm{d}t \tag{by \eqref{eq:bs-repl-deriv-risk-free-inv}}.
    \end{align*}
    Substituting $V(t)$ and by \eqref{eq:multi-bs-eq-risk-free},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) + A(t)^{-1} y(t) r(t) A(t) \mathrm{d}t - y(t) A(t) r(t) A(t)^{-1} \mathrm{d}t\\
        &\ \ \ \ - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1}\\
      &= \sum_{i=1}^d A(t)^{-1} x_i(t) \mathrm{d}S_i(t) - \sum_{i=1}^d x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t.
    \end{align*}
    Then, by Theorem~\ref{thm:bs-risk-neutral-dynamics},
    \begin{align*}
      \mathrm{d}\widetilde{V}(t)
      &= \sum_{i=1}^d \Big[ A(t)^{-1} x_i(t) r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^d A(t)^{-1} x_i(t) c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t) \\
        &\ \ \ \ \ \ \ \ \ - x_i(t) S_i(t) r(t) A(t)^{-1} \mathrm{d}t \Big]\\
      &= \sum_{i,j=1}^d x_i(t) c_{ij}(t) \widetilde{S}_i(t) \mathrm{d}W^Q_j(t)
      = \mathbf{C}(t) \mathbf{\theta}(t) \cdot \mathrm{d}\mathbf{W}^Q(t),
    \end{align*}
    where $\theta_i(t) = x_i(t) \widetilde{S}_i(t)$, for $i=1,\ldots,d$.

    Both this last equation and \eqref{eq:bs-repl-deriv-gamma} are satisfied if $\mathbf{C} \mathbf{\theta} = \mathbf{\Gamma}$. This implies that $\mathbf{\theta} = \mathbf{C}^{-1} \mathbf{\Gamma}$. But then,
    \begin{align*}
      x_i = \frac{(\mathbf{C}^{-1} \mathbf{\Gamma})_i}{\widetilde{S}_i},
    \end{align*}
    for all $i=1,\ldots,d$. Moreover, since $V(t) = \sum_{i=1}^d x_i(t) S_i(t) + y(t) A(t)$, we also have
    \begin{align*}
      y(t) = \frac{1}{A(t)} \left(\sum_{i=1}^d x_i(t) S_i(t) - V(t)\right).
    \end{align*}

    Hence, the strategy constructed above is self-financing with a discounted value process that forms a $Q$-martingale and such that $V(T) = H(T)$. We are only left to show admissibility for that strategy to replicate $H$. But, for all $t \in [0,T]$, we have that, almost surely,
    \begin{align*}
      V(t) &= \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right)\\
      &\ge \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} (-L) \mid \mathcal{F}^\mathbf{W}_t\right)
      = - e^{-\int_t^T r(s) \mathrm{d}s} L,
    \end{align*}
    which is a deterministic constant.

    Therefore, we have constructed a replicating strategy, thus proving that the derivative is indeed replicable. Moreover, we have also shown that its value process has continuous paths.
  \end{proof}
\end{theorem}

As promised, we now show how to express the price of a derivative as a $Q$-expectation. This is an important result since it allows us to price derivatives provided that they are replicable, without having to know any replicating strategy.

\begin{theorem}\label{thm:bs-derivative-pricing}
  (\cite{capinski_blackscholes_2012}, Theorem 3.11 \footref{foot:basic-bs-model})
  If a derivative $H$ is replicable by a strategy whose value process $V$ has continuous paths, then
  \begin{align}\label{eq:bs-derivative-pricing}
    \widetilde{H}(t) = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t),
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    By Theorem~\ref{thm:bs-repl-strat-derivative-prices}, we have that $H(t) = V(t)$, for all $t \in [0,T]$. Multiplying both sides by the discounting factor $\exp\left(-\int_0^t r(s) \mathrm{d}s\right)$, we also have that $\widetilde{H}(t) = \widetilde{V}(t)$, for all $t \in [0,T]$.

    By definition of replicating strategy, the process $\widetilde{V}$ is a Q-martingale. Therefore,
    \begin{align*}
      \widetilde{H}(t) = \widetilde{V}(t)
      = \mathbb{E}_Q(\widetilde{V}(T) \mid \mathcal{F}^\mathbf{W}_t)
      = \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t).
    \end{align*}
  \end{proof}
\end{theorem}

Alternatively, by multiplying both sides by $A(t)$, we can write \eqref{eq:bs-derivative-pricing} as
\begin{align*}
  H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H(T) \mid \mathcal{F}^\mathbf{W}_t\right),
\end{align*}
for all $t \in [0,T]$. In particular at time 0, since $\mathcal{F}^\mathbf{W}_0$ is the trivial $\sigma$-field, we have
\begin{align*}
  H(0) = \mathbb{E}_Q\left(\widetilde{H}(T)\right)
  = \mathbb{E}_Q\left(e^{-\int_0^T r(s) \mathrm{d}s} H(T)\right).
\end{align*}

\subsection{European Options}

Up until now, we have covered pricing of generic derivative securities. We now focus on more specific derivatives: European options and, in particular, European basket options which are the main concern of this dissertation.

\begin{definition}
  The \textbf{positive part function} $(\cdot)^+ : \mathbb{R} \to \mathbb{R}$ is given by
  \begin{align*}
    x^+ = \left\{\begin{array}{ll}
      0 & \text{if } x < 0,\\
      x & \text{if } x \ge 0.
    \end{array}\right.
  \end{align*}
\end{definition}

\begin{definition}\label{def:bs-european-option}
  European \textbf{call and put options} with \textit{expiry time} $T$, \textit{strike price} $K > 0$, and \textit{underlying} $U$, a non-negative replicable $\mathcal{F}^\mathbf{W}_t$-adapted process with continuous paths, are derivative securities with payoffs
  \begin{align*}
    H_\text{call} = (U(T) - K)^+, && H_\text{put} = (K - U(T))^+.
  \end{align*}
\end{definition}

There exists an important relation between the prices of corresponding call and put options. Given the price of one, we can easily find the price of the other.

\begin{theorem}[Put-Call Parity]\label{thm:put-call-parity}
  (\cite{capinski_blackscholes_2012}, Theorem 3.16)
  Let $C(t)$ and $P(t)$ be the price processes of European call and put options respectively with identical expiry time $T$, strike prike $K$, and underlying $U$. Then
  \begin{align}\label{eq:put-call-parity}
    C(t) - P(t) = U(t) - e^{-\int_t^T r(s) \mathrm{d}s} K,
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    Let $(x_1^U,\ldots,x_d^U,y^U)$ the strategy with value $V^U$ replicating the underlying $U$. It exists since $U$ is replicable by Definition~\ref{def:bs-european-option}.

    A portfolio consisting of one long such call option and one short such put option is equivalent to a derivative with payoff
    \begin{align*}
      (U(T) - K)^+ - (K - U(T))^+
      &= U(T) - K.
    \end{align*}

    We thus can replicate it by the strategy $(x_1,\ldots,x_d,y)$ where
    \begin{align*}
      x_i(t) &= x_i^U(t), \text{ for } i=1,\ldots,d,\\
      y(t) &= y^U(t) - \frac{K}{A(T)}.
    \end{align*}
    Note that $y(t)$ is $\mathcal{F}^\mathbf{W}_t$-adapted since $y^U(t)$ is adapted and $r$ is a deterministic function. The value of the strategy at time $T$ is
    \begin{align*}
      V(T) = \sum_{i=1}^d x_i^U(T) S_i(T) + y^U(T) A(T) - \frac{K}{A(T)} A(T)
      = U(T) - K.
    \end{align*}

    Hence, Theorem~\ref{thm:bs-repl-strat-derivative-prices} implies that equation \eqref{eq:put-call-parity} indeed holds.
  \end{proof}
\end{theorem}

At any time $t$, we say that a call option on the underlying $U(t)$ is \textit{in the money} if $U(t) > K$, \textit{at the money} if $U(t) = K$ and \textit{out of the money} if $U(t) < K$. For put options, we say that it is \textit{in the money} if $U(t) < K$, \textit{at the money} if $U(t) = K$ and \textit{out of the money} if $U(t) > K$.

We can show that such derivatives are replicable and that we can thus price them using the results developed above.

\begin{theorem}\label{thm:bs-option-pricing}
  A European option with price process $H$ and payoff $H_\text{payoff}$ on an underlying $U$ such that $U(T)$ is square-integrable, is replicable by a strategy whose value process has continuous paths. Moreover,
  \begin{align}\label{eq:bs-option-pricing}
    H(t) = \mathbb{E}_Q\left(e^{-\int_t^T r(s) \mathrm{d}s} H_\text{payoff} \mid \mathcal{F}^\mathbf{W}_t\right).
  \end{align}

  \begin{proof}
    First note that
    \begin{align*}
      H_\text{call}^2 = ((U(T) - K)^+)^2 &\le (U(T) - K)^2,\\
      H_\text{put}^2 = ((K - U(T))^+)^2 &\le (U(T) - K)^2,
    \end{align*}
    so, in both cases, we have
    \begin{align*}
      \mathbb{E}_Q(H_\text{payoff}^2)
      \le \mathbb{E}_Q((U(T) - K)^2)
      = \mathbb{E}_Q(U(T)^2) - 2K\mathbb{E}_Q(U(T))) + K^2
      < \infty
    \end{align*}
    since $U(T) \in L^2(\Omega, Q)$ and hence $U(T) \in L^1(\Omega, Q)$ as well.

    In addition, since the payoff of an option is non-negative, it is bounded below by the deterministic constant $0$. Thus, by Theorem~\ref{thm:bs-replicability}, there exists a replicating strategy whose value process has continuous paths.

    By Theorem~\ref{thm:bs-derivative-pricing}, the existence of such a strategy in turn implies that \eqref{eq:bs-option-pricing} holds.
  \end{proof}
\end{theorem}

The results above can be specialised to basket options as defined below.

\begin{definition}
  A European \textbf{basket option} is a European option on an underlying of the form
  \begin{align*}
    B_\mathbf{w}(t) = \sum_{i=1}^{d} w_i S_i(t) = \mathbf{w}^T \mathbf{S}(t),
  \end{align*}
  with $\mathbf{w} = (w_1, \ldots, w_d)$, called the \textbf{weights}, such that $w_i \ge 0, i=1,\ldots,d$.
\end{definition}

Note that this is well-defined since $U$ is clearly replicable by the constant strategy $(w_1, \ldots, w_d, 0)$ and since $S_i, i=1,\ldots,d$ are non-negative $\mathcal{F}^\mathbf{W}_t$-adapted processes.

We denote the price of a European basket call option with weights $\mathbf{w}$ by $C_\mathbf{w}(t)$ and its put option counterpart by $P_\mathbf{w}(t)$.

\begin{theorem}
  Consider a European basket option with underlying $U$ and weights $\mathbf{w}$. Then $U(T)$ is square-integrable.

  \begin{proof}
    By Assumption~\ref{ass:bs-stock-price-square-integrability}, $S_i \in L^2(\Omega, Q)$ for all $i=1,\ldots,d$. Since $L^2(\Omega, Q)$ is a vector space over $\mathbb{R}$, the linear combination $U(T) = \sum_{i=1}^{d} w_i S_i(T)$ is also in $L^2(\Omega, Q)$, thus completing the proof.
  \end{proof}
\end{theorem}

\begin{corollary}
  The price process of a European basket option satisfies \eqref{eq:bs-option-pricing}.

  \begin{proof}
    The result immediately follows from the previous theorem together with Theorem~\ref{thm:bs-option-pricing}.
  \end{proof}
\end{corollary}

\section{The Fokker-Planck Equation}\label{sec:fokker-planck}

In this section, we depart somewhat from financial mathematics to show a key result: the Fokker-Planck equation. This result is at the core of the proofs of Dupire's Equation and its generalisation to multiple assets, which are the main results shown in this dissertation.

This section is based on \textcite{pavliotis_stochastic_2014}. We work in a filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}^\mathbf{W}_t, P)$ without stating it explicitly as to not clutter the argument unnecessarily.

\begin{definition}
  A \textbf{Markov process} is a stochastic process $\mathbf{X}(t)$ that satisfies the \textit{Markov condition}:
  \begin{align*}
    \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
    = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}),
  \end{align*}
  for all bounded Borel functions $f$ and for $0 \le s < t \le T$.
\end{definition}

We also say that a Markov process is Markov, using the name as an adjective. Informally, the future evolution of a Markov process only depends on its current state, independently from its past evolution.

For $0 \le s < t \le T$ and $\mathbf{\Gamma} \in \mathcal{B}({\mathbb{R}^d})$, the Markov condition implies that
\begin{align*}
  P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s)
  &= \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
  = \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)})
  = \phi(\mathbf{X}(s)),
\end{align*}
for some Borel function $\phi$ since any $\mathcal{F}_{\mathbf{X}(s)}$-measurable random variable can be expressed as a Borel function of $\mathbf{X}(s)$.
This justifies the following definition since it ensures that the defined object exists.

\begin{definition}
  Let $\mathbf{X}(t)$ be a $d$-dimensional Markov process. A \textbf{transition (probability) function} of $\mathbf{X}$ is a Borel function $\nu(\mathbf{\Gamma}, t; \mathbf{x}, s)$, for $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$, and $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$, such that
  \begin{align}
    \nu(\mathbf{\Gamma}, t; \mathbf{X}(s), s) = P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s).\label{eq:fp-transition-function}
  \end{align}
\end{definition}

Note that, the function $\mathbf{\Gamma} \mapsto \nu(\mathbf{\Gamma}, t; \mathbf{x}, s)$ is a probability measure for any fixed $0 \le s < t \le T$ and $\mathbf{x} \in \mathbb{R}^d$. In the rest of this section, we focus on transition functions that have a density, leading to the following definition.

\begin{definition}
  Let $\mathbf{X}(t)$ be a Markov process such that it has a transition function $\nu(\mathbf{\Gamma},t;\mathbf{x},s)$ that admits a density $\rho$ with respect to the Lebesgue measure, i.e.
  \begin{align*}
    \nu(\mathbf{\Gamma},t;\mathbf{y},s) = \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x},
  \end{align*}
  for all $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$, $\mathbf{y} \in \mathbb{R}^d$, and $0 \le s < t \le T$.
  We say that $\rho$ is a \textbf{transition (probability) density} of the Markov process.
\end{definition}

In order to prove the main result of this section, we will make use of the following property of the transition probability density. There exist several versions of this equation including one in terms of the transition function that does not require the existence of a density. However, we only state a single version here for brevity.

\begin{theorem}[Chapman-Kolmogorov Equation]\label{thm:fp-chapman-kolmogorov}
  (\cite{pavliotis_stochastic_2014}, Section 2.2)
  Let $\mathbf{X}(t)$ be a Markov process with transition probability density $\rho$. Then
  \begin{align*}
    \rho(\mathbf{x}, t; \mathbf{y}, s) = \int_{\mathbb{R}^d} \rho(\mathbf{x}, t; \mathbf{z}, u) \rho(\mathbf{z}, u; \mathbf{y}, s) \mathrm{d}\mathbf{z},
  \end{align*}
  for $0 \le s < u < t \le T$, for $\mathbf{y} \in \mathrm{Im}(\mathbf{X}(s))$, and for almost all $\mathbf{x} \in \mathbb{R}^d$.

  \begin{proof}
    Let $\mathbf{\Gamma} \in \mathcal{B}(\mathbb{R}^d)$ and $0 \le s < u < t \le T$. Then
    \begin{align*}
      \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{X}(s),s) \mathrm{d}\mathbf{x}
      &= \nu(\mathbf{\Gamma},t;\mathbf{X}(s),s)
      = P(\mathbf{X}(t) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(\mathbb{E}(\mathbf{1}_\mathbf{\Gamma}(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_u) \mid \mathcal{F}^\mathbf{W}_s) \tag{tower property}\\
      &= \mathbb{E}(\nu(\mathbf{\Gamma},t;\mathbf{X}(u),u) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \int_{\mathbb{R}^d} \nu(\mathbf{\Gamma},t;\mathbf{z},u) \nu(\mathrm{d}\mathbf{z},u;\mathbf{X}(s),s) \tag{by \eqref{eq:fp-transition-function}}\\
      &= \int_{\mathbb{R}^d} \nu(\mathbf{\Gamma},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}\\
      &= \int_{\mathbb{R}^d} \int_\mathbf{\Gamma} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{z}\\
      &= \int_\mathbf{\Gamma} \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z} \mathrm{d}\mathbf{x},
    \end{align*}
    by Fubini's Theorem since the integral of a density is finite.

    Since this holds for all $\mathbf{\Gamma}$, the integrand must be equal for almost all $\mathbf{x}$, i.e.
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{X}(s),s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{X}(s),s) \mathrm{d}\mathbf{z}.
    \end{align*}

    And since this itself holds for all $\omega \in \Omega$, then for all $\mathbf{y} \in \mathrm{Im}(\mathbf{X}(s))$ such that $\mathbf{y} = \mathbf{X}(s,\omega)$ for some $\omega \in \Omega$, we have
    \begin{align*}
      \rho(\mathbf{x},t;\mathbf{y},s)
      &= \int_{\mathbb{R}^d} \rho(\mathbf{x},t;\mathbf{z},u) \rho(\mathbf{z},u;\mathbf{y},s) \mathrm{d}\mathbf{z}.
    \end{align*}
  \end{proof}
\end{theorem}

In the following, the notation $C^2_0(\mathbb{R}^d)$ denotes the set of twice-continuously differentiable functions on $\mathbb{R}^d$ such that they vanish at $\pm \infty$, i.e. for $f \in C^2_0(\mathbb{R}^d)$, we have
\begin{align*}
  \lim_{x \to \pm \infty} f(x) = 0.
\end{align*}
This implies that the first and second derivatives also vanish at $\pm \infty$.

We can now state and prove the main result of this section. Note that it is also called the \textit{Forward Kolmogorov Equation} in other resources.

\begin{theorem}[Fokker-Planck Equation]\label{thm:fokker-planck}
  (\cite{pavliotis_stochastic_2014}, Theorem 2.2)
  Let $\mathbf{X}(t)$ be an It\^o diffusion that is Markov, with transition probability density $\rho(\mathbf{x},t;\mathbf{y},s)$ that is in $C^2$ as a function of $\mathbf{x}$ and $t$, satisfying equations of the form
  \begin{align*}
    \mathrm{d}X_i(t) = a_i(t, \mathbf{X}(t)) \mathrm{d}t + \sum_{j=1}^d b_{ij}(t, \mathbf{X}(t)) \mathrm{d}W_j(t),
  \end{align*}
  for $i=1,\ldots,d$. Then, for all $0 \le s < t \le T$, $\mathbf{x} \in \mathbb{R}^d$ and $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$,
  \begin{align*}
    \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
    &= - \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x})\rho(\mathbf{x},t;\mathbf{y},s)]\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)].
  \end{align*}

  \begin{proof}
    Let $f \in C^2_0(\mathbb{R}^d)$, $0 \le s < t \le T$ and $h > 0$. Then, by the It\^o Formula (Theorem~\ref{thm:ito-formula}),
    \begin{align*}
      f(\mathbf{X}&(t+h)) - f(\mathbf{X}(t))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\\
        &\ \ \ \ \ + \sum_{i,j=1}^{d} \int_t^{t+h} f_{x_i}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) \mathrm{d}W_j(s)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s.
    \end{align*}

    Since $f$ and its partial derivatives in the previous equation are in $C_0$, then they are bounded. Indeed, for any $g \in C_0(\mathbb{R}^d)$ and any $\epsilon > 0$, there exists a closed rectangle $R \subseteq \mathbb{R}^d$ such that $|g(\mathbf{x})| < \epsilon$ for $\mathbf{x} \in \mathbb{R}^d \setminus R$. Moreover, $R$ is compact and $g$ is continuous so $|g|$ is bounded by some $M$ on $R$ by the Boundedness Theorem. Hence, it is bounded on the whole $\mathbb{R}^d$ (by $\max\{\epsilon, M\}$).

    By Lemma~\ref{lem:ito-diffusion-characteristics-m2}, $b_{ij}(s,\mathbf{X}(s)) \in \mathcal{M}^2$, so, together with the boundedness of $f_{x_i}(\mathbf{X}(s))$, we have that the integrands $f_{x_i}(\mathbf{X}(s))b_{ij}(s,\mathbf{X}(s)) \in \mathcal{M}^2$ in the previous equation are in $\mathcal{M}^2$. Therefore, by Theorem~\ref{thm:stochastic-integral-expectation-m2} and by linearity of the expectation,
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s)) \mathrm{d}s\right)\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(\int_t^{t+h} f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s)) \mathrm{d}s\right).
    \end{align*}
    By the Fubini Theorem, we have
    \begin{align*}
      \mathbb{E}(f&(\mathbf{X}(t+h)) - f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s.
    \end{align*}

    Therefore, we have
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \lim_{h \to 0} \frac{1}{h} (\mathbb{E}(f(\mathbf{X}(t+h))) - \mathbb{E}(f(\mathbf{X}(t))))\\
      &= \lim_{h \to 0} \frac{1}{h} \mathbb{E}(f(\mathbf{X}(t+h)) - f(\mathbf{X}(t))) \tag{linearity}\\
      &= \sum_{i=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}(f_{x_i}(\mathbf{X}(s)) a_i(s,\mathbf{X}(s))) \mathrm{d}s\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \lim_{h \to 0} \frac{1}{h} \int_t^{t+h} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(s)) b_{ij}(s,\mathbf{X}(s)) b_{lj}(s,\mathbf{X}(s))\right) \mathrm{d}s\\
      &= \sum_{i=1}^{d} \mathbb{E}(f_{x_i}(\mathbf{X}(t)) a_i(t,\mathbf{X}(t)))\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \mathbb{E}\left(f_{x_i x_l}(\mathbf{X}(t)) b_{ij}(t,\mathbf{X}(t)) b_{lj}(t,\mathbf{X}(t))\right)\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x})\rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{x}.
    \end{align*}

    Using the Chapman-Kolmogorov Equation (Theorem~\ref{thm:fp-chapman-kolmogorov}), we get
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x})\rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}
    \end{align*}
    and, by the Fubini Theorem,
    \begin{align*}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d}  \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.
    \end{align*}

    By integration by parts and since $f$ and its partial derivatives vanish at $\pm \infty$, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}
      &= - \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x},
    \end{align*}
    for all $i=1,\ldots,d$. Similarly using integration by parts twice, we have
    \begin{align*}
      \int_{\mathbb{R}^d} f_{x_i x_l}(\mathbf{x}) &b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x}\\
      &= - \int_{\mathbb{R}^d} f_{x_i}(\mathbf{x}) \frac{\partial}{\partial x_l}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}\\
      &= \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x}.
    \end{align*}
    Hence,
    \begin{align}
      \frac{\partial}{\partial t}&\mathbb{E}(f(\mathbf{X}(t)))\notag\\
      &= \sum_{i=1}^{d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) \left( - \int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \right) \mathrm{d}\mathbf{y}\notag\\
        &\ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \left.\int_{\mathbb{R}^d} \right(\rho(\mathbf{y},s;\mathbf{X}(0),0) \notag\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.\int_{\mathbb{R}^d} f(\mathbf{x}) \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \mathrm{d}\mathbf{x} \right)\mathrm{d}\mathbf{y}\notag\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \left(- \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \right.\notag\\
        &\ \ \ \ \ + \left.\frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)] \right) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}.\label{eq:fp-proof-1}
    \end{align}

    But
    \begin{align}
      \frac{\partial}{\partial t}\mathbb{E}(f(\mathbf{X}(t)))
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{X}(0),0) \mathrm{d}\mathbf{y}\notag\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}\tag{Chapman-Kolmogorov}\\
      &= \frac{\partial}{\partial t}\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\tag{Fubini}\\
      &= \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\partial}{\partial t}[f(\mathbf{y}) \rho(\mathbf{x},t;\mathbf{y},s) \rho(\mathbf{y},s;\mathbf{X}(0),0)] \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \tag{by the Leibniz Integral Rule since $f\rho \in C^2$}\\
      &= \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \rho(\mathbf{y},s;\mathbf{X}(0),0) f(\mathbf{x}) \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}\label{eq:fp-proof-2}
    \end{align}
    since $f$ does not depend on $t$.

    But then, since \eqref{eq:fp-proof-1} and \eqref{eq:fp-proof-2} are equal for any choice of $f \in C^2_0(\mathbb{R}^d)$, we have
    \begin{align*}
      \frac{\partial}{\partial t}\rho(\mathbf{x},t;\mathbf{y},s)
      &= - \sum_{i=1}^d \frac{\partial}{\partial x_i}[a_i(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)]\\
        &\ \ \ \ \ + \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[b_{ij}(t,\mathbf{x}) b_{lj}(t,\mathbf{x}) \rho(\mathbf{x},t;\mathbf{y},s)],
    \end{align*}
    for all $\mathbf{y} \in \mathbb{R}^d$ such that $\rho(\mathbf{y},s;\mathbf{X}(0),0) \neq 0$, i.e. for all $\mathbf{y} \in \mathrm{supp}(\mathbf{X}(s))$, thus completing the proof.
  \end{proof}
\end{theorem}

For the rest of this section, we prove some results that will be used to show interesting properties of the local volatility model, which is the subject of the next section. In \textcite{oksendal_stochastic_2003}, it is shown that all It\^o diffusions are Markov. However, the following results were simpler to prove and sufficient for our purposes.

\begin{lemma}\label{lem:fp-ito-diffusion-indep-increments}
  Let $\mathbf{X}$ be an It\^o diffusion satisfying
  \begin{align}\label{eq:fp-ito-diffusion-deterministic-drift}
    \mathrm{d}\mathbf{X}(t) = a(t) \mathrm{d}t + b(t,\mathbf{X}(t)) \mathrm{d}\mathbf{W}(t),
  \end{align}
  where $a$ is a deterministic function and $b(t,\mathbf{X}(t)) \in \mathcal{M}^2$. Then the increments $\mathbf{X}(t) - \mathbf{X}(s)$ are independent from $\mathcal{F}^\mathbf{W}_s$, for all $0 \le s < t \le T$.

  \begin{proof}
    Let $0 \le s < t \le T$. Firstly, the assumption that $b(t,\mathbf{S}(t)) \in \mathcal{M}^2$ implies that the stochastic integral is a martingale (as a function of the upper endpoint of integration), so that
    \begin{align}\label{eq:fp-ito-diffusion-indep-increments-martingale}
      \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)
      &= \int_s^s b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) = 0.
    \end{align}
    Therefore,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u \mid \mathcal{F}^\mathbf{W}_s\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right) \tag{$a$ deterministic}\\
      &= \int_s^t a(u) \mathrm{d}u.
    \end{align*}
    Secondly,
    \begin{align*}
      \mathbb{E}&(\mathbf{X}(t) - \mathbf{X}(s))\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u)\right)\\
      &= \mathbb{E}\left(\int_s^t a(u) \mathrm{d}u\right) + \mathbb{E}\left(\mathbb{E}\left(\int_s^t b(u,\mathbf{X}(u)) \mathrm{d}\mathbf{W}(u) \mid \mathcal{F}^\mathbf{W}_s\right)\right) \tag{tower property}\\
      &= \int_s^t a(u) \mathrm{d}u + \mathbb{E}\left(0\right) \tag{by \eqref{eq:fp-ito-diffusion-indep-increments-martingale}}\\
      &= \int_s^t a(u) \mathrm{d}u.
    \end{align*}
    But then,
    \begin{align*}
      \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s) \mid \mathcal{F}^\mathbf{W}_s)
      = \mathbb{E}(\mathbf{X}(t) - \mathbf{X}(s)),
    \end{align*}
    so the required independence follows.
  \end{proof}
\end{lemma}

\begin{theorem}\label{thm:fp-ito-diffusion-markov}
  If $\mathbf{X}$ is an It\^o diffusion such that the increments $\mathbf{X}(t) - \mathbf{X}(s)$ are independent from $\mathcal{F}^\mathbf{W}_s$ for all $0 \le s < t \le T$, then $\mathbf{X}$ is Markov.

  \begin{proof}
    Let $f$ be a bounded Borel function. Take $f^*(x,y) = f(x+y)$. It is a bounded Borel function since $f$ is as well. Since $\mathbf{X}(s)$ is $\mathcal{F}^\mathbf{W}_s$-measurable and $\mathbf{X}(t) - \mathbf{X}(s)$ is independent from $\mathcal{F}^\mathbf{W}_s$, then, by Theorem~\ref{thm:cond-exp-measurable-independent},
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)
      &= \mathbb{E}(f^*(\mathbf{X}(s), \mathbf{X}(t) - \mathbf{X}(s)) \mid \mathcal{F}^\mathbf{W}_s)
      = g_{f^*}(\mathbf{X}(s))
    \end{align*}
    where $g_{f^*}$ is a Borel function.

    In other words, $\mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)$ is $\mathcal{F}_{\mathbf{X}(s)}$-measurable and thus
    \begin{align*}
      \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_{s})
      = \mathbb{E}(f(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}).
    \end{align*}
    That is, $\mathbf{X}$ is indeed a Markov process.
  \end{proof}
\end{theorem}

\begin{theorem}\label{thm:fp-markov-composing}
  Let $g$ be an invertible Borel function and let $\mathbf{X}$ be a Markov process. If $\mathbf{Y}$ is a stochastic process given by $\mathbf{Y}(t) = g(\mathbf{X}(t))$, then $\mathbf{Y}$ is Markov.

  \begin{proof}
    Let $f$ be a bounded Borel function and let $0 \le s < t \le T$. Since $g$ is an invertible Borel function, then $\mathcal{F}_{\mathbf{X}(s)} = \mathcal{F}_{\mathbf{Y}(s)}$. Indeed, $\mathcal{F}_{\mathbf{Y}(s)} \subseteq \mathcal{F}_{\mathbf{X}(s)}$ since $\mathbf{Y}(s) = g(\mathbf{X}(s))$ and $\mathcal{F}_{\mathbf{X}(s)} \subseteq \mathcal{F}_{\mathbf{Y}(s)}$ since $\mathbf{X}(s) = g^{-1}(\mathbf{Y}(s))$. Moreover, since $g$ is Borel, then $f^* = f \circ g$ is a bounded Borel function. Hence,
    \begin{align*}
      \mathbb{E}(f(\mathbf{Y}(t)) \mid \mathcal{F}^\mathbf{W}_s)
      &= \mathbb{E}(f^*(\mathbf{X}(t)) \mid \mathcal{F}^\mathbf{W}_s)\\
      &= \mathbb{E}(f^*(\mathbf{X}(t)) \mid \mathcal{F}_{\mathbf{X}(s)}) \tag{$\mathbf{X}$ is Markov}\\
      &= \mathbb{E}(f(\mathbf{Y}(t)) \mid \mathcal{F}_{\mathbf{Y}(s)}),
    \end{align*}
    so $\mathbf{Y}$ is indeed Markov.
  \end{proof}
\end{theorem}

\section{Local Volatility Model}\label{sec:local-vol}

Dupire's work and the work from \textcite{amster_towards_2009} applies to a local volatility model, developed here. Since this section is not based on any reference as we only derive what we need to show Dupire's Equation (Theorem~\ref{thm:dupire}) and its generalisation (Theorem~\ref{thm:generalisation}), no references are given.

We specialise the multi-asset Black-Scholes model developed before by restricting the risky assets to be It\^o diffusions, i.e.
\begin{align*}
  \mathrm{d}S_i(t) = a_i(t,\mathbf{S}(t)) \mathrm{d}t + \sum_{j=1}^{d} b_{ij}(t,\mathbf{S}(t)) \mathrm{d}W_j(t),
\end{align*}
for all $i=1,\ldots,d$ for some $a,b$ as in Theorem~\ref{thm:sde-solution}. By Theorem~\ref{thm:bs-solution}, the prices of the risky assets are strictly positive. We can thus take
\begin{align*}
  \mu_i(t,\mathbf{S}(t)) = \frac{a_i(t, \mathbf{S}(t))}{S_i(t)},
  && c_{ij}(t,\mathbf{S}(t)) = \frac{b_{ij}(t, \mathbf{S}(t))}{S_i(t)},
\end{align*}
for $i,j=1,\ldots,d$ since $S_i(t) > 0, i=1,\ldots,d$, so that
\begin{align}\label{eq:local-dynamics}
  \mathrm{d}S_i(t) = \mu_i(t,\mathbf{S}(t)) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t,\mathbf{S}(t)) S_i(t) \mathrm{d}W_j(t),
\end{align}
for all $i=1,\ldots,d$, where $\mu_i(t,\mathbf{S}(t)), c_{ij}(t,\mathbf{S}(t)), i,j=1,\ldots,d$ satisfy Assumptions~\ref{ass:drift-vol-regularity} and \ref{ass:vol-matrix-invertible}. Such a model is called a \textit{local volatility model}.

The crucial property of such a model for our purposes is that prices of risky assets are Markov, as shown below.

\begin{theorem}\label{thm:local-stock-prices-markov}
  The prices and discounted prices of the risky assets are Markov under the risk-neutral probability $Q$.

  \begin{proof}
    By Theorem~\ref{thm:bs-risk-neutral-dynamics-discounted}, the discounted prices $\widetilde{S}_i$, for $i=1,\ldots,d$, have zero drift. Moreover, by Assumption~\ref{ass:drift-vol-regularity}, $\mathbf{C}(t,\mathbf{S}(t))$ is adapted and bounded, so it is in $\mathcal{M}^2$. Hence, by Lemma~\ref{lem:fp-ito-diffusion-indep-increments}, $\widetilde{\mathbf{S}}$ has independent increments and, by Theorem~\ref{thm:fp-ito-diffusion-markov}, $\widetilde{\mathbf{S}}$ is a Markov process.

    Moreover, since $g(\mathbf{x}) = e^{\int_0^t r(s) \mathrm{d}s} \mathbf{x}$ is an invertible Borel function such that $\mathbf{S}(t) = g(\widetilde{\mathbf{S}}(t))$, then $\mathbf{S}$ is Markov as well by Theorem~\ref{thm:fp-markov-composing}.
  \end{proof}
\end{theorem}

We denote the transition function of $\mathbf{S}$ as $\nu$ and assume that it has a density.

\begin{assumption}\label{ass:local-density}
  The price process of risky assets $\mathbf{S}$ admits a transition probability density $\rho(\mathbf{x},t;\mathbf{y},s)$ such that $\rho \in C^2$.
\end{assumption}

This implies that, in such a model, the Fokker-Planck Equation (Theorem~\ref{thm:fokker-planck}) holds for $\rho$. Since it is used in the proof of Dupire's Equation and its generalisation, this justifies our restriction to local volatility models. We can express our previous pricing results in terms of $\rho$ as shown below.

\begin{theorem}\label{thm:local-derivative-pricing}
  Let $H_\text{payoff} = f(\mathbf{S}(T))$ be the payoff of some derivative for some Borel function $f : \mathbb{R}^d \to \mathbb{R}$.
  If the derivative is replicable by a strategy whose value process $V$ has continuous paths, then
  \begin{align}\label{eq:local-derivative-pricing}
    \widetilde{H}(t) = e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
  \end{align}
  for all $t \in [0,T]$.

  \begin{proof}
    Let $t \in [0,T]$. By Theorem~\ref{thm:bs-derivative-pricing},
    \begin{align*}
      \widetilde{H}(t)
      &= \mathbb{E}_Q(\widetilde{H}(T) \mid \mathcal{F}^\mathbf{W}_t)\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \mathbb{E}_Q(H(T) \mid \mathcal{F}^\mathbf{W}_t) \tag{$r$ is deterministic}\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \mathbb{E}_Q(f(\mathbf{S}(T)) \mid \mathcal{F}^\mathbf{W}_t)\\
      &= e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \nu(\mathrm{d}\mathbf{x},T;\mathbf{S}(t),t)
    \end{align*}
    since $\nu(\mathbf{\Gamma},T;\mathbf{S}(t),t) = P(\mathbf{S}(T) \in \mathbf{\Gamma} \mid \mathcal{F}^\mathbf{W}_t)$ by definition of Markov process.

    But then, by Assumption~\ref{ass:local-density},
    \begin{align*}
      \widetilde{H}(t)
      &= e^{-\int_0^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
    \end{align*}
    as required.
  \end{proof}
\end{theorem}

Since $\widetilde{H}(t) = e^{-\int_0^t r(s) \mathrm{d}s} H(t)$, then dividing both sides of \eqref{eq:local-derivative-pricing} by the discounting factor in $\widetilde{H}(t)$, we obtain
\begin{align*}
  H(t) = e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} f(\mathbf{x}) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
\end{align*}
for all $t \in [0,T]$.

\begin{corollary}\label{cor:local-option-pricing}
  The prices of European basket call and put options with weights $\mathbf{w}$ and strike price $K$ are
  \begin{align*}
    C_\mathbf{w}(t)
    &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} \left(\mathbf{w}^T\mathbf{x} - K\right)^+ \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},\\
    P_\mathbf{w}(t)
    &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}^d} \left(K - \mathbf{w}^T\mathbf{x}\right)^+ \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
  \end{align*}
  for all $t \in [0,T]$.

  \begin{proof}
    The payoff of the call option is $(\mathbf{w}^T \mathbf{S}(T) - K)^+$ so it can be expressed as $g(\mathbf{S}(T))$ where $g(\mathbf{x}) = (\mathbf{w}^T \mathbf{x} - K)^+$ is a Borel function so the result follows for $C_\mathbf{w}$ from Theorem~\ref{thm:local-derivative-pricing}.

    The same argument with $g(\mathbf{x}) = (K - \mathbf{w}^T \mathbf{x})$ gives us the result for $P_\mathbf{w}$.
  \end{proof}
\end{corollary}

For the remaining of this dissertation, we restrict ourselves to local volatility models.

\section{Dupire's Equation}\label{sec:dupire}

In the case where $d=1$, i.e. there is one risky asset driven by a unique Wiener process, an interesting result was derived by \textcite{dupire_pricing_1993} for local volatility models making use of the fact that, in such models, the price of the risky asset is a Markov process.

Since we work in a single-asset specialisation of the local volatility model developed in the previous section, we simply write $S$ for the risky asset, with dynamics
\begin{align*}
  \mathrm{d}S(t) = r(t) S(t) \mathrm{d}t + c(t,S(t)) S(t) \mathrm{d}W^Q(t).
\end{align*}

We can view an option on $S$ as a basket option with a single weight of 1 so the results from previous sections still hold in this degenerate case.

In this section, we fix $t \ge 0$ and let $C(T,K)$ denote the price at time $t$ of a European call option on the underlying $S$ with expiry time $T$ and strike price $K$.

In \textcite{dupire_pricing_1993}, the asymptotic behaviour of $C(T,K)$ and its partial derivatives is only mentioned in passing. Here, we first show some results about said behaviour before moving to proving Dupire's Equation itself. Just like in the paper, we merely assume the asymptotic behaviour of $\frac{\partial}{\partial T} C(T,K)$ (see Assumption~\ref{ass:dupire-asymptotic-dT}). The paper proves the equation in the absence of interest rates for simplicity. Here, we do take them into account.

\begin{theorem}\label{thm:dupire-asymptotic}
  The function $K \mapsto C(T,K)$ vanishes almost surely at infinity, i.e.
  \begin{align*}
    \lim_{K \to \infty} C(T,K) = 0,
  \end{align*}
  almost surely.

  \begin{proof}
    Let $(f_K)_{K > 0}$ be the sequence $f_K(x) = (x-K)^+ \rho(x,T;S(t),t)$. Note that
    \begin{align*}
      |f_K(x)|
      &\le |x \rho(x,T;S(t),t)|,
    \end{align*}
    for $x > 0$, and
    \begin{align*}
      \int_0^\infty |x\rho(x,T;S(t),t)| \mathrm{d}x
      &= \int_0^\infty x\rho(x,T;S(t),t) \mathrm{d}x \tag{positive integrand}\\
      &= \mathbb{E}_Q(S(T) \mid \mathcal{F}^W_t)
      = S(t) < \infty,
    \end{align*}
    since $S$ is a $Q$-martingale. Hence, $x \rho(x,T;S(t),t)$ is integrable and so, by Lebesgue's Dominated Convergence Theorem, we have
    \begin{align*}
      \lim_{K \to \infty} C(T,K)
      &= \lim_{K \to \infty} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_\mathbb{R} (x-K)^+ \rho(x,T;S(t),t) \mathrm{d}x\right) \\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \lim_{K \to \infty} \int_0^\infty f_K(x) \mathrm{d}x\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_0^\infty \lim_{K \to \infty} f_K(x) \mathrm{d}x\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_0^\infty 0 \mathrm{d}x = 0,
    \end{align*}
    since $(f_K)$ converges pointwise to $0$ as required. Note that we were allowed to limit the domain of integration to $[0,\infty)$ since $S(T) > 0$ so $\rho(x,T;S(t),t) = 0$ for almost all $x < 0$.
  \end{proof}
\end{theorem}

\begin{lemma}\label{lem:dupire-asymptotic-dK}
  The following expressions also vanish as $K \to \infty$:
  \begin{align*}
    K \frac{\partial}{\partial K} C(T,K), && K^2 \frac{\partial^2}{\partial K^2} C(T,K).
  \end{align*}

  \begin{proof}
    First, note that
    \begin{align*}
      e^{-\int_t^T r(s) \mathrm{d}s}& \int_K^\infty (-K) \rho(x,T;S(t),t)\mathrm{d}x\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (x-K-x) \rho(x,T;S(t),t)\mathrm{d}x\\
      &= C(T,K) - e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty x \rho(x,T;S(t),t)\mathrm{d}x \tag{by Corollary~\ref{cor:local-option-pricing}}\\
      &= C(T,K) - e^{-\int_t^T r(s) \mathrm{d}s} \mathbb{E}_Q(\mathbf{1}_{\{S(T) > K\}} S(T) \mid \mathcal{F}^\mathbf{W}_t).
    \end{align*}
    As $K \to \infty$, $C(T,K)$ converges almost surely to 0 by Theorem~\ref{thm:dupire-asymptotic}. Moreover, $\mathbf{1}_{\{S(T) > K\}} S(T)$ converges pointwise to 0. Hence,
    \begin{align}
      \lim_{K \to \infty} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (-K) \rho(x,T;S(t),t)\mathrm{d}x\right) = 0,\label{eq:dupire-asymptotic-int-K}
    \end{align}
    almost surely.

    Since
    \begin{align*}
      \frac{\partial}{\partial K} (K C(T,K)) = K \frac{\partial}{\partial K} C(T,K) + C(T,K),
    \end{align*}
    then
    \begin{align*}
      K \frac{\partial}{\partial K} C(T,K)
      &= \frac{\partial}{\partial K} (K C(T,K)) - C(T,K)\\
      &= \frac{\partial}{\partial K} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty K(x-K) \rho(x,T;S(t),t)\mathrm{d}x \right) - C(T,K)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \frac{\partial}{\partial K} \left[K(x-K) \rho(x,T;S(t),t)\right]\mathrm{d}x\\
        &\ \ \ \ - K(K-K)\rho(K,T;S(t),t) - C(T,K) \tag{Leibniz Integral Rule}\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (x-2K) \rho(x,T;S(t),t)\mathrm{d}x - C(T,K)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (-K) \rho(x,T;S(t),t)\mathrm{d}x + C(T,K) - C(T,K)\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty (-K) \rho(x,T;S(t),t)\mathrm{d}x,
    \end{align*}
    so
    \begin{align}
      \lim_{K \to \infty} \left(K \frac{\partial}{\partial K} C(T,K)\right)
      = 0,\label{eq:dupire-asymptotic-dK}
    \end{align}
    almost surely, by \eqref{eq:dupire-asymptotic-int-K}.

    Similarly, since
    \begin{align*}
      \frac{\partial^2}{\partial K^2} (K^2 C(T,K))
      = 2C(T,K) + 4K \frac{\partial}{\partial K} C(T,K) + K^2 \frac{\partial^2}{\partial K^2} C(T,K)
    \end{align*}
    and
    \begin{align*}
      \frac{\partial^2}{\partial K^2} (K^2 C(T,K))
      &= e^{-\int_t^T r(s)\mathrm{d}s} \frac{\partial^2}{\partial K^2} \int_K^\infty K^2 (x-K) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= e^{-\int_t^T r(s)\mathrm{d}s} \int_K^\infty (2x-6K) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= 4 e^{-\int_t^T r(s)\mathrm{d}s} \int_\mathbb{R} (-K) \rho(x,T;S(t),t) \mathrm{d}x + 2 C(T,K),
    \end{align*}
    so that
    \begin{align*}
      \lim_{K \to \infty} \frac{\partial^2}{\partial K^2} (K^2 C(T,K)) = 0,
    \end{align*}
    by \eqref{eq:dupire-asymptotic-int-K} and by Theorem~\ref{thm:dupire-asymptotic}, then
    \begin{align*}
      &\lim_{K \to \infty} \left(K^2 \frac{\partial^2}{\partial K^2} C(T,K)\right)\\
      &= - 2\lim_{K \to \infty} C(T,K)
        - 4 \lim_{K \to \infty} \left(K \frac{\partial}{\partial K} C(T,K)\right)
        + \lim_{K \to \infty} \frac{\partial^2}{\partial K^2} (K^2 C(T,K))\\
      &= 0,
    \end{align*}
    by Theorem~\ref{thm:dupire-asymptotic} and \eqref{eq:dupire-asymptotic-dK}
  \end{proof}
\end{lemma}

Considering Theorem~\ref{thm:dupire-asymptotic}, it seems reasonable to make the following assumption.

\begin{assumption}\label{ass:dupire-asymptotic-dT}
  The partial derivative $\frac{\partial}{\partial T} C(T,K)$ also vanishes at infinity, i.e.
  \begin{align*}
    \lim_{K \to \infty} \frac{\partial}{\partial T} C(T,K) = 0.
  \end{align*}
\end{assumption}

Having shown the asymptotic behavious of the call option price and its partial derivatives, we now prove the main result.

\begin{theorem}[Dupire Equation]\label{thm:dupire}
  (\cite{dupire_pricing_1993}, Section 3)
  The function $C(T,K)$ satisfies the partial differential equation
  \begin{align*}
    \frac{\partial}{\partial T}C(T,K) = \frac{1}{2} c(T,K)^2 \frac{\partial^2}{\partial K^2} C(T,K) - r(T) K \frac{\partial}{\partial K}C(T,K).
  \end{align*}

  \begin{proof}
    By Corollary~\ref{cor:local-option-pricing}, we have
    \begin{align*}
      C(T,K) &= e^{-\int_t^T r(s) \mathrm{d}s} \int_{\mathbb{R}} \left(x - K\right)^+ \rho(x,T;S(t),t) \mathrm{d}x.
    \end{align*}

    Dividing by the discounting factor and differentiating with respect to $K$, we obtain
    \begin{align*}
      \frac{\partial}{\partial K}&\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right)\\
      &= \frac{\partial}{\partial K} \int_{\mathbb{R}} \left(x - K\right)^+ \rho(x,T;S(t),t) \mathrm{d}x\\
      &= \frac{\partial}{\partial K} \int_K^\infty \left(x - K\right) \rho(x,T;S(t),t) \mathrm{d}x\\
      &= \int_K^\infty \frac{\partial}{\partial K} \left(x - K\right) \rho(x,T;S(t),t) \mathrm{d}x - (K-K) \rho(K,T;S(t),s) \frac{\partial}{\partial K} K \tag{Leibniz Integral Rule}\\
      &= - \int_K^\infty \rho(x,T;S(t),t) \mathrm{d}x.
    \end{align*}
    Differentiating with respect to $K$ once again, we have
    \begin{align}
      \frac{\partial^2}{\partial K^2}\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right)
      &= - \frac{\partial}{\partial K}\int_K^\infty \rho(x,T;S(t),t) \mathrm{d}x\notag\\
      &= \rho(K,T;S(t),t),\label{eq:dupire-rho}
    \end{align}
    by the Fundamental Theorem of Calculus.

    By the Fokker-Plank Equation (Theorem~\ref{thm:fokker-planck}) with $a(t,x) \coloneqq r(t) x$ and $b(t,x) := c(t,x) x$ at the point $(K,T)$, we have
    \begin{align*}
      \frac{\partial}{\partial T}&\rho(K,T;S(t),t)\\
      &= - \frac{\partial}{\partial K}[r(T) K \rho(K,T;S(t),t)]
        + \frac{1}{2} \frac{\partial^2}{\partial K^2}[c(T,K)^2 K^2 \rho(K,T;S(t),t)].
    \end{align*}
    Substituting $\rho(K,T;S(t),t)$ using \eqref{eq:dupire-rho} in the term on the LHS, we obtain
    \begin{align*}
      \frac{\partial}{\partial T}&\rho(K,T;S(t),t)\\
      &= \frac{\partial^2}{\partial K^2} \frac{\partial}{\partial T}\left(e^{\int_t^T r(s) \mathrm{d}s} C(T,K)\right) \tag{by Schwarz's Theorem since $\rho \in C^2$}\\
      &= \frac{\partial^2}{\partial K^2} \left(e^{\int_t^T r(s) \mathrm{d}s} \left(r(T) C(T,K) + \frac{\partial}{\partial T}C(T,K)\right)\right).
    \end{align*}
    Next, integrating the term in the partial derivative with respect to $K$, we have
    \begin{align*}
      \int r(T) K &\rho(K,T;S(t),t) \mathrm{d}K\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \int K \frac{\partial^2}{\partial K^2} C(T,K) \mathrm{d}K \tag{by \eqref{eq:dupire-rho}}\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - \int \frac{\partial}{\partial K} C(T,K) \mathrm{d}K \right)\\
      &= r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) + \gamma(T)\right),
    \end{align*}
    by integration by parts, where $\gamma(T)$ is the constant of integration, so
    \begin{align*}
      \frac{\partial}{\partial K} &[r(T) K \rho(K,T;S(t),t)]\\
      &= \frac{\partial^2}{\partial K^2} \int r(T) K \rho(K,T;S(t),t) \mathrm{d}K\\
      &= \frac{\partial^2}{\partial K^2} \left[r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) + \gamma\right)\right].
    \end{align*}
    Hence, substituting in the Fokker-Planck equation and integrating twice with respect to $K$, we get
    \begin{align*}
      e^{\int_t^T r(s) \mathrm{d}s} &\left(r(T) C(T,K) + \frac{\partial}{\partial T}C(T,K)\right)\\
      &= - r(T) e^{\int_t^T r(s) \mathrm{d}s} \left( K \frac{\partial}{\partial K} C(T,K) - C(T,K) \right)\\
        &\ \ \ \ \ + \frac{1}{2} c(T,K)^2 K^2 e^{\int_t^T r(s) \mathrm{d}s}\frac{\partial^2}{\partial K^2} C(T,K) + \alpha(T) K + \beta(T),
    \end{align*}
    where $\alpha(T), \beta(T)$ are the constants of integration of each integration ($\gamma$ is incorporated into $\beta$ in the process). Multiplying by the discounting factor and noticing that the terms in $r(T)C(T,K)$ cancel out, we have
    \begin{align*}
      \frac{\partial}{\partial T}C(T,K)
      &= - r(T) K \frac{\partial}{\partial K} C(T,K)
        + \frac{1}{2} c(T,K)^2 K^2 \frac{\partial^2}{\partial K^2} C(T,K)\\
        &\ \ \ \ \ + e^{-\int_t^T r(s)\mathrm{d}s}\alpha(T) K + e^{-\int_t^T r(s)\mathrm{d}s}\beta(T).
    \end{align*}
    Finally, when $K \to \infty$, all terms besides the constants of integration vanish by Lemma~\ref{lem:dupire-asymptotic-dK} and Assumption~\ref{ass:dupire-asymptotic-dT} so the constants must be zero themselves. That is,
    \begin{align*}
      \frac{\partial}{\partial T}C(T,K)
      &= - r(T) K \frac{\partial}{\partial K} C(T,K)
        + \frac{1}{2} c(T,K)^2 K^2 \frac{\partial^2}{\partial K^2} C(T,K)
    \end{align*}
    as expected.
  \end{proof}
\end{theorem}

\section{Generalisation to Multiple Assets}\label{sec:generalisation}

Since, contrary to Dupire's Equation in the single-asset case, the partial differential equation that is satisfied by the prices of European basket call options will turn out to involve partial derivatives with respect to the expiry time $T$ and to the weights $w_i, i=1,\ldots,d$, we fix $t$ and $K$ and denote by $C(T,\mathbf{w})$ the price at time $t$ of a European basket call option with expiry time $T$, strike price $K$ and weights $\mathbf{w}$.

We make yet another major restriction to the model, making the volatility coefficients dependent on time only. As such, we write $c_{ij}(t)$ for $i,j=1,\ldots,d$ instead of $c_{ij}(t,\mathbf{S}(t))$. That is, we rewrite \eqref{eq:local-dynamics} as
\begin{align*}
  \mathrm{d}S_i(t) &= \mu_i(t, \mathbf{S}(t)) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t),
\end{align*}
such that the risky assets have dynamics
\begin{align*}
  \mathrm{d}S_i(t) &= r(t) S_i(t) \mathrm{d}t + \sum_{j=1}^{d} c_{ij}(t) S_i(t) \mathrm{d}W^Q_j(t),
\end{align*}
for $i=1,\ldots,d$, where $c_{ij}(t), i,j=1,\ldots,d$ are deterministic, as shown by Theorem~\ref{thm:bs-risk-neutral-dynamics}. Note that, in that theorem, the volatility coefficients $c_{ij}, i,j=1,\ldots,d$ were stochastic while they are deterministic in this restriction of the local volatiliy model.

\begin{theorem}\label{thm:generalisation}
  (\cite{amster_towards_2009}, Theorem 1)
  In a local volatility model with time-dependent only volatility coefficients, the function $C(T,\mathbf{w})$ satisfies the partial differential equation
  \begin{align*}
      \frac{\partial}{\partial T} C(T,\mathbf{w})
      &= \sum_{i=1}^{d} r(T) w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
      &\ \ \ \ \ + \frac{1}{2} \sum_{i,j,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w}).
  \end{align*}
\end{theorem}

Before proving the theorem, we introduce a useful lemma. Similary to the proof of Dupire's equation, we will want to be able to express the integral without the positive part in the integrand to make it differentiable. We have
\begin{align}\label{eq:generalisation-int-over-V}
  C(T,\mathbf{w}) = e^{-\int_t^T r(s) \mathrm{d}s} \int_V (\mathbf{w}^T\mathbf{x} - K) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
\end{align}
where $V = \{ \mathbf{x} \in \mathbb{R}^d : \mathbf{w}^T \mathbf{x} \ge K \}$. However, the space $V$ depends on the weights, thus constituting a difficulty when calculating partial derivatives such as $\frac{\partial}{\partial w_i}C(T,\mathbf{w})$. Instead, we can circumvent this difficulty with a change of variable. In the paper, this is embedded in the proof of the theorem but we prove it in greater detail here as a separate lemma for clarity.

\begin{lemma}\label{lem:generalisation-var-change}
  Let $B = \mathbf{w}^T \mathbf{x}$ and $\mathbf{Q} = (Q_1,\ldots,Q_{d-1})$ with
  \begin{align}\label{eq:generalisation-var-change-Q}
    Q_i = \frac{w_i x_i}{B} = \frac{w_i x_i}{\mathbf{w}^T \mathbf{x}}.
  \end{align}
  Then
  \begin{align*}
    C(T,\mathbf{w}) = e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
  \end{align*}
  where $A = \{\mathbf{Q} : Q_i \ge 0, i=1,\ldots,d-1, \sum_{i=1}^{d-1} Q_i \le 1\}$ and
  \begin{align*}
    \mathbf{x}(\mathbf{Q},B) = \left(\frac{Q_1 B}{w_1},\dots, \frac{Q_{d-1} B}{w_{d-1}}, \frac{(1 - \sum_{i=1}^{d-1} Q_i) B}{w_{d}}\right).
  \end{align*}

  \begin{proof}
    We have
    \begin{align*}
      \frac{\partial}{\partial Q_j} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_j} \frac{Q_i B}{w_i} = 0, \text{ for } i \neq j,\\
      \frac{\partial}{\partial Q_i} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_i} \frac{Q_i B}{w_i} = \frac{B}{w_i}, \text{ for } i = 1,\ldots,d-1,\\
      \frac{\partial}{\partial Q_i} x_d(\mathbf{Q},B)
      &= \frac{\partial}{\partial Q_i} \frac{(1-\sum_{k=1}^{d-1} Q_k) B}{w_d} = \frac{-B}{w_d}, \text{ for } i = 1,\ldots,d-1,\\
      \frac{\partial}{\partial B} x_i(\mathbf{Q},B)
      &= \frac{\partial}{\partial B} \frac{Q_i B}{w_i} = \frac{Q_i}{w_i}, \text{ for } i=1,\ldots,d-1,\\
      \frac{\partial}{\partial B} x_d(\mathbf{Q},B)
      &= \frac{\partial}{\partial B} \frac{(1-\sum_{i=1}^{d-1} Q_i) B}{w_d} = \frac{1-\sum_{i=1}^{d-1} Q_i}{w_d}.
    \end{align*}
    so the Jacobian of the change of variable is
    \begin{align*}
      J = \frac{\partial \mathbf{x}}{\partial (\mathbf{Q}, B)} = \begin{bmatrix}
        \frac{B}{w_1} & 0 & \cdots & 0 & \frac{-B}{w_d}\\
        0 & \frac{B}{w_2} & \cdots & 0 & \frac{-B}{w_d}\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & \frac{B}{w_{d-1}} & \frac{-B}{w_d}\\
        \frac{Q_1}{w_1} & \cdots & \cdots & \frac{Q_{d-1}}{w_{d-1}} & \frac{1-\sum_{i=1}^{d-1} Q_i}{w_d}
      \end{bmatrix}.
    \end{align*}
    We apply a sequence of elementary operations to calculate the determinant of the Jacobian. Dividing each column by $w_1, \ldots, w_d$ respectively, we obtain
    \begin{align*}
      A_1 &= \begin{bmatrix}
        B & 0 & \cdots & 0 & -B\\
        0 & B & \cdots & 0 & -B\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & B & -B\\
        Q_1 & \cdots & \cdots & Q_{d-1} & 1-\sum_{i=1}^{d-1} Q_i
      \end{bmatrix}
    \end{align*}
    and $\det J = \frac{\det A_1}{w_1 \cdots w_d}$. Next, we add each of the first $d-1$ columns to the last one, thus obtaining
    \begin{align*}
      A_2 &= \begin{bmatrix}
        B & 0 & \cdots & 0 & 0\\
        0 & B & \cdots & 0 & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & \cdots & 0 & B & 0\\
        Q_1 & \cdots & \cdots & Q_{d-1} & 1
      \end{bmatrix}
    \end{align*}
    and $\det A_2 = \det A_1$. But $\det A_2 = B^{d-1}$ so
    \begin{align*}
      \det J = \frac{\det A_1}{w_1 \cdots w_d} = \frac{\det A_2}{w_1 \cdots w_d} = \frac{B^{d-1}}{w_1 \cdots w_d}.
    \end{align*}

    Hence, with $V$ as in \eqref{eq:generalisation-int-over-V},
    \begin{align*}
      C(T,\mathbf{w})
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_V \left(\mathbf{w}^T\mathbf{x} - K\right) \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x}\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) |\det J| \mathrm{d}\mathbf{Q} \mathrm{d}B\\
      &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B.
    \end{align*}
    as required.
  \end{proof}
\end{lemma}

We now prove the theorem in much greater detail than in the paper but following the same structure.

\begin{proof}[Proof of Theorem~\ref{thm:generalisation}]
  We start by calculating the partial derivatives of $C(T,\mathbf{w})$ with respect to $w_i$, for any $i=1,\ldots,d$, and with respect to $T$ using the result from Lemma~\ref{lem:generalisation-var-change}. For $i=1,\ldots,d$, we have
  \begin{align}
    &\frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
    &= \frac{\partial}{\partial w_i} \left(e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B\right)\notag\\
    &= e^{-\int_t^T r(s) \mathrm{d}s}\int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial w_i} \left(\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}\right) \mathrm{d}\mathbf{Q} \mathrm{d}B,\label{eq:generalisation-deriv-wi}
  \end{align}
  by repeated use of the Leibniz Integral Rule since
  \begin{align*}
    \int_K^\infty \int_A & f(\mathbf{Q},B) \mathrm{d}\mathbf{Q} \mathrm{d}B \\
    &= \int_K^\infty \int_0^1 \int_{Q_1}^1 \cdots \int_{Q_1 + \cdots + Q_{d-2}}^1 f(\mathbf{Q},B) \mathrm{d}Q_{d-1} \cdots \mathrm{d}Q_2 \mathrm{d}Q_1 \mathrm{d}B,
  \end{align*}
  for all integrable $f$. In this context, $\mathbf{Q}$ is quantified over by the integral rather than being defined as in \eqref{eq:generalisation-var-change-Q} so it does not depend on $\mathbf{w}$ and
  \begin{align*}
    \frac{\partial}{\partial w_i} x_i(\mathbf{Q},B)
    &= \frac{\partial}{\partial w_i} \frac{Q_i B}{w_i}
    = \frac{- Q_i B}{w_i^2}, \text{ for } i=1,\ldots,d-1,\\
    \frac{\partial}{\partial w_d} x_d(\mathbf{Q},B)
    &= \frac{\partial}{\partial w_d} \frac{(1 - \sum_{i=1}^{d-1} Q_i) B}{w_d}
    = \frac{\partial}{\partial w_d} \frac{Q_d B}{w_d}
    = \frac{- Q_d B}{w_d^2},
  \end{align*}
  where we take $Q_d = 1 - \sum_{i=1}^{d-1} Q_i$. Thus,
  \begin{align*}
    &\frac{\partial}{\partial w_i} \left(\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}\right)\\
    &= \frac{\partial}{\partial w_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}
      + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} \frac{B^{d-1}}{w_1 \cdots w_d}\\
    &= \frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} x_i(\mathbf{Q},B) \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &\ \ \ \ \ + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{\partial}{\partial w_i} \frac{B^{d-1}}{w_1 \cdots w_d} \\
    &= \frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{-Q_i B}{w_i^2} \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &\ \ \ \ \ + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{-1}{w_i} \frac{B^{d-1}}{w_1 \cdots w_d}\\
    &= -\frac{1}{w_i} \left(\frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{Q_i B}{w_i}
      + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right) \frac{B^{d-1}}{w_1 \cdots w_d}.
  \end{align*}
  Hence, substituting this back into \eqref{eq:generalisation-deriv-wi}, we have
  \begin{align}
    &\frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
    &= - \frac{1}{w_i}e^{-\int_t^T r(s) \mathrm{d}s}\int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\notag\\
      &\ \ \ \ \ \left(x_i(\mathbf{Q},B)\frac{\partial}{\partial x_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) + \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right) \mathrm{d}\mathbf{Q} \mathrm{d}B.\notag\\
    &= - \frac{1}{w_i}e^{-\int_t^T r(s) \mathrm{d}s}\notag\\
    &\ \ \ \ \ \ \ \ \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}
        \frac{\partial}{\partial x_i}\left[x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{Q} \mathrm{d}B.\label{eq:generalisation-dC-dw}
  \end{align}

  As in the proof of Dupire's Equation, we make use of the Fokker-Planck Equation (Theorem~\ref{thm:fokker-planck}). For all $\mathbf{x} \in \mathrm{Im}(\mathbf{S}(T))$, we have
  \begin{align}
    &\sum_{i=1}^d \frac{\partial}{\partial x_i}[r(T) x_i \rho(\mathbf{x},T;\mathbf{S}(t),t)]\notag\\
      &= \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}[c_{ij}(T) x_i(T) c_{lj}(T) x_l(T) \rho(\mathbf{x},T;\mathbf{S}(t),t)]
        -\frac{\partial}{\partial T}\rho(\mathbf{x},T;\mathbf{S}(t),t).\label{eq:generalisation-fokker-planck}
  \end{align}
  Note that this equation holds almost surely since it holds if we substitute $\mathbf{S}(t)$ for any $\mathbf{y} \in \mathrm{supp}(\mathbf{S}(t))$.

  By \eqref{eq:generalisation-dC-dw},
  \begin{align*}
    \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
    &= - \sum_{i=1}^{d} \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \  r(T)\frac{\partial}{\partial x_i}\left[x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{Q} \\
    &= - \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \sum_{i=1}^{d} \frac{\partial}{\partial x_i}\left[r(T)x_i(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{Q} \mathrm{d}B,
  \end{align*}
  and therefore, by \eqref{eq:generalisation-fokker-planck},
  \begin{align}
    \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
    &= \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}
      \left( \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\right.\notag\\
      &\ \ \ \ \ \ \ - \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T) x_i(\mathbf{Q},B) \notag\\
      &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ c_{lj}(T) x_l(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B. \label{eq:generalisation-one-deriv}
  \end{align}

  Next, we calculate the partial derivative of call prices with respect to expiry time. Using Lemma~\ref{lem:generalisation-var-change} once again, we have
  \begin{align*}
    \frac{\partial}{\partial T} &C(T,\mathbf{w})\\
    &= e^{-\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T}\int_K^\infty \int_A \left(B - K\right) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B\\
    &= e^{-\int_t^T r(s) \mathrm{d}s} \int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
  \end{align*}
  by repeated use of the Leibniz Integral Rule. But then,
  \begin{align*}
    e^{\int_t^T r(s) \mathrm{d}s} &\frac{\partial}{\partial T} C(T,\mathbf{w})\\
    &= \int_K^\infty \int_A \left(B - K\right) \frac{\partial}{\partial T}\rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \mathrm{d}\mathbf{Q} \mathrm{d}B,
  \end{align*}
  so, substituting in \eqref{eq:generalisation-one-deriv}, we obtain
  \begin{align*}
    \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
    &= e^{\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T} C(T,\mathbf{w}) - \int_K^\infty \int_A \left(B - K\right) \frac{B^{d-1}}{w_1 \cdots w_d}\notag\\
      &\ \ \ \ \ \ \ \Bigg( \frac{1}{2} \sum_{i,l,j=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T) x_i(\mathbf{Q},B) \notag\\
      &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ c_{lj}(T) x_l(\mathbf{Q},B) \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B.
  \end{align*}
  Applying Lemma~\ref{lem:generalisation-var-change} the other way around, we obtain
  \begin{align}
    \sum_{i=1}^{d} &r(T) e^{\int_t^T r(s) \mathrm{d}s} w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\notag\\
    &= e^{\int_t^T r(s) \mathrm{d}s} \frac{\partial}{\partial T} C(T,\mathbf{w}) - \frac{1}{2} \sum_{j=1}^{d} \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) \notag\\
      &\ \ \ \ \ \ \ \Bigg( \sum_{i,l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \Bigg) \mathrm{d}\mathbf{x}. \label{eq:generalisation-two-derivs}
  \end{align}

  For any $j = 1,\ldots,d$, we are now concerned with the integral
  \begin{align*}
    I_j \coloneqq \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) \left( \sum_{i,l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \right) \mathrm{d}\mathbf{x}.
  \end{align*}

  Let $\mathbf{F} = (F_1,\ldots,F_d)$ be the vector field given by
  \begin{align*}
    F_l(\mathbf{x}) = \frac{\partial}{\partial x_i} \big[c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big],
  \end{align*}
  for $l=1,\ldots,d$, whose divergence is
  \begin{align*}
    \nabla \cdot \mathbf{F} = \sum_{l=1}^{d} \frac{\partial^2}{\partial x_l \partial x_i}\big[c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big],
  \end{align*}
  so that
  \begin{align*}
    I_j = \sum_{i=1}^{d} \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) (\nabla \cdot \mathbf{F})(\mathbf{x}) \mathrm{d}\mathbf{x}.
  \end{align*}
  Since
  \begin{align*}
    \nabla \cdot ((\mathbf{w}^T \mathbf{x} - K) \mathbf{F})
    &= \nabla (\mathbf{w}^T \mathbf{x} - K) \cdot \mathbf{F} + (\mathbf{w}^T \mathbf{x} - K) (\nabla \cdot \mathbf{F})\\
    &= \mathbf{w} \cdot \mathbf{F} + (\mathbf{w}^T \mathbf{x} - K) (\nabla \cdot \mathbf{F}),
  \end{align*}
  then
  \begin{align*}
    I_j = \sum_{i=1}^{d} \left[\int_V \nabla \cdot \left(\left(\mathbf{w}^T \mathbf{x} - K\right) \mathbf{F}(\mathbf{x})\right) \mathrm{d}\mathbf{x} - \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x} \right].
  \end{align*}
  Let $S$ be the boundary of $V$. Hence,
  \begin{align*}
    S = \{\mathbf{x} : \mathbf{w}^T \mathbf{x} = K\},
  \end{align*}
  so, by the Divergence Theorem, we have
  \begin{align*}
    I_j &= \sum_{i=1}^{d} \left[\int_S \left(\mathbf{w}^T \mathbf{x} - K\right) \mathbf{F}(\mathbf{x}) \cdot \mathrm{d}\mathbf{x} - \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x} \right]\\
    &= - \sum_{i=1}^{d} \int_V \mathbf{w}^T \mathbf{F}(\mathbf{x}) \mathrm{d}\mathbf{x} \tag{since $(\mathbf{w}^T \mathbf{x} - K) = 0$ on $S$}\\
    &= - \sum_{l=1}^{d} w_l \int_V \sum_{i=1}^d \frac{\partial}{\partial x_i} \big[c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t)\big] \mathrm{d}\mathbf{x}.
  \end{align*}

  Similarly, let $\mathbf{G} = (G_1,\ldots,G_d)$ be the vector field such that
  \begin{align*}
    G_i(\mathbf{x}) = c_{ij}(T) x_i c_{lj}(T) x_l \rho(\mathbf{x},T;\mathbf{S}(t),t),
  \end{align*}
  for $i=1,\ldots,d$, so that
  \begin{align*}
    I_j
    &= - \sum_{l=1}^{d} w_l \int_V \nabla \cdot \mathbf{G}(\mathbf{x}) \mathrm{d}\mathbf{x}
    = - \sum_{l=1}^{d} w_l \int_S \mathbf{G}(\mathbf{x}) \cdot \mathrm{d}\mathbf{x},
  \end{align*}
  by the Divergence Theorem, where $S$ is to be understood as a oriented area over which we take the flux. Since $S$ is a level set of $\mathbf{x} \mapsto \mathbf{w}^T \mathbf{x}$, the gradient $\nabla (\mathbf{w}^T \mathbf{x}) = \mathbf{w}$ is perpendicular to the tangent plane of $S$ at any $\mathbf{x}$ and points in the direction of increasing values of $\mathbf{w}^T \mathbf{x}$, i.e. it points inside $V$. In other words, $- \frac{\mathbf{w}}{\|\mathbf{w}\|}$ is a normal vector of $S$ pointing outwards. Therefore,
  \begin{align}
    I_j
    &= - \sum_{l=1}^{d} w_l \int_S \mathbf{G}(\mathbf{x}) \cdot \frac{-\mathbf{w}}{\|\mathbf{w}\|} \mathrm{d}\mathbf{x}\notag\\
    &= \sum_{i,l=1}^{d} \frac{w_i w_l}{\|\mathbf{w}\|} c_{ij}(T) c_{lj}(T) \int_S x_i x_l \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},\label{eq:generalisation-Ij}
  \end{align}
  since the volatility coefficients do not depend on $\mathbf{x}$, thus justifying the restriction made in this section.

  Applying Lemma~\ref{lem:generalisation-var-change} to \eqref{eq:generalisation-dC-dw}, we have
  \begin{align*}
    \frac{\partial}{\partial w_i} C(T,\mathbf{w})
    &= - \frac{1}{w_i}e^{-\int_t^T r(s) \mathrm{d}s}
    \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) \frac{\partial}{\partial x_i}\left[x_i \rho(\mathbf{x},T;\mathbf{S}(t),t) \right] \mathrm{d}\mathbf{x},
  \end{align*}
  for $i=1,\ldots,d$. Hence, taking the vector field $\mathbf{H} = (0,\ldots,0,H_i,0,\ldots,0)$ with
  \begin{align*}
    H_i(\mathbf{x}) = x_i \rho(\mathbf{x},T;\mathbf{S}(t),t),
  \end{align*}
  we have
  \begin{align*}
    \frac{\partial}{\partial x_i}\left[x_i \rho(\mathbf{x},T;\mathbf{S}(t),t) \right]
    &= \frac{\partial}{\partial x_i} H_i(\mathbf{x})
    = \sum_{i=1}^{d} \frac{\partial}{\partial x_i} H_i(\mathbf{x}) = (\nabla \cdot \mathbf{H})(\mathbf{x}),
  \end{align*}
  so
  \begin{align*}
    e^{\int_t^T r(s) \mathrm{d}s} &\frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
    &= - \frac{1}{w_i} \int_V \left(\mathbf{w}^T \mathbf{x} - K\right) (\nabla \cdot \mathbf{H})(\mathbf{x}) \mathrm{d}\mathbf{x}\\
    &= - \frac{1}{w_i} \left( \int_V \nabla \cdot ((\mathbf{w}^T \mathbf{x} - K) \mathbf{H}) \mathrm{d}\mathbf{x} - \int_V \nabla (\mathbf{w}^T \mathbf{x} - K) \cdot \mathbf{H}(\mathbf{x}) \mathrm{d}\mathbf{x} \right)\\
    &= - \frac{1}{w_i} \left( \int_S (\mathbf{w}^T \mathbf{x} - K) \mathbf{H} \cdot \mathrm{d}\mathbf{x} - \int_V \mathbf{w}^T \mathbf{H}(\mathbf{x}) \mathrm{d}\mathbf{x} \right)\\
    &= \frac{1}{w_i} \int_V \mathbf{w}^T \mathbf{H}(\mathbf{x}) \mathrm{d}\mathbf{x}
    = \int_V x_i \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x},
  \end{align*}
  by the Divergence Theorem and since $(\mathbf{w}^T \mathbf{x} - K)$ vanishes on $S$, similarly to the derivation above implying $\mathbf{F}$. Differentiating with respect to $w_l$, for $l=1,\ldots,d$, and letting $\delta_{il} = 1$ if $i=l$, $\delta_{il} = 0$ otherwise, we thus have
  \begin{align*}
    e^{\int_t^T r(s) \mathrm{d}s} &\frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w})\\
    &= \frac{\partial}{\partial w_l} \int_V x_i \rho(\mathbf{x},T;\mathbf{S}(t),t) \mathrm{d}\mathbf{x}\\
    &= \int_K^\infty \int_A \frac{\partial}{\partial w_l} \left(\frac{Q_i B}{w_i} \rho(\mathbf{x},T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d} \right) \mathrm{d}\mathbf{Q} \mathrm{d}B \tag{by Lemma~\ref{lem:generalisation-var-change} and the Leibniz Integral Rule as before}\\
    &= \int_K^\infty \int_A \Bigg(
      \delta_{il} \frac{- Q_i B}{w_i w_l} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &\ \ \ \ \ \ \ \ \ + \frac{Q_i B}{w_i} \frac{\partial}{\partial x_l} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{- Q_l B}{w_l^2} \frac{B^{d-1}}{w_1 \cdots w_d}\\
      &\ \ \ \ \ \ \ \ \ + \frac{Q_i B}{w_i} \rho(\mathbf{x}(\mathbf{Q},B),T;\mathbf{S}(t),t) \frac{-1}{w_l} \frac{B^{d-1}}{w_1 \cdots w_d}
    \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B\\
    &= -\frac{1}{w_l} \int_K^\infty \int_A \frac{B^{d-1}}{w_1 \cdots w_d} \Bigg(
      (1 + \delta_{il}) \frac{Q_i B}{w_i} \rho(\mathbf{x},T;\mathbf{S}(t),t)\\
      &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \frac{Q_i B}{w_i} \frac{\partial}{\partial x_l} \rho(\mathbf{x},T;\mathbf{S}(t),t) \frac{Q_l B}{w_l} \Bigg) \mathrm{d}\mathbf{Q} \mathrm{d}B\\
    &= -\frac{1}{w_l} \int_V \left( (1 + \delta_{il}) x_i \rho(\mathbf{x},T;\mathbf{S}(t),t)
      + x_i x_l \frac{\partial}{\partial x_l} \rho(\mathbf{x},T;\mathbf{S}(t),t) \right) \mathrm{d}\mathbf{x}, \tag{by Lemma~\ref{lem:generalisation-var-change}}
  \end{align*}
  but
  \begin{align*}
    \frac{\partial}{\partial x_l} \left(x_i x_l \rho(\mathbf{x},T;S(t),t)\right)
    &= \frac{\partial x_i}{\partial x_l} x_l \rho(\mathbf{x},T;S(t),t)
    + x_i \frac{\partial x_l}{\partial x_l} \rho(\mathbf{x},T;S(t),t)\\
    &\ \ \ \ \ + x_i x_l \frac{\partial}{\partial x_l} \rho(\mathbf{x},T;S(t),t)\\
    &= \delta_{il} x_l \rho(\mathbf{x},T;S(t),t) + x_i \rho(\mathbf{x},T;S(t),t)\\
    &\ \ \ \ \ + x_i x_l \frac{\partial}{\partial x_l} \rho(\mathbf{x},T;S(t),t)\\
    &= (1 + \delta_{il}) x_i \rho(\mathbf{x},T;S(t),t) + x_i x_l \frac{\partial}{\partial x_l} \rho(\mathbf{x},T;S(t),t),
  \end{align*}
  so, taking the vector field $\mathbf{K} = (0,\ldots,0,K_l,0,\ldots,0)$ with
  \begin{align*}
    K_l &= x_i x_l \rho(\mathbf{x},T;S(t),t),
  \end{align*}
  we obtain
  \begin{align*}
    e^{\int_t^T r(s) \mathrm{d}s} \frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w})
    &= -\frac{1}{w_l} \int_V \frac{\partial}{\partial x_l} \left(x_i x_l \rho(\mathbf{x},T;S(t),t)\right) \mathrm{d}\mathbf{x}\\
    &= -\frac{1}{w_l} \int_V \nabla \cdot \mathbf{K} \mathrm{d}\mathbf{x}
    = -\frac{1}{w_l} \int_S \mathbf{K} \cdot \mathrm{d}\mathbf{x}\\
    &= -\frac{1}{w_l} \int_S \mathbf{K} \cdot \frac{-\mathbf{w}}{\|\mathbf{w}\|} \mathrm{d}\mathbf{x}\\
    &= \frac{1}{\|\mathbf{w}\|} \int_S x_i x_l \rho(\mathbf{x},T;S(t),t) \mathrm{d}\mathbf{x},
  \end{align*}
  by the Divergence Theorem and since $\frac{-\mathbf{w}}{\|\mathbf{w}\|}$ is a normal vector of the surface $S$ of $V$ pointing outwards as before. Substituting in \eqref{eq:generalisation-Ij}, we obtain
  \begin{align*}
    I_j
    &= e^{\int_t^T r(s) \mathrm{d}s} \sum_{i,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w}).
  \end{align*}

  Finally, substituting $I_j$ in \eqref{eq:generalisation-two-derivs} and dividing both sides by $e^{\int_t^T r(s) \mathrm{d}s}$, we have
  \begin{align*}
    \sum_{i=1}^{d} &r(T) w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
    &= \frac{\partial}{\partial T} C(T,\mathbf{w}) - \frac{1}{2} \sum_{i,j,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w}),
  \end{align*}
  as required.
\end{proof}

\subsection{Numerical Example}

In order to show how the result from Theorem~\ref{thm:generalisation} could be used in practice, we now present a scheme to approximate the price of a basket option using finite difference methods.

In the case of Dupire's Equation, we can easily make $c(T,K)$ the subject as
\begin{align}
  c(T,K) = \sqrt{\frac{\frac{\partial}{\partial T} C(T,K) + r(T) K \frac{\partial}{\partial K} C(T,K)}{\frac{1}{2}\frac{\partial^2}{\partial K^2} C(T,K)}},\label{eq:dupire-calibration}
\end{align}
so that, using Dupire's Equation, we can get an explicit scheme to \textit{calibrate} the local volatility model, i.e. find $c$ such that the prices given by the model match market data, in a fairly straightforward manner. This is not as easy in the case of the equation obtained in Theorem~\ref{thm:generalisation} so we focus here instead on using this equation to price basket options.

For the sake of comparison, we also use Monte Carlo simulations to price the same basket options in the C++ implementation. No effort is made here to optimise these methods and any concerns about errors or how fast and whether the method converges are out of scope. Implementations for both are provided in the accompanying C++ files. For convenience, the source code is also listed in the appendix.

For both methods, we use the example of a European basket call option with expiry time $T^*$, strike price $K$ and weights $\mathbf{w}^*$, whose price at time 0 we want to approximate. Note that corresponding put options could then be priced using Put-Call Parity (Theorem~\ref{thm:put-call-parity}).

\subsubsection{Finite Difference Methods}\label{sec:finite-difference}

We begin by deriving some approximations of derivatives, called \textit{finite differences}, which we will then use to discretise the PDE from Theorem~\ref{thm:generalisation}. By Taylor's Theorem, for any function $f : \mathbb{R} \to \mathbb{R}$ in $C^2$, we have
\begin{align*}
  f(x+h) &= f(x) + f'(x) h + o(h),
\end{align*}
for $x \in \mathbb{R}$ and $h > 0$, so
\begin{align*}
  f'(x) &= \frac{f(x+h) - f(x)}{h} + o(h).
\end{align*}
Similarly, since
\begin{align*}
  f(x+h) &= f(x) + f'(x) h + \frac{1}{2} f''(x) h^2 + o(h^2),\\
  f(x-h) &= f(x) - f'(x) h + \frac{1}{2} f''(x) h^2 + o(h^2),
\end{align*}
then, by subtracting the second from the first and after a bit of algebra, we get
\begin{align*}
  f'(x) &= \frac{f(x+h) - f(x-h)}{2h} + o(h^2),
\end{align*}
and by adding them together, we get
\begin{align*}
  f''(x) &= \frac{f(x+h) - 2f(x) + f(x-h)}{h^2} + o(h^2).
\end{align*}

Finally, for any function $f : \mathbb{R}^2 \to \mathbb{R}$ in $C^2$, we have
\begin{align*}
  f(x&+ (-1)^a h_x, y + (-1)^b h_y)\\
  &= f(x,y) + (-1)^a \frac{\partial}{\partial x} f(x,y) h_x + (-1)^b \frac{\partial}{\partial y} f(x,y) h_y\\
  &\ \ \ + \frac{1}{2} \frac{\partial^2}{\partial x^2} f(x,y) h_x^2 + (-1)^{a+b} \frac{\partial^2}{\partial x \partial y} f(x,y) h_x h_y + \frac{1}{2} \frac{\partial^2}{\partial y^2} f(x,y) h_y^2\\
  &\ \ \ + o(h_x^2 + h_y^2),
\end{align*}
for $x,y \in \mathbb{R}$, $h_x, h_y > 0$ and $a,b=0,1$, by Taylor's Theorem and using the fact that $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$ by Schwarz's Theorem . Hence,
\begin{align*}
  f(x+h_x,y+h_y) &- f(x+h_x,y-h_y) - f(x-h_x,y+h_y) + f(x-h_x,y-h_y)\\
  &= 4 \frac{\partial^2}{\partial x \partial y} f(x,y) h_x h_y + o(h_x^2 + h_y^2),
\end{align*}
or equivalently,
\begin{align*}
  \frac{\partial^2}{\partial x \partial y} f(x,y)
  &= \frac{1}{4 h_x h_y} [f(x+h_x,y+h_y) - f(x+h_x,y-h_y)\\
  &\ \ \ \ \ \ \ \ - f(x-h_x,y+h_y) + f(x-h_x,y-h_y)]\\
  &\ \ \ \ + o(h_x^2 + h_y^2).
\end{align*}

By Theorem~\ref{thm:generalisation}, we have
\begin{align*}
  \frac{\partial}{\partial T} C(T,\mathbf{w})
  &= \sum_{i=1}^{d} r(T) w_i \frac{\partial}{\partial w_i} C(T,\mathbf{w})\\
  &\ \ \ \ \ + \frac{1}{2} \sum_{i,j,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \frac{\partial^2}{\partial w_i \partial w_l} C(T,\mathbf{w}).
\end{align*}
Using the results established here and assuming that we choose the steps $h_T, h_{w_i}$ for $i=1,\ldots,d$ to be small enough that the error is negligeable, we can approximate the partial derivatives in the previous equation to obtain
\begin{align}\label{eq:fd-implicit}
  \frac{\hat{C}(T+h_T,\mathbf{w}) - \hat{C}(T,\mathbf{w})}{h_T}
  &= \sum_{i=1}^{d} r(T) w_i \hat{\theta}_i(T,\mathbf{w})\notag\\
    &\ \ \ \ \ + \frac{1}{2} \sum_{i,j,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \hat{\phi}_{il}(T,\mathbf{w}),
\end{align}
where, for $i,j=1,\ldots,d$,
\begin{align*}
  \hat{\theta}_i(T,\mathbf{w})
  &= \frac{\hat{C}(T,w_1,\ldots,w_i+h_{w_i},\ldots,w_d) - \hat{C}(T,w_1,\ldots,w_i-h_{w_i},\ldots,w_d)}{2h_{w_i}},
\end{align*}
\begin{align*}
  \hat{\phi}_{il}(T,\mathbf{w})
  &= \frac{1}{4 h_{w_i} h_{w_l}} \Bigg( \hat{C}(T,w_1,\ldots,w_i+h_{w_i},\ldots,w_l+h_{w_l},\ldots,w_d)\\
  &\ \ \ \ \ - \hat{C}(T,w_1,\ldots,w_i+h_{w_i},\ldots,w_l-h_{w_l},\ldots,w_d)\\
  &\ \ \ \ \ - \hat{C}(T,w_1,\ldots,w_i-h_{w_i},\ldots,w_l+h_{w_l},\ldots,w_d)\\
  &\ \ \ \ \ + \hat{C}(T,w_1,\ldots,w_i-h_{w_i},\ldots,w_l-h_{w_l},\ldots,w_d) \Bigg),\\
\end{align*}
for $i \neq l$, and
\begin{align*}
  \hat{\phi}_{ii}(T,\mathbf{w})
  &= \frac{1}{2 h_{w_i}^2} \Bigg( \hat{C}(T,w_1,\ldots,w_i+h_{w_i},\ldots,w_d)\\
  & \ \ \ \ \ \ - 2\hat{C}(T,\mathbf{w}) + \hat{C}(T,w_1,\ldots,w_i-h_{w_i},\ldots,w_d) \Bigg).
\end{align*}
Making $\hat{C}(T+h_T, \mathbf{w})$ the subject of \eqref{eq:fd-implicit}, we have
\begin{align}
  \hat{C}&(T+h_T,\mathbf{w})\notag\\
  &= \hat{C}(T,\mathbf{w}) + h_T \sum_{i=1}^{d} r(T) w_i \hat{\theta}_i(T,\mathbf{w})
     + \frac{h_T}{2} \sum_{i,j,l=1}^{d} w_i w_l c_{ij}(T) c_{lj}(T) \hat{\phi}_{il}(T,\mathbf{w})\notag\\
  &= \hat{C}(T,\mathbf{w}) + h_T r(T) \sum_{i=1}^{d} w_i \hat{\theta}_i(T,\mathbf{w}) + \frac{h_T}{2} \sum_{i,l=1}^{d} w_i w_l \hat{\phi}_{il}(T,\mathbf{w}) \sum_{j=1}^d c_{ij}(T) c_{lj}(T)\notag\\
  &= \hat{C}(T,\mathbf{w}) + h_T r(T) \sum_{i=1}^{d} w_i \hat{\theta}_i(T,\mathbf{w}) + h_T \sum_{i,l=1}^{d} w_i w_l \hat{\phi}_{il}(T,\mathbf{w}) A_{il}(T),\label{eq:fd-chat-recur}
\end{align}
where $\mathbf{A}(T) = \frac{1}{2} C(T) C(T)^T$, thus giving us an explicit formula to estimate option prices with expiry time $T+h_T$ given those with expiry time $T$.

Moreover, we can simply let
\begin{align}
  \hat{C}(0, \mathbf{w})
  &\coloneq C(0, \mathbf{w})\notag\\
  &= e^{-\int_0^0 r(s) \mathrm{d}s} \mathbb{E}_Q((\mathbf{w}^T \mathbf{S}(0) - B)^+)
  = (\mathbf{w}^T \mathbf{S}(0) - B)^+,\label{eq:fd-chat-base}
\end{align}
since $\mathbf{S}(0)$ is deterministic, thereby providing a base case for the recursive definition of $\hat{C}$.

Therefore, we can approximate $C(T^*, \mathbf{w}^*)$ by computing $\hat{C}(T^*, \mathbf{w}^*)$ following the previous formulae by choosing $h_T = \frac{T^*}{n}$ for a large enough $n$ and small enough values of $h_{w_i}, i=1,\ldots,d$. We can visualise $\hat{C}(T^*, \mathbf{w}^*)$ and its transitive dependencies as a tree in the $(T,\mathbf{w})$ space, whose root is at $(T^*, \mathbf{w}^*)$ and leaves are at $(0,\mathbf{w})$ for $w_i = w_i^* - n h_{w_i}, \ldots, w_i^* + (n-1) h_{w_i}, w_i^* + n h_{w_i}$ for each $i=1,\ldots,d$. In order to avoid too much numerical instability caused by the term in $\frac{h_T}{h_{w_i} h_{w_l}}$ in \eqref{eq:fd-chat-recur}, we can choose $h_{w_i} = \frac{w_i^*}{\sqrt{n}}$ for $i=1,\ldots,d$.

C++ does not lend itself well to recursion due to the creation of a new stack frame for each function call, potentially resulting in a stack overflow. Moreover, most nodes are required by several others so we would have to implement memoisation to avoid recomputing the same node several times.
Therefore, we instead compute the values of the nodes at expiry time 0 using \eqref{eq:fd-chat-base} and then loop over each subsequent expiry time step $k h_T$ for $k=1,\ldots,n$ to compute the values $\hat{C}(k h_T, \mathbf{w})$ for each $\mathbf{w}$ using \eqref{eq:fd-chat-recur} and using the values from the previous loop iteration. The process ends with computing the desired $\hat{C}(T^*, \mathbf{w}^*)$.

This method is implemented in \path{src/pde.h} in the files submitted alongside this dissertation.

\subsubsection{Monte Carlo Method}\label{sec:monte-carlo}

From \eqref{eq:bs-discounted-stock-price}, we get
\begin{align*}
  s_i = \ln \frac{\widetilde{S_i}(T^*)}{S_i(0)} = - \frac{1}{2} \sum_{j=1}^{d} \int_0^T c_{ij}(t)^2 \mathrm{d}t + \sum_{j=1}^{d} \int_0^T c_{ij}(t) \mathrm{d}W^Q_j(t),
\end{align*}
for all $i=1,\ldots,d$. We take $\mathbf{s} = (s_1,\ldots,s_d)$.

By breaking the interval $[0,T^*]$ into $n$ subintervals of equal length $h = \frac{T^*}{n}$, for all $i,j=1,\ldots,d$, we can approximate $s_i$ by
\begin{align*}
  \hat{s_i} &= - \frac{1}{2} \sum_{j=1}^{d} \sum_{k=1}^{n} c_{ij}(kh)^2 h + \sum_{j=1}^{d} \sum_{k=1}^{n}  c_{ij}(kh) (W^Q_j((k+1)h) - W^Q_j(kh)).
\end{align*}
As $n \to \infty$, both sums over $k$ converge to the corresponding integral by definition of Riemann and It\^o integrals. Letting $\hat{\mathbf{s}} = (\hat{s}_1,\ldots,\hat{s}_d)$, note that we can write
\begin{align*}
  \hat{\mathbf{s}} &= \sum_{k=1}^{n} \left( - \frac{1}{2} (\mathbf{C}(kh) * \mathbf{C}(kh)) [h\ h \cdots h]^T + \mathbf{C}(kh) \mathbf{\Delta}_k \right),
\end{align*}
where $\mathbf{\Delta}_k$ is the vector of increments of the Wiener processes and $*$ denotes the entry-wise product of two matrices. We can simulate realisations of $\mathbf{\Delta}_k$ since each of the increments of the Wiener processes (for each $k=1,\ldots,n$ and $j=1,\ldots,d$) follows a normal distribution with mean 0 and variance $h$, independent from one another.

Similarly, we can approximate the discounting factor $e^{-\int_0^{T^*} r(s) \mathrm{d}s}$ by
\begin{align*}
  \hat{D} = \exp \left(-\sum_{k=1}^{n} r(kh) h\right).
\end{align*}
Note that, by definition of Riemann integrals, $\hat{D}$ converges to the discounting factor as $n \to \infty$.

Therefore, by choosing a large enough $n$, we can compute a realisation of $\hat{\mathbf{s}}$ and therefore approximate a realisation of $\mathbf{S}(T^*)$ by
\begin{align*}
  \hat{S}_i(T^*) = \hat{D}^{-1} S_i(0) \exp\left(\hat{s_i}\right),
\end{align*}
for all $i=1,\ldots,d$. We can then approximate a realisation of the discounted payoff
\begin{align}\label{eq:monte-carlo-discounted-payoff}
  e^{-\int_0^{T^*} r(s) \mathrm{d}s} ((\mathbf{w}^*)^T \mathbf{S}(T^*) - K)^+
\end{align}
by
\begin{align*}
  \hat{D} ((\mathbf{w}^*)^T \hat{\mathbf{S}}(T^*) - K)^+.
\end{align*}

Sampling a large enough amount $m$ of simulations, we can approximate the $Q$-expectation of \eqref{eq:monte-carlo-discounted-payoff} by taking the sample average since the sample average converges to the expectation by the Law of Large Numbers. That is, we can approximate the price of the basket option $C(T^*,\mathbf{w}^*)$ by
\begin{align*}
  \hat{C}(T^*,\mathbf{w}^*) = \hat{D} \frac{1}{m} \sum_{l=1}^{m} ((\mathbf{w}^*)^T \hat{\mathbf{S}}_l(T^*) - K)^+,
\end{align*}
where $\hat{\mathbf{S}}_l(T^*)$ denotes the $l$-th simulation as described above.

This method is implemented in \path{src/monte_carlo.h}.

\subsubsection{Sanity Check}

Running the program gives the following output, where we can see that the pricer using finite difference methods produces similar prices than the ones we get from Monte Carlo simulations, thus providing evidence that it works as expected.

{\small
\begin{verbatim}
Parameters of the example model:
S(0) = (100, 20, 50)
r(t) = 0.025 + t * 0.01
c_{ij}(t) = 0.05 + 0.01 * (t-5+(i+j%5-2))^2 + 0.1 * (i+j%5-2)

=== Test for basket option in the money ===

Weights:              (2.5, 3, 1)
Expiry time:          0.273973
Strike price:         250
Initial basket value: 360

Monte Carlo price:    115.998
PDE price:            116.027

=== Test for basket option out of the money ===

Weights:              (4, 1.5, 6)
Expiry time:          0.273973
Strike price:         850
Initial basket value: 730

Monte Carlo price:    41.3468
PDE price:            40.6889
\end{verbatim}
}

\subsubsection{Volatility Smile}

\begin{figure}
  \caption{Implied volatility of optimised parametric models
  %%%%%%%%%%%%%
\comment{What is Model1 and Model2? }
%%%%%%%%%%%%%
  }
  \includegraphics[width=1\textwidth]{graphs/fitted_models.png}
  \label{fig:fitted-models}
\end{figure}

The motivation behind local volatility models was to account for the volatility smile that occurs in market data but that the classic Black-Scholes model is not able to fit. Indeed, the implied volatility of options as a function of the strike price (or of the expiry time) is not constant in practice and typically forms a graph in the shape of a smile, giving it its name. If we were to plot the implied volatiliy of option prices obtained from the classic Black-Scholes model, we would of course obtain a constant graph since the volatility is constant in that model.

Consider the model from Section~\ref{sec:generalisation} with two risky assets and a volatility matrix of the form
\begin{align*}
  \mathbf{C}(t) = \begin{bmatrix}
    a_1 + a_2 t + a_3 t^2 & a_4 + a_5 t + a_6 t^2\\
    a_7 + a_8 t + a_9 t^2 & a_{10} + a_{11} t + a_{12} t^2
  \end{bmatrix},
\end{align*}
for some parameters $\mathbf{a} = (a_1, \ldots, a_{12})$, with constant interest rate $r = 0.02$ and initial prices $\mathbf{S}(0) = (100, 50)$.

For each price of a basket call option with expiry time $T$, strike price $K$ and weights $\mathbf{w}$, as given by the pricer using Theorem~\ref{thm:generalisation} or by market data, we can calculate the implied volatility $\sigma_\text{BS}$ given by the classic Black-Scholes model, viewing the basket option as a call option with the same expiry time and strike price but on the underlying $B(t)$ satisfying
\begin{align*}
  \mathrm{d}B(t) = r B(t) \mathrm{d}t + \sigma_\text{BS} B(t) \mathrm{d}W^Q(t).
\end{align*}
The details of the calculation of the implied volatility from the price of a call option are omitted here since the classic Black-Scholes model is out of scope. Varying the strike price (or the expiry time), we can plot the graph of the implied volatility.

In particular, for any choice of parameters $\mathbf{a}$, we can fix $T=1$ and $\mathbf{w}=(1,2)$ and use the method described in Point~\ref{sec:finite-difference} to approximate the option prices $C(1,K)$ for $K=100,105,\ldots,300$ and, for each of the option prices obtained, calculate the implied volatility $\sigma_\text{BS}^\mathbf{a}(K)$.

Since we do not have actual market data, let us pretend, for the sake of this example, that the implied volatility of the prices of the options traded on the market with expiry time $T=1$, weights $\mathbf{w}=(1,2)$ and strike price $K$, are given by $\sigma^\text{market}_\text{BS}(K) = 0.0001 (K - 200)^2 + 0.25$ for each $K=100,105,\ldots,300$. This polynomial has a smile shape with the lowest implied volatility at the money so it is somewhat representative of what we can expect from actual market data.

We can then optimise the parameters such that the implied volatility fits market data as closely as possible. That is, we want to find the parameters $\mathbf{a}$ that minimise the error between the values $\sigma_\text{BS}^\mathbf{a}(K)$ and $\sigma_\text{BS}^\text{market}(K)$. More precisely, we take this error to be
\begin{align*}
  \sum_{K = 100,105,\ldots,300} \left( \sigma_\text{BS}^\text{market}(K) - \sigma_\text{BS}^\mathbf{a}(K) \right)^2.
\end{align*}

%%%%%%%%%%%%%
\comment{To compute the implied volatility we need to use some option prices. Without knowing these it is hard to see how the plots were produced. I suggest to write out the parameters of the model which is used in this example and the option prices used for the computation of the implied volatility.}
%%%%%%%%%%%%%

This optimisation is performed using the Python package \texttt{scipy.optimize} in the accompanying \path{notebook.py} and produces the results shown on the first graph of Figure~\ref{fig:fitted-models}. The blue graph shows the implied volatility $\sigma_\text{BS}^\mathbf{a}(K)$ obtained from option prices produced by our model with optimised parameters $\mathbf{a}$, while the orange graph shows the implied volatility $\sigma_\text{BS}^\text{market}(K)$ that we pretend to get from the prices traded on the market.

Note that this is simply to demonstrate how the model is able to fit the volatility smile contrary to the classic Black-Scholes model, rather than showing how to actually calibrate the model in practice.

Similarly, we can fix $K=200$ and do the same for $T=0.1,0.125,\ldots,1$ for fake implied volatility generated by the arbitrary polynomial $T \mapsto 0.5 (T - 0.5)^2 + 0.25$. The results are shown on the second graph of Figure~\ref{fig:fitted-models}.

% The results are shown in Figure~\ref{fig:fitted-models}. In both cases, we set the weights to $\mathbf{w} = (1,2)$. On the left, we fix the expiry time to 1 with target $K \mapsto 0.0001 (K - 200)^2 + 0.25$
%%%%%%%%%%%%%
\comment{it is not clear where the $K \mapsto 0.0001 (K - 200)^2 + 0.25$ came from. Why such formula?}
%%%%%%%%%%%%%
% , while, on the right, we fix the strike price to 200 with target $T \mapsto 0.5 (T - 0.5)^2 + 0.25$. %%%%%%%%%%%%%
\comment{same here. It is not clear why such curve is taken.}
%%%%%%%%%%%%%

In this example, we see that the model is able to fit the (fake) volatility smile as a function of the expiry time but not so much the smile as a function of the strike price. This is unsurprising considering that, in our model, the volatility matrix is time-dependent only contrary to a more general local volatility model where it also depends on the asset prices.

The code used to generate these results is available in the accompanying file \path{notebook.py} which uses bindings to the C++ pricer from \path{src/python.cpp}.

\subsection{Caveats}

Finally, we end by pointing to some limitations of the model presented here. This is, by no means, an exhaustive list.

The local volatility model assumes that the volatility coefficients are deterministic functions of time and asset prices. This non-parametric approach allows calibration to fit market data. For example, in the single-asset case, \eqref{eq:dupire-calibration} can be used to find a function $c(t,S(t))$ such that the option prices given by the model perfectly match the prices at which these options are traded. However, the model would need to be recalibrated as the market moves, since it makes the unrealistic assumption that volatility is a deterministic function of time and asset prices.

To make matters worse, Theorem~\ref{thm:generalisation} assumes that volatility coefficients depends on time only and not on asset prices. This is an even less realistic assumption and will reduce the ability of the model to accurately capture the assets' dynamics.

In the multi-asset case, we would also have to deal with the scarcity of market data. Indeed, calibration would require the price of many basket options with different weights but, in practice, not many of them are traded.

Finally, contrary to Dupire's Equation where we only have partial derivatives with respect to expiry time and strike price respectively, Theorem~\ref{thm:generalisation} has many different partial derivatives. This implies that, as $d$ becomes larger, the number of nodes required by numerical methods can grow very large. For example, in the method presented above, the number of nodes, and therefore the time complexity of the algorithm, is in $\Theta(n^{d+1})$ since the space $(T,\mathbf{w})$ has $d+1$ dimensions. This can quickly make the problem intractable.

\pagebreak
\appendix
\section{Appendix}

\subsection{Source Code of Numerical Example}

The code is written using C++ templates to make it work with an arbitrary number of assets. It uses Boost uBLAS for vectors and matrices. Both pricers can work in parallel if OpenMP is available.

The file \path{src/model.h} provides type definitions used in the rest, most notably \texttt{Model} and \texttt{BasketOption} which are just some simple \texttt{struct}s.

The pricer using finite difference methods is implemented in \path{src/pde.h}, more specifically, in the function \texttt{PDE::pricer}, which takes the number of time steps and the parameters of the model and the basket call option and return its approximated price. The code follows the structure of the explanation from Point~\ref{sec:finite-difference} above and makes use of the class \texttt{TreeLayer} and iterators for bookeeping such that the function itself is unencumbered by memory details and remains fairly readable.

Monte Carlo simulations are implemented in \path{src/monte_carlo.h} in the function \texttt{MonteCarlo::pricer} with a similar interface than \texttt{PDE::pricer}. The code follows the same structure than the explanation in Point~\ref{sec:monte-carlo}.

Finally, the file \path{src/main.cpp} contains an example consisting of pricing two basket options (in and out of the money) with both pricing methods to check that the returned approximated prices are similar as they should.

The code is provided with many comments to make it more digestable.

\lstinputlisting[language=c++,caption=src/model.h]{src/model.h}
\needspace{5cm}
\lstinputlisting[language=c++,caption=src/pde.h]{src/pde.h}
\needspace{5cm}
\lstinputlisting[language=c++,caption=src/monte\_carlo.h]{src/monte_carlo.h}
\needspace{5cm}
\lstinputlisting[language=c++,caption=src/main.cpp]{src/main.cpp}

\pagebreak
\renewcommand{\bibfont}{\normalfont\small}
\printbibliography

\end{document}
